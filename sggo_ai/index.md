
## 一、AI算法基础

1、样本不平衡的解决方法？
2、交叉熵函数系列问题？与最大似然函数的关系和区别？
3、HMM、MEMM vs CRF 对比？
4、SVM和LR的区别与联系？
5、crf的损失函数是什么？lstm+crf怎么理解？
6、GBDT vs Xgboost
7、评估指标f1和auc的区别是哪些?
8、sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？
9、神经网络中的激活函数的对比？

## 二、NLP高频问题

1、word2vec和tf-idf 相似度计算时的区别？
2、word2vec和NNLM对比有什么区别？（word2vec vs NNLM）
3、 word2vec负采样有什么作用？
4、word2vec和fastText对比有什么区别？（word2vec vs fastText）
5、glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）
6、 elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）
7、LSTM和GRU的区别？

## 三、其他算法问题

1、怎么进行单个样本的学习？
2、 决策树 bagging boosting adaboost 区别？RF的特征随机目的是什么？
3、transformer各部分怎么用？Q K V怎么计算；Attention怎么用？
4、HMM 假设是什么？CRF解决了什么问题？CRF做过特征工程吗？HMM中的矩阵意义？5、说以一下空洞卷积？膨胀卷积怎么理解？什么是Piece-CNN？
6、怎么解决beam-search局部最优问题？global embedding 怎么做？
7、数学题：什么是半正定矩阵？机器学习中有什么应用？
8、卷积的物理意义是什么？傅里叶变换懂吗？
9、说一下Bert？
10、推导word2vec？
11、怎么理解传统的统计语言模型？现在的神经网络语言模型有什么不同？
12、神经网络优化的难点是什么？这个问题要展开来谈。
13、attention你知道哪些？
14、自动文章摘要抽取时，怎么对一篇文章进行分割？（从序列标注、无监督等角度思考）
15、在做NER任务时，lstm后面可以不用加CRF吗？
16、通过画图描述TextRank？
17、LDA和pLSA有什么区别？
18、Transformer在实际应用中都会有哪些做法？
19、讲出过拟合的解决方案？
20、说一下transforemr、LSTM、CNN间的区别？从多个角度进行讲解？
21、梯度消失的原因和解决办法有哪些？
22、数学题：贝叶斯计算概率？
23、数学题：25只兔子赛跑问题，共5个赛道，最少几次比赛可以选出前5名？
24、数学题：100盏灯问题？

## Reference

- [【NLP/AI算法面试必备-2】NLP/AI面试全记录（持续更新）][1]
- [【NLP/AI算法面试必备-1】学习NLP/AI，必须深入理解“神经网络及其优化问题”][2]
- [JayLouNLP算法工程师][2]

[1]: https://zhuanlan.zhihu.com/p/57153934
[2]: https://www.zhihu.com/people/lou-jie-9/posts
[3]: https://zhuanlan.zhihu.com/p/56633392