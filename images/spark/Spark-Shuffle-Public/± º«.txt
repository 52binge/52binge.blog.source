方案一：推卸责任的做法： spark自己不解决，找HiveETL解决

方案二：碰运气的做法： 原来的并行度导致了倾斜，调整并行度， 如果是自定义的分区规则决定必须是n个分区，是n个Task
	依然使用默认的HasPartitoiner，那么这种碰运气的方案是有用的

方案三：无用数据直接过滤。 产生数据倾斜是由于部分无效数据导致的。把这部分无效数据过滤掉即可！
	你们的大宽表：有些字段的值：null   department
	select department, count(*) as total from employee group by deparment;
	底层的分区规则规则，不管是什么规则，都会把所有的 null 记录分发到同一个Task
	GM角色： select department, count(*) as total from employee group by deparment where role != "gm";
	rdd = sparkContext.xxxx()
	rdd.filter(x => true | false)  把无效数据进行过滤。

	NX 的 DP  DE  DM 大数据课程中个，也会做代码实现，运行看效果。

方案四：大小表做链接, mapjoin  
	spark中如何实现 mapjoin的逻辑呢？ 使用spark的广播机制！ 
	就是可以把小表数据当做广播变量，使用广播机制，把该变量数据，广播到所有的executor里面去。 


方案五：现在假设一个数据倾斜场景中的数据分为两种：
	一种是导致倾斜的数据集合：  单独处理
	一种是不导致倾斜的数据集合： 单独处理
	最后把结果合起来！ 

方案六： 两阶段聚合 （聚合类逻辑的通用解决方案）  纵向切分
		原来：一次hash散列导致倾斜
		现在：一次随机shuffle + 一次hash散列


方案七： 增加随机字段/链接字段 +  扩容RDD
	表A  表B 
	RDD1   rdd2

	两种场景：
	1、如果 两张表做笛卡尔积
	2、如果两张表做join，并且导致数据倾斜的某些key比较多。 
	拆分出来单独处理（依然可能有数据倾斜 ==> 加随机前缀）
	如果是导致倾斜的key只有一个，这个key的数据量非常。  加随机前缀  复制

	1、笛卡尔积
	2、导致倾斜的key的数据；量特别大。 不能使用单个task解决

方案八：  任务横切，一分为二，单独处理  横向切分



10：20之前搞定！