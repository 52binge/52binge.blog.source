---
date: 2019-08-14 15:19:48
---

图像和语音属于感知智能，而文本属于认知智能，所以号称是“人工智能的明珠”，难度很大。

1.文本挖掘简介和抽取算法概况

2.传统抽取算法原理及案例：HMM、CRF（重点）

3.基于深度学习的抽取算法原理及案例：双向LSTM、预训练模型（重点）

4.抽取算法的应用实践

## 1. NER Introduce

命名识别任务（Named-entity recognition），简称 NER，是自然语言处理中的一个非常基础和重要的任务。命名实体识别任务是指在非结构化的文本中抽取出特定意义的实体，包括人名、地名、机构名等。 

命名实体识别是未登录词中数量最多、识别难度最大、对分词效果影响最大的问题，同时它也是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。 

## 2. NER Baseline

**Bilstm+CRF** 是一个非常强的 baseline 模型，是目前基于深度学习的 NER 方法中最主流的模型。

该模型主要包括 Embedding 层，双向 LSTM 层和 CRF 层。 

**ELMO / GPT / Bert**

1. ELMO 是基于双向 LSTM 的语言模型.
2. GPT 是单向 Transformer 语言模型.
3. Bert 是双向 Transformer 语言模型.

**NLP 领域已经开始从单一任务学习**，发展为**`多任务两阶段学习`**：

1. 第一阶段利用语言模型进行预训练；
2. 第二个阶段在下游任务上 finetune。

这些语言模型在 NER 都达到了非常好的效果。 

> 姜兴华，浙江大学计算机硕士 ，研究方向机器学习，自然语言处理，在 ACM-multimedia、IJCAI 会议上发表过多篇文章。在 ByteCup2018 比赛中获得第一名。
>
> 崔德盛，北京邮电大学模式识别实验室 ，主要的研究方向是自然语言处理和广告推荐，曾获 2017 知乎看山杯挑战赛亚军，2017 摩拜算法挑战赛季军，2019 搜狐算法大赛冠军。

---

本次比赛提供了全新的数据集，包括一个大规模的未标注的语料和一个 10000 条标注数据的文档多字段抽取数据集。同时，数据集还做了独特的加密，只提供单词的 id，并不提供单词的字符串（很多预训练好的模型比如 word2vec, elmo 和 bert 都没法直接使用）。 

参赛选手可以直接使用标注数据集训练单任务模型，比如：bilstm+crf 模型，因为该数据集没有原始的单词字符串表示，所以没法使用 Pos-tagger 等信息辅助算法学习。另外，本比赛提供了一个大规模的未标注语料，参赛者也可以使用 word2vec [7] , Glove [8]  等工具训练词向量，将词向量作为单任务模型的词向量初始化。为了更好地利用这个未标注语料，参赛者也可以训练语言模型（ELMO, Bert 等），然后在语言模型上进行下游的 NER 任务。 

## Reference

- 摘要心得: **`baeline`** 是非常重要的.
- [达观数据高翔详解文本抽取（附“达观杯”参赛方式）][1]
- [第三届“达观杯”文本智能算法大赛参赛指南][2]

[1]: https://zhuanlan.zhihu.com/p/75342886
[2]: https://www.jishuwen.com/d/2TEc#tuit

