---
title: Business intelligence
date: 2022-04-23 00:20:48
music:
  server: netease   # netease, tencent, kugou, xiami, baidu
  type: song        # song, playlist, album, search, artist
  id: 17423740      # song id / playlist id / album id / search keyword
  autoplay: true
valine:
  placeholder: æœ‰ä»€ä¹ˆæƒ³å¯¹æˆ‘è¯´çš„å‘¢ï¼Ÿ
---

Diligence is not a race against time, but **continuous**, dripping water wears through the rock. 

Plan | Time | Topic | Level2
:---: | --- | --- | ---
**2022.06** | | | 
1. | 7:00~7:30 | HIMYM EP02 | 
2. | 7:30~8:20 | [2021 leetcode](/2021/03/19/leetcode/2021-leetcode/) |  2.1 binary-search <br> 2.2 dfs + stack <br> 2.3 dynamic programming <br> 2.4 sliding window & hash 
3. | 8:30~9:30 | spark basic | 3.1 mr vs spark (4) <br> 3.2 rdd / dataframe / dataset <br> 3.3 rdd operations - transformation + action <br> 3.4 cache + persist <br> 3.5 spark join 
**2022.07** | | | 
1. | 7:30~8:20 | sql window |  
2. | 8:30~9:30 | project / spark |  
3. | 8:30~9:30 | flink |  

> 1. [Sparké¢è¯•æ•´ç† hdc520 å¤§å…¨å¥½æ€»ç»“](https://www.cnblogs.com/hdc520/p/12588379.html) 2. [2021 Leetcode all](/lc/) / [2021 Leetcode](/2021/03/19/leetcode/2021-leetcode/) 3. [SQL](/categories/sql/)

<p style="font-style:italic;color:cornflowerblue;">å°èˆŸå¾æ­¤é€ æ±Ÿæµ·å¯„é¤˜ç”ŸğŸ§˜ is inputting <img src=/images/tw/main-progress-blue-dot.gif style="box-shadow:none; margin:0;height:16px">
</p>

> `2022.05.27`: review: **spark vs MR**   [Sparké¢è¯•æ•´ç† hdc520 å¤§å…¨å¥½æ€»ç»“](https://www.cnblogs.com/hdc520/p/12588379.html)
>
> 1. DAGè®¡ç®—æ¨¡å‹. sparké‡åˆ° wide dependency æ‰ä¼šå‡ºç°shufferï¼Œè€Œhadoopæ¯æ¬¡MapReduceéƒ½ä¼šæœ‰ä¸€æ¬¡shufferï¼›
> 2. MapReduce æ¯æ¬¡shuffle æ“ä½œåï¼Œå¿…é¡»å†™åˆ°ç£ç›˜, è€Œsparkå¯ä»¥ cache/persist.   RDDåœ¨æ¯æ¬¡transformationåå¹¶ä¸ç«‹å³æ‰§è¡Œï¼Œè€Œä¸”actionåæ‰æ‰§è¡Œï¼Œæœ‰è¿›ä¸€æ­¥å‡å°‘äº†I/Oæ“ä½œã€‚
> 3. MRå®ƒå¿…é¡»ç­‰mapè¾“å‡ºçš„æ‰€æœ‰æ•°æ®éƒ½å†™å…¥æœ¬åœ°ç£ç›˜æ–‡ä»¶ä»¥åï¼Œæ‰èƒ½å¯åŠ¨reduceæ“ä½œ
> 4. spark åˆ©ç”¨å¤šçº¿ç¨‹æ¥æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼ˆHadoop MapReduceé‡‡ç”¨çš„æ˜¯è¿›ç¨‹æ¨¡å‹ï¼‰ï¼Œå‡å°‘ä»»åŠ¡çš„å¯åŠ¨å’Œåˆ‡æ¢å¼€é”€ï¼›
>
> **sparkçš„RDDä¸DataFrameä»¥åŠDatasetçš„åŒºåˆ«**
> **ï¼ˆ1ï¼‰RDDç‰¹ç‚¹**
> ã€€ã€€ï¼‘ï¼‰å¼¹æ€§ï¼šRDDçš„æ¯ä¸ªåˆ†åŒºåœ¨sparkèŠ‚ç‚¹ä¸Šå­˜å‚¨æ—¶é»˜è®¤æ˜¯æ”¾åœ¨å†…å­˜ä¸­çš„ï¼Œè‹¥å†…å­˜å­˜å‚¨ä¸ä¸‹ï¼Œåˆ™å­˜å‚¨åœ¨ç£ç›˜ä¸­ã€‚
> ã€€ã€€ï¼’ï¼‰åˆ†å¸ƒæ€§ï¼šæ¯ä¸ªRDDä¸­çš„æ•°æ®å¯ä»¥å¤„åœ¨ä¸åŒçš„åˆ†åŒºä¸­ï¼Œè€Œåˆ†åŒºå¯ä»¥å¤„åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸­ï¼ 
> ã€€ã€€ï¼“ï¼‰å®¹é”™æ€§ï¼šå½“ä¸€ä¸ªRDDå‡ºç°æ•…éšœæ—¶ï¼Œå¯ä»¥æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»æ¥é‡æ–°è®¡ç®—å‡ºå‘ç”Ÿæ•…éšœçš„RDD.
> 
> **2ã€sparkçš„ç®—å­**
> ï¼ˆ1ï¼‰transformç®—å­ï¼šmapè½¬æ¢ç®—å­ï¼Œfilterç­›é€‰ç®—å­ï¼Œflatmapï¼ŒgroupByKeyï¼ŒreduceByKeyï¼ŒsortByKeyï¼Œjoinï¼Œcogroupï¼ŒcombinerByKeyã€‚
> ï¼ˆ2ï¼‰actionç®—å­ï¼šreduceï¼Œcollectï¼Œcountï¼Œtakeï¼Œaggregateï¼ŒcountByKeyã€‚
> 
> **(2.5) coalesceä¸repartitionçš„åŒºåˆ«**
>ã€€ã€€1ï¼‰coalesce ä¸ repartition éƒ½æ˜¯å¯¹RDDè¿›è¡Œé‡æ–°åˆ’åˆ†ï¼Œrepartitionåªæ˜¯coalesceæ¥å£ä¸­å‚æ•°shuffleä¸ºtrueçš„å®ç°ã€‚
>ã€€ã€€2ï¼‰è‹¥coalesceä¸­shuffleä¸ºfalseæ—¶ï¼Œä¼ å…¥çš„å‚æ•°å¤§äºç°æœ‰çš„åˆ†åŒºæ•°ç›®ï¼ŒRDDçš„åˆ†åŒºæ•°ä¸å˜ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸ç»è¿‡shuffleï¼Œæ˜¯æ— æ³•å°†RDDçš„åˆ†åŒºæ•°å˜å¤šçš„ã€‚
>ã€€ã€€3ï¼‰è‹¥å­˜åœ¨è¿‡å¤šçš„å°ä»»åŠ¡çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡coalesceæ–¹æ³•ï¼Œæ”¶ç¼©åˆå¹¶åˆ†åŒºï¼Œå‡å°‘åˆ†åŒºçš„ä¸ªæ•°ï¼Œå‡å°ä»»åŠ¡è°ƒåº¦æˆæœ¬ï¼Œå°½é‡é¿å…shuffleï¼Œè¿™æ ·ä¼šæ¯”repartitionæ•ˆç‡é«˜ã€‚
>
>**(2.6) reduceByKeyä¸groupByKeyçš„åŒºåˆ«ï¼š**
>ã€€ã€€pairRdd.reduceByKey(_+_).collect.foreach(println)ç­‰ä»·äºpairRdd.groupByKey().map(t => (t._1,t._2.sum)).collect.foreach(println)
>ã€€ã€€reduceByKeyçš„ç»“æœï¼šï¼ˆhello,2ï¼‰(world,3) groupByKeyçš„ç»“æœï¼šï¼ˆhello,(1,1)ï¼‰ï¼ˆworld,(1,1,1)ï¼‰
>ã€€ã€€ä½¿ç”¨reduceByKey()çš„æ—¶å€™ï¼Œä¼šå¯¹åŒä¸€ä¸ªKeyæ‰€å¯¹åº”çš„valueè¿›è¡Œæœ¬åœ°èšåˆï¼Œç„¶åå†ä¼ è¾“åˆ°ä¸åŒèŠ‚ç‚¹çš„èŠ‚ç‚¹ã€‚è€Œä½¿ç”¨groupByKey()çš„æ—¶å€™ï¼Œå¹¶ä¸è¿›è¡Œæœ¬åœ°çš„æœ¬åœ°èšåˆï¼Œè€Œæ˜¯å°†å…¨éƒ¨æ•°æ®ä¼ è¾“åˆ°ä¸åŒèŠ‚ç‚¹å†è¿›è¡Œåˆå¹¶ï¼ŒgroupByKey()ä¼ è¾“é€Ÿåº¦æ˜æ˜¾æ…¢äºreduceByKey()ã€‚è™½ç„¶groupByKey().map(func)ä¹Ÿèƒ½å®ç°reduceByKey(func)åŠŸèƒ½ï¼Œä½†æ˜¯ï¼Œä¼˜å…ˆä½¿ç”¨reduceByKey(func)ï¼
>
>**(2.7) sparkçš„cacheå’Œpersistçš„åŒºåˆ«ï¼š**
>ã€€ã€€1ï¼‰è®¡ç®—æµç¨‹DAGç‰¹åˆ«é•¿,æœåŠ¡å™¨éœ€è¦å°†æ•´ä¸ªDAGè®¡ç®—å®Œæˆå¾—å‡ºç»“æœï¼Œè‹¥è®¡ç®—æµç¨‹ä¸­çªç„¶ä¸­é—´ç®—å‡ºçš„æ•°æ®ä¸¢å¤±äº†,sparkåˆä¼šæ ¹æ®RDDçš„ä¾èµ–å…³ç³»é‡æ–°è®¡ç®—ï¼Œè¿™æ ·ä¼šæµªè´¹æ—¶é—´ï¼Œä¸ºé¿å…æµªè´¹æ—¶é—´å¯ä»¥å°†ä¸­é—´çš„è®¡ç®—ç»“æœé€šè¿‡cacheæˆ–è€…persistæ”¾åˆ°å†…å­˜æˆ–è€…ç£ç›˜ä¸­
>ã€€ã€€2ï¼‰cacheæœ€ç»ˆè°ƒç”¨äº†persistæ–¹æ³•ï¼Œé»˜è®¤çš„å­˜å‚¨çº§åˆ«ä»…æ˜¯å­˜å‚¨å†…å­˜ä¸­çš„ï¼›persistæ˜¯æœ€æ ¹æœ¬çš„åº•å±‚å‡½æ•°ï¼Œæœ‰å¤šä¸ªå­˜å‚¨çº§åˆ«ï¼Œexecutoræ‰§è¡Œæ—¶ï¼Œ60%ç”¨æ¥ç¼“å­˜RDDï¼Œ40%ç”¨æ¥å­˜æ”¾æ•°æ®ï¼
>
>**ä¹ã€sparkå¦‚ä½•åˆ†åŒºï¼š**
>
>ã€€ã€€åˆ†åŒºæ˜¯RDDå†…éƒ¨å¹¶è¡Œè®¡ç®—çš„ä¸€ä¸ªè®¡ç®—å•å…ƒï¼ŒRDDçš„æ•°æ®é›†åœ¨é€»è¾‘ä¸Šè¢«åˆ’åˆ†ä¸ºå¤šä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸€ä¸ªåˆ†ç‰‡ç§°ä¸ºåˆ†åŒºï¼Œåˆ†åŒºçš„ä¸ªæ•°å†³å®šäº†å¹¶è¡Œè®¡ç®—çš„ç²’åº¦ï¼Œè€Œæ¯ä¸ªåˆ†åŒºçš„æ•°å€¼è®¡ç®—éƒ½æ˜¯åœ¨ä¸€ä¸ªä»»åŠ¡ä¸­è¿›è¡Œçš„ï¼Œå› æ­¤ä»»åŠ¡çš„ä¸ªæ•°ï¼Œä¹Ÿæ˜¯ç”±RDD(å‡†ç¡®æ¥è¯´æ˜¯ä½œä¸šæœ€åä¸€ä¸ªRDD)çš„åˆ†åŒºæ•°å†³å®šã€‚sparké»˜è®¤åˆ†åŒºæ–¹å¼æ˜¯HashPartitionerï¼åªæœ‰Key-Valueç±»å‹çš„RDDæ‰æœ‰åˆ†åŒºçš„ï¼ŒéKey-Valueç±»å‹çš„RDDåˆ†åŒºçš„å€¼æ˜¯Noneï¼Œæ¯ä¸ªRDDçš„åˆ†åŒºIDèŒƒå›´ï¼š0~numPartitions-1ï¼Œå†³å®šè¿™ä¸ªå€¼æ˜¯å±äºé‚£ä¸ªåˆ†åŒºçš„ã€‚

> `2022.05.24`  **DFS / Stack**
>  2.1 å­—ç¬¦ä¸²è§£ç  â€œ3[a2[c]]â€ == â€œaccaccâ€, stack == [(3, ""), (2,"a")]
>
>  [215. æ•°ç»„ä¸­çš„ç¬¬Kä¸ªæœ€å¤§å…ƒç´ ](https://leetcode-cn.com/problems/kth-largest-element-in-an-array/)  from heapq import heapify, heappush, heappop 
>  pythonä¸­çš„heapæ˜¯å°æ ¹å †:  heapify(hp) , heappop(hp), heappush(hp, v) 
>
> `2022.05.22`  **binary-search**
>  1.1 äºŒåˆ†æŸ¥æ‰¾, while l <= r  1.2 two_sum (easy hash) 1.3 [3sum](https://leetcode.cn/problems/3sum/),(first, second, third)
>  1.4 [34. åœ¨æ’åºæ•°ç»„ä¸­æŸ¥æ‰¾å…ƒç´ çš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªä½ç½®](https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/)

> `2022.05.21` full outer joinç›¸å¯¹æ¥è¯´è¦å¤æ‚ä¸€ç‚¹ï¼Œfull outer joinä»…é‡‡ç”¨sort merge joinå®ç°ï¼Œå·¦è¾¹å’Œå³è¡¨æ—¢è¦ä½œä¸ºstreamIterï¼Œåˆè¦ä½œä¸ºbuildIter
> {% image "/images/spark/spark-full-outer-join.png", width="600px", alt="" %}

> `2022.05.20` 
> 1.1 [Sparkå¤„ç†æ•°æ®æ¯”Hiveå¿«çš„åŸå› ](https://book.itheima.net/study/1269935677353533441/1270196659166420993/1270200848609222657)
>
> æ€»ç»“ï¼š Sparkæ¯”Mapreduceè¿è¡Œæ›´å¿« - Sparkåœ¨è®¡ç®—æ¨¡å‹å’Œè°ƒåº¦ä¸Šæ¯”MRåšäº†æ›´å¤šçš„ä¼˜åŒ–ï¼Œä¸éœ€è¦è¿‡å¤šåœ°å’Œç£ç›˜äº¤äº’ã€‚ä»¥åŠå¯¹JVMä½¿ç”¨çš„ä¼˜åŒ–ã€‚
>   (1) `æ¶ˆé™¤äº†å†—ä½™çš„HDFSè¯»å†™` ï¼ˆä¸éœ€è¦è¿‡å¤šåœ°å’Œç£ç›˜äº¤äº’ï¼‰
>   (2) `æ¶ˆé™¤äº†å†—ä½™çš„MapReduceé˜¶æ®µ` 
>   (3) `JVMçš„ä¼˜åŒ–` [MapReduceæ“ä½œï¼Œå¯ä¸ªTaskä¾¿å¯æ¬¡JVMï¼Œè¿›ç¨‹çš„æ“ä½œã€‚Spark çº¿ç¨‹]
>
> [4ç‚¹ç­”æ¡ˆåœ¨è¿™é‡Œ æœ€ä½³Answer](https://www.cnblogs.com/hdc520/p/12588379.html)
> 
> 1.2 [reduceByKey vs groupByKey](https://blog.csdn.net/zongzhiyuan/article/details/49965021)
> åœ¨sparkä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ä¸€åˆ‡çš„æ“ä½œéƒ½æ˜¯åŸºäºRDDçš„ã€‚åœ¨ä½¿ç”¨ä¸­ï¼ŒRDDæœ‰ä¸€ç§éå¸¸ç‰¹æ®Šä¹Ÿæ˜¯éå¸¸å®ç”¨çš„formatâ€”â€”pair RDDï¼Œå³RDDçš„æ¯ä¸€è¡Œæ˜¯ï¼ˆkey, valueï¼‰çš„æ ¼å¼ã€‚è¿™ç§æ ¼å¼å¾ˆåƒPythonçš„å­—å…¸ç±»å‹ï¼Œä¾¿äºé’ˆå¯¹keyè¿›è¡Œä¸€äº›å¤„ç†ã€‚
é’ˆå¯¹pair RDDè¿™æ ·çš„ç‰¹æ®Šå½¢å¼ï¼Œsparkä¸­å®šä¹‰äº†è®¸å¤šæ–¹ä¾¿çš„æ“ä½œï¼Œä»Šå¤©ä¸»è¦ä»‹ç»ä¸€ä¸‹reduceByKeyå’ŒgroupByKeyï¼Œ
>
> groupByKey å½“é‡‡ç”¨groupByKeyæ—¶ï¼Œç”±äºå®ƒä¸æ¥æ”¶å‡½æ•°ï¼Œsparkåªèƒ½å…ˆå°†æ‰€æœ‰çš„é”®å€¼å¯¹(key-value pair)éƒ½ç§»åŠ¨ï¼Œè¿™æ ·çš„åæœæ˜¯é›†ç¾¤èŠ‚ç‚¹ä¹‹é—´çš„å¼€é”€å¾ˆå¤§ï¼Œå¯¼è‡´ä¼ è¾“å»¶æ—¶
>
> ```python
> lines = sc.textFile("/Users/blair/ghome/github/spark3.0/pyspark/spark-src/word_count.text", 2)
>
lines = lines.filter(lambda x: 'New York' in x)
#lines.take(3)

words = lines.flatMap(lambda x: x.split(' '))

wco = words.map(lambda x: (x, 1))

#print(wco.take(5))
from operator import add
word_count = wco.reduceByKey(add)

word_count.collect()
```

> `2022.05.19` English My Job 
> {% image "/images/bi/interview-consecutive-login-sql01.jpg", width="650px", alt="" %}
> [2021 blair Notes](/2021/01/09/bi/dwh-summary-2-interview/) / [2020 Interview Questions - Data Warehouse](https://jishuin.proginn.com/p/763bfbd32925)  
> ```sql
-- 1. how to è¿ç»­ 
select 
  user_id, count(1) cnt
from
  (
    select 
      user_id, 
      login_date, 
      row_number() over(partition by user_id order by login_date) as rn
    from tmp.tmp_last_3_day
  ) t
group by user_id, date_sub(login_date, t.rn)
having count(1) >= 3;
```

> `2022.05.18` shuffleå½¢å¼æœ‰å‡ ç§ï¼Ÿéƒ½åšå“ªäº›ä¼˜åŒ– & English BBC - <å¦‚æœåœ¨ç›¸é‡,æˆ‘ä¼šè®°å¾—ä½ > the good old songs
>
> [sparkåŸºç¡€ä¹‹shuffleæœºåˆ¶ã€åŸç†åˆ†æåŠShuffleçš„ä¼˜åŒ–ï¼ˆå¾ˆå¥½å¾ˆè¯¦ç»†)](https://blog.csdn.net/BigData_Mining/article/details/82622502)
> Shuffleå°±æ˜¯å¯¹æ•°æ®è¿›è¡Œé‡ç»„ï¼Œç”±äºåˆ†å¸ƒå¼è®¡ç®—çš„ç‰¹æ€§å’Œè¦æ±‚ï¼Œåœ¨å®ç°ç»†èŠ‚ä¸Šæ›´åŠ ç¹çå’Œå¤æ‚
> 1. HashShuffleï¼ˆ<=spark1.6,ä¼šäº§ç”Ÿå¾ˆå¤šå°æ–‡ä»¶, Writerè´¹å†…å­˜æ˜“GCï¼‰
> 2. Sort-Based Shuffle (æœ‰å¤šé‡modelï¼Œä¸å±•å¼€)
>  {% image "/images/spark/spark-shuffle-maptask.png", width="650px", alt="" %}
> Transformation æ“ä½œå¦‚:repartitionï¼Œjoinï¼Œcogroupï¼Œä»¥åŠä»»ä½• *By æˆ–è€… *ByKey çš„ Transformation éƒ½éœ€è¦ shuffle æ•°æ®9,åˆç†çš„é€‰ç”¨æ“ä½œå°†é™ä½ shuffle æ“ä½œçš„æˆæœ¬,æé«˜è¿ç®—é€Ÿåº¦


> `2022.05.17` SparkSQL Join & English BBC - è¯¸äº‹ä¸é¡ºçš„ä¸€å¤© The English we We Speak 
>
> 1. join å®ç°æœ‰å‡ ç§å‘¢ï¼Œæºç æœ‰ç ”ç©¶è¿‡å—ï¼Ÿ åº•å±‚æ˜¯æ€ä¹ˆå®ç°çš„
>
> [é¢è¯•å¿…çŸ¥çš„Spark SQLå‡ ç§Joinå®ç° - (streamIterä¸ºå¤§è¡¨ï¼ŒbuildIterä¸ºå°è¡¨)](https://www.51cto.com/article/626552.html)
> sort merge join / broadcast join / hash join


### Spark ç²¾å“

[Sparkä¼šæŠŠæ•°æ®éƒ½è½½å…¥åˆ°å†…å­˜ä¹ˆï¼Ÿ](https://www.jianshu.com/p/b70fe63a77a8)

> Shuffleä¸è¿‡æ˜¯å·å·çš„å¸®ä½ åŠ ä¸Šäº†ä¸ªç±»ä¼¼saveAsLocalDiskFileçš„åŠ¨ä½œã€‚ç„¶è€Œï¼Œå†™ç£ç›˜æ˜¯ä¸€ä¸ªé«˜æ˜‚çš„åŠ¨ä½œã€‚æ‰€ä»¥æˆ‘ä»¬å°½å¯èƒ½çš„æŠŠæ•°æ®å…ˆæ”¾åˆ°å†…å­˜ï¼Œå†æ‰¹é‡å†™åˆ°æ–‡ä»¶é‡Œï¼Œè¿˜æœ‰è¯»ç£ç›˜æ–‡ä»¶ä¹Ÿæ˜¯ç»™è´¹å†…å­˜çš„åŠ¨ä½œã€‚
>
> **Cache/Persistæ„å‘³ç€ä»€ä¹ˆï¼Ÿ**
> 
> å…¶å®å°±æ˜¯ç»™æŸä¸ªStageåŠ ä¸Šäº†ä¸€ä¸ªsaveAsMemoryBlockFileçš„åŠ¨ä½œ, Sparkå…è®¸ä½ æŠŠä¸­é—´ç»“æœæ”¾å†…å­˜é‡Œ.

[Author çŸ¥ä¹](https://www.zhihu.com/people/allwefantasy/posts)

[èåœå§: Is the ByteDance interview difficult and how should you deal with it?](https://www.zhihu.com/question/339135205/answer/1178925849)

### 1. skill dismantling

> 1.1 åº•å±‚åŸç†ï¼Œæºç ç†è§£ã€‚
> 1.2 é¡¹ç›®ç›¸å…³ï¼Œéš¾ç‚¹ï¼Œä»“åº“å»ºæ¨¡
> 1.3 show **sql**

**sparkå’Œhive**ï¼š

3. æ˜¯é€šè¿‡ä»€ä¹ˆç®¡ç†shuffleä¸­çš„å†…å­˜ï¼Œç£ç›˜ çš„

4. è®²è®²è°“è¯ä¸‹æ¨ï¼Ÿ

5. full outer joinåŸç†

6. sparkä¸ºä»€ä¹ˆæ¯”hiveå¿«

7. è®²è®²sparksqlä¼˜åŒ–

8. è®²è®²RDD, DAG, Stage

9. è¯´è¯´groupByKey, reduceByKey

10. sparkæ˜¯æ€ä¹ˆè¯»å–æ–‡ä»¶,åˆ†ç‰‡çš„ï¼Ÿ
11. æœ‰æ²¡æœ‰é‡åˆ°è¿‡sparkè¯»å–æ–‡ä»¶ï¼Œæœ‰ä¸€äº›taskç©ºè·‘çš„ç°è±¡ï¼Ÿ
12. çª—å£å‡½æ•°ä¸­ å‡ ä¸ªrankå‡½æ•°æœ‰å•¥ä¸åŒï¼ˆsparkã€hiveä¸­çª—å£å‡½æ•°å®ç°åŸç†å¤ç›˜ Hive sqlçª—å£å‡½æ•°æºç åˆ†æ sparksqlæ¯”hivesqlä¼˜åŒ–çš„ç‚¹ï¼ˆçª—å£å‡½æ•°ï¼‰ï¼‰parquetæ–‡ä»¶å’Œorcæ–‡ä»¶æœ‰å•¥ä¸åŒmr shuffle æ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿå…·ä½“åŸç†æ˜¯ä»€ä¹ˆï¼Ÿè·Ÿsparkæœ‰ä»€ä¹ˆä¸åŒï¼Ÿè®²è®²hive sqlä¼˜åŒ–hive æ•°æ®å€¾æ–œå‚æ•°åŸç†è®²è®²sparkå†…å­˜æ¨¡å‹ï¼Ÿï¼ˆä»ä¸€ä¸ªsqlä»»åŠ¡ç†è§£sparkå†…å­˜æ¨¡å‹ ï¼‰

**2. Show SQL**

> å°±ä¼šé—®è¿˜æœ‰æ²¡æœ‰æ›´ä¼˜åŒ–çš„æ–¹å¼ï¼Ÿ
> çª—å£å‡½æ•°ï¼Œgroupingsets cubeè¿™äº› éƒ½ä¼šç”¨åˆ°ã€‚æœ‰å¥½å¤šæ˜¯è®¡ç®—æ»‘åŠ¨çš„é‚£ç§
> è¿™ä¸ªsql åœ¨hiveä¸­èµ·å‡ ä¸ªjobï¼Œä¸ºä»€ä¹ˆæ˜¯è¿™ä¹ˆå‡ ä¸ªjobï¼Ÿ

3. Be confident and positive

4. Interview tips (overcoming nervousness)

5. last last last last

## 1. Spark Summary

- [1.1 Spark Summary 1 - Basic knowledge](/2020/10/31/spark/spark-summary-1-basic-questions/)

- [1.2 Spark - troubleshooting](/2021/01/21/spark/spark-summary-3-trouble-shooting/)

- [1.3 [è½¬] Spark å¤šä¸ªStageæ‰§è¡Œæ˜¯ä¸²è¡Œæ‰§è¡Œçš„ä¹ˆï¼Ÿ](https://www.jianshu.com/p/5fe79b67ea00)

- [1.4 [è½¬] Sparkå®½ä¾èµ–å’Œçª„ä¾èµ–æ·±åº¦å‰–æ](https://www.jianshu.com/p/736a4e628f0f)

{% image "/images/spark/spark-summary-1.3-stage-rdd-partition-join.webp", width="600px", alt="Spark Stages" %}

> DataFrameæ˜¯spark1.3.0ç‰ˆæœ¬æå‡ºæ¥çš„
> DataSetæ˜¯spark1.6.0ç‰ˆæœ¬æå‡ºæ¥çš„
> ä½†åœ¨ spark2.0 ç‰ˆæœ¬ä¸­ï¼ŒDataFrameå’ŒDataSetåˆå¹¶ä¸ºDataSet.

## 2. SparkSQL

- [2.1 SparkSQL åº•å±‚å®ç°åŸç†](/2020/10/17/spark/sparkSQL-all-knowleage/)

- [2.2 SparkSQL ä»å…¥é—¨åˆ°è°ƒä¼˜](/2021/02/03/spark/spark-summary-5-sql-optimization-indoor/)

- [2.3 Spark SQLå‡ ç§Joinå®ç°](/2020/09/26/spark/spark-sql-join-core/)

- [2.4 SparkSQL Multidimensional Analysis - Group by](/2021/02/10/spark/spark-summary-6-sql-optimization-cube-group-by/)

## 3. Spark Basic

- [3.1 Spark Introduce 1](/2016/06/29/spark/spark-review-1-introduce/)

- [3.2 Spark Intrduce 2 - RDD](/2020/07/07/spark/spark-review-2/)

- [3.3 Spark æ ¸å¿ƒæ¦‚å¿µè¯¦è¿° 2.2](/2020/07/21/spark/spark-aura-2.2-core-introduce/)

- [3.4 Spark Core ä¸­çš„ RDD è¯¦è§£ 3.1](/2020/07/23/spark/spark-aura-3.1-RDD-detail/)

- [3.5 spark åŸºç¡€æ¦‚å¿µå¤ä¹  3.2](/2020/07/27/spark/spark-aura-3.2-spark-basic-summary/)

- [3.6 Spark Task-Commit æµç¨‹è§£æ 4.2](/2020/07/29/spark/spark-aura-4.2-task-commit/)

- [3.7 SparkCore ä¸­çš„å·¥ä½œåŸç† - ä»»åŠ¡æ‰§è¡Œæµç¨‹ 5.1](/2020/07/31/spark/spark-aura-5.1-sparkCore/)

- [3.8 Spark Shuffle Optimize 10 items](/2020/08/12/spark/spark-ma-public-shuffle-optimization/)

- [3.9 SparkSql - ç»“æ„åŒ–æ•°æ®å¤„ç† (ä¸‹)](/2020/08/28/spark/spark-aura-9.2-SparkSql/)

- [3.10 Spark Chap 7 å†…å­˜æ¨¡å‹å’Œèµ„æºè°ƒä¼˜](/2020/10/19/spark/spark-aura-7.1-memory/)

- [3.11 Spark Practice](/2021/01/06/spark/python-spark-practice/)

- [3.12 Spark - Data Skew Advanced](/2021/01/27/spark/spark-summary-4-data-skew/)

## 4. Spark Review

- [4.1 Spark Tutorials 1 - Introduceã€Ecosysmã€Featuresã€Shell Commands](/2020/09/19/spark/Spark-Tutorials-Part1/)

- [4.2 Spark Tutorials 2 - SparkContextã€Stageã€Executorã€RDD](/2020/09/19/spark/Spark-Tutorials-Part2/)

- [4.3 Spark Tutorials 3 - RDD](/2020/09/25/spark/Spark-Tutorials-Part3/)

## 5. Spark Interview

- [5.1 RDDã€DataFrameå’ŒDataSetçš„åŒºåˆ«](/2021/01/03/spark/spark-rdd-ds-df/)

```python
class TreeNode:
    def __init__(self, x):
        self.val = x
        self.left = None
        self.right = None

class Solution:
    def isBalanced(self, root: TreeNode) -> bool:
        def maxHigh(root):
            if root == None:
                return 0
            return max(maxHigh(root.left), maxHigh(root.right)) + 1

        if root == None:
            return True
        return abs(maxHigh(root.left) - maxHigh(root.right)) <= 1 and self.isBalanced(root.left) and self.isBalanced(root.right)
```
