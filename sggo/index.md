
## 1. 兴趣挖掘

  - 数据量 sina week 10W+, 4W+
  - ABTest 效果评估
  - 什么 tree
  - depth
  - 特征选择，RF 与 GBDT 区别
  - loss

## 2. Chatbot

 - Attention 区别 (hard or soft)
 - 遇到过什么问题 （model_bucket）
 - 流程
 - BeamSearch
 - MMI
 - PPL 的意义

## 3. 评分卡

评分卡主要有三种，申请评分卡、行为评分卡、催收评分卡

申请评分卡要求最为严格，也最为重要，可解释性也要求最强，一般用 LR

**分箱的好处：**

> 列表内容离散化后的特征对异常数据有很强的鲁棒性.
> 
> 离散化后可以进行特征交叉，由 M+N 个变量变为 M\*N 个变量，进一步引入非线性，提升表达能力；
> 
> 列表内容特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
> 
> 将所有变量变换到相似的尺度上。

 - LR 如何处理数据不平衡 ?   答： 好坏样本34：1， 有时候关注坏用本个数， 好样本欠采样等.
 - GBDT 与Xgboost 区别 ?  答： [RF、GBDT、XGBoost 区别](https://zhuanlan.zhihu.com/p/34679467)
 - GBDT 如何判断特征重要度？ 答： 特征j的全局重要度通过特征j在单颗树中的重要度的平均值来衡量
 - 如何判断你的模型是否过拟合 ? 以及过拟合的处理方式？ 答：[画 learning_curve](https://blog.csdn.net/aliceyangxi1987/article/details/73598857)
 - Info Gain vs Info Gain ratio vs Gini vs CART.. 
 
> Info Gain =Entropy(S) - Entropy(S|“阴晴”) 最大的特征. 
> Info Gain ratio 减少信息增益方法对取值数较多的特征的影响。(可减少过拟合，这对某特征取值过多的一惩罚)
> Gini 是介于0~1之间的数，0-完全相等，1-完全不相等；
 
 - LR 分析的变量 & GBDT 分析的变量分别是多少？  26维，84维
 - 变量如何分箱？ IV 值的计算。 卡方分箱和woe编码进行转换

> 信息熵，代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。
> 
> 交叉熵，用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力值。

Reference

> [Python三大评分卡之行为评分卡](https://zhuanlan.zhihu.com/p/34370741)
> 
> [玩转逻辑回归之金融评分卡模型](https://zhuanlan.zhihu.com/p/36539125)
> 
> [拍拍贷教你如何用GBDT做评分卡](http://www.sfinst.com/?p=1389)

**评估指标ks：**

等分 10 份，两条洛伦兹曲线， TPR 与 FPR 的差值. 好坏客户的区程度.

### 3.1 数据获取

**数据情况：**

> 23W+ 去掉一些灰用户，剩余 
>
> M1标准： 好/坏: 13W+ 坏 4K
>
> 30~50 : 1 都是正常的

### 3.2 EDA 

> Exploratory Data Analysis,EDA**

> 每个字段的缺失值情况、异常值情况、平均值、中位数、最大值、最小值、分布情况等

### 3.3 数据预处理

> 数据清洗，变量分箱和 WOE 编码三个步骤

1. 缺失值太多
2. 非数值变量多 (emp_title..)
3. id, member_id 等
4. loan_amnt != df.funded_amnt
5. 空值填充为 0
6. 带 % 的浮点，去掉 %

> 好坏比是 34:1 是非常难以处理的样本了 

#### 3.3.1 数据清洗

> 缺失值太多

#### 3.3.2 变量分箱

- 对连续变量进行分段离散化；
- 将多状态的离散变量进行合并，减少离散变量的状态数。

> 分箱方法很多，最常见的方法之一： Merge分箱中的 Chimerge 分箱.

> **Merge 分箱**
>
> Chimerge 其基本思想是如果两个相邻的区间具有类似的类分布，则这两个区间合并；
否则，它们应保持分开。Chimerge通常采用**卡方值**来衡量两相邻区间的类分布情况。
>
> - 连续值按升序排列，离散值先转化为坏客户的比率，然后再按升序排列；
> - 为了减少计算量，对于状态数大于某一阈值 (建议为100) 的变量，利用等频分箱进行粗分箱。
> - 若有缺失值，则缺失值单独作为一个分箱。

**卡方值的意义：**

> 卡方值仅仅只是一个中间过程，通过卡方值计算出p值，p值才是我们最重要需要的。p小于0.05意味着存在显著差异。
> 
> 卡方值是非参数检验中的一个统计量，主要用于非参数统计分析中。它的作用是检验数据的相关性。如果卡方值的显著性（即SIG.）小于0.05，说明两个变量是显著相关的。

**总结一下特征分箱的优势**：

> 1. 特征分箱可以有效处理特征中的缺失值和异常值。
> 2. 特征分箱后，数据和模型会更稳定。
> 3. 特征分箱可以简化逻辑回归模型，降低模型过拟合的风险，提高模型的泛化能力。
> 4. 将所有特征统一变换为类别型变量。
> 5. 分箱后变量才可以使用标准的评分卡格式，即对不同的分段进行评分。
> 6. 分箱后降低模型运算复杂度，提升模型运算速度，对后后期生产上线较为友好

> 上一步：数据预处理——缺失值、异常值、重复值处理
>
> 下一步：变量显著性检验——计算 WOE、IV

在LR中，单变量离散化为N个哑变量后，每个哑变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；

[变量分箱实践](https://zhuanlan.zhihu.com/p/52312186)
[One-Hot编码与哑变量](http://www.jiehuozhe.com/article/3)
[方差、标准差和均方根误差的区别总结](https://blog.csdn.net/zengxiantao1994/article/details/77855644)
[基于卡方分箱的评分卡建模](https://www.cnblogs.com/wzdLY/p/9649101.html)

> A箱
> (26.2-19)\*(26.2-19) / 26.2  -> **(26.2 是 正类的期望频数， 19 是真实频数)**
>
> (16.8-24)\*(16.8-24) / 16.8 (16.8 是 负类的期望频数)
>
> B箱
> (26.8-34)\*(26.8-34) / 26.8 (26.8 是 正类的期望频数)
> (17.2-10)\*(17.2-10) / 17.2 (17.2 是 负类的期望频数)
>
> 先建立原假设：A、B两种疗法没有区别。根据卡方值的计算公式，计算：卡方值=10.01。
>
> 方差: 来描述变量与均值的偏离程度

#### 3.3.3 WOE编码 (weight of evidence)

将离散变量转化为连续变量。WOE编码是评分卡模型常用的编码方式。

> WOE也可以理解为当前分箱中坏客户和好客户的比值，和所有样本中这个比值的差异

> 当分箱中坏客户和好客户的比例等于随机坏客户和好客户的比值时，说明这个分箱没有预测能力，即WOE=0。 (WOE为0，说明该箱出的特征对结果没有区分度)

实际上WOE编码相当于把分箱后的特征**从非线性可分映射到近似线性**可分的空间内。

![](https://pic2.zhimg.com/80/v2-f9641d365b592361e541b4d5458ebf2d_hd.jpg)

总结一下WOE编码的优势：

> 1. 可提升模型的预测效果
> 2. 将自变量规范到同一尺度上
> 3. WOE能反映自变量取值的贡献情况
> 4. 有利于对变量的每个分箱进行评分
> 5. 转化为连续变量之后，便于分析变量与变量之间的相关性
> 6. 与独热向量编码相比，可以保证变量的完整性，同时避免稀疏矩阵和维度灾难


#### 3.3.4 IV 值

IV称为信息价值(information value)，自变量的IV值越大，表示自变量的预测能力越强。

![](https://www.zhihu.com/equation?tex=IV_i+%3D%28%5Cfrac%7B%5C%23B_i%7D%7B%5C%23B_T%7D-%5Cfrac%7B%5C%23G_i%7D%7B%5C%23G_T%7D%29+%2A+log+%28%5Cfrac%7B%5C%23B_i%2F%5C%23B_T%7D%7B%5C%23G_i%2F%5C%23G_T%7D%29%3D%28%5Cfrac%7B%5C%23B_i%7D%7B%5C%23B_T%7D-%5Cfrac%7B%5C%23G_i%7D%7B%5C%23G_T%7D%29+%2A+WOE_i+%5C%5C)

变量对应的IV值为所有分箱对应的 IV 值之和：

![](https://www.zhihu.com/equation?tex=IV+%3D+%5Csum%5Climits_i%5En+IV_i+%5C%5C)

IV排序后，选择IV>0.02的变量，共58个变量IV>0.02

#### 3.3.5 多变量分析

保留相关性低于阈值0.6的变量，剩余27个变量

**为什么要进行相关性分析？**

> 理想状态下，系数权重会有无数种取法，使系数权重变得无法解释，导致变量的每个分段的得分也有无数种取法（后面我们会发现变量中不同分段的评分会用到变量的系数）

**总结一下变量筛选的意义：**

1. 剔除跟目标变量不太相关的特征
2. 消除由于线性相关的变量，避免特征冗余
3. 减轻后期验证、部署、监控的负担
4. 保证变量的可解释性

#### 3.3.6 显著性分析

删除P值不显著的变量，剩余12个变量了。

特征相关度筛选

```py
cor = df.corr()
cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array
cor = cor.stack()
cor[(cor > 0.55) | (cor < -0.55)] # 特征相关度筛选
```

**GBDT, GBRT, Xgboost, RF grid search**

```py
param_grid = {
    'learning_rate': [0.1, 0.05, 0.02, 0.01],
    'max_depth': [1,2,3,4],
    'min_samples_split': [50,100,200,400],
    'n_estimators': [100,200,400,800]
}
```

**特征重要度：**

```
# Top Ten， GBRT top 10 的参数有哪些，可以作为参考， GBRT 不仅是一个分类器，还能帮你筛选变量
# 比如 feature_importance = 0 的话，那么下一次 这个特征，你就去掉就可以了
# 重新用模型GBRT重新跑，或者用其他模型LR跑
# 或者其他的 RF 也可以帮助你做特征筛选
```

最后选择出 84 个变量， 然后在放到不同的模型中做训练，在 ensemble 应该效果还是不错的。

> 为什么要用回归，不用分类，其实我们在做分类器的过程中，大部分用回归的算法，效果好一些，具体为什么，我有点忘记了

**特征不稳定的，不可以作为入模变量：**
    
> 挑选变量的时候，开始每个月我一直在看它的均值和方差的变化是否在容忍的范围内，超过50%舍

**模型更新周期：**

> 信贷产品比较长的话，2个月更新一次比较好，贷款周期短的话，周更新都可以， 月更新

## 4. 文本分类

 - FastText 多分类
 - RCNN & RNN 最大池化层
 - RNN 与 LSTM 与 GRU 区别
 - ELMO-Like

最好的方法：

### 4.1 ELMO-Like 腾讯词向量

腾讯词向量 + 自己\*128 + 自己BiGRU + 2*BiGRU

> 一次输入一个Batch=128条评论，20个属性都4分类成功， 1 个 epoch， 1200秒=20多分钟
> 
> 每个评论一个 epoch 输入1次， 参数共享

### 4.2 Fastext

利用训练集，来训练 **20 个** 4分类 分类器, 训练 15 分钟

```py
sk_clf = FirstColFtClassifier(lr=learning_rate, epoch=epoch,
                              wordNgrams=word_ngrams,
                              minCount=min_count, verbose=2)
sk_clf.fit(train_data_format, train_label)
```

验证集，计算 F1

```
    for column in columns[2:]:
        true_label = np.asarray(validate_data_df[column])
        classifier = classifier_dict[column]
        pred_label = classifier.predict(validata_data_format).astype(int)
        f1_score = get_f1_score(true_label, pred_label)
        f1_score_dict[column] = f1_score

    f1_score = np.mean(list(f1_score_dict.values()))
```

> min_count设置为2貌似也有一些负向影响， word_ngrams 2， epoch 10 .

### 4.3 RCNN

这个看似不重要，其实确实很重要的点。一开我以为 padding 的最大长度取整个评论平均的长度的2倍差不多就可以啦(对于char level 而言，max_length 取 400左右)，但是会发现效果上不去，当时将 max_length 改为 1000 之后，macro f-score提示明显，我个人认为是在多分类问题中，那些长度很长的评论可能会有部分属于那些样本数很少的类别，padding过短会导致这些长评论无法被正确划分。


