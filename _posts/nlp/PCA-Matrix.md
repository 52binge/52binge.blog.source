---
title: 线性代数的本质 - 系列合集
date: 2020-03-11 22:00:21
categories: nlp
tags: BERT
---

[【官方双语/合集】线性代数的本质 - 系列合集](https://www.bilibili.com/video/av6731067/?p=14)

<!-- more -->


## 01. 向量究竟是什么？

## 02. 线性组合、张成的空间与基等线性代数基础概念

## 03. Matrics as linear transformations 矩阵与线性变换

矩阵乘法的意义就是特定的向量转换 (不同坐标系的向量转换)

<img src="/images/nlp/matrix/liner-03-1.png" width="550" alt="Are you ready?"/>

## 04. Matrix multiplication as composition

## 09. 基变换


<!--<img src="/images/nlp/matrix/matrix-0.png" width="600" alt="Are you ready?"/>
-->

<img src="/images/nlp/matrix/matrix-1.png" width="550" alt="Are you ready?"/>

Jennifer's 坐标系的基向量，是另外的样子. (在我们的坐标系中的表示)

<img src="/images/nlp/matrix/av6731067-1.png" width="550" alt="Are you ready?"/>

矩阵乘法的意义就是特定的向量转换 (不同坐标系的向量转换)

<img src="/images/nlp/matrix/av6731067-2.png" width="550" alt="Are you ready?"/>

Jennifer's 坐标系的 vector 与 我们坐标系的 vector 的互相转换：

<img src="/images/nlp/matrix/av6731067-3.png" width="550" alt="Are you ready?"/>

<img src="/images/nlp/matrix/av6731067-4.png" width="550" alt="Are you ready?"/>


## Reference

- [【简化数据】奇异值分解(SVD)](https://blog.csdn.net/u012162613/article/details/42214205)
- [线性代数的"基"情](http://www.heibanke.com/2016/04/08/svd_demo/)
- [A1.1:奇异值分解到底分解了啥（SVD](https://zhuanlan.zhihu.com/p/69651700)

