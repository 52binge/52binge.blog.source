---
title: NLP 中的 Transformer 和 BERT（not finish）
toc: true
date: 2019-04-08 11:00:21
categories: nlp
tags: BERT
---

**2018.10** google 发布 **BERT** 模型. 引爆整个AI圈的 NLP 模型. 在 NLP领域 刷新 11 项记录.

**BERT** 其实是 language_encoder，把输入的 sentence 或 paragraph 转成 feature_vector（embedding）.

**Paper**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

**BERT** 的创新点在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的 NLP 任务.

<!-- more -->

## 1. NLP 的发展



## Reference

- [AINLP BERT相关论文、文章和代码资源汇总][6]
- [自然语言处理中的Transformer和BERT][1]
- [【NLP】Google BERT详解][7]
- [放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较][2]
- [RNN和LSTM弱！爆！了！注意力模型才是王道][3]
- [NLP-Attention模型][4]
- [NLP突破性成果 BERT 模型详细解读][5]

[1]: https://zhuanlan.zhihu.com/p/53099098
[2]: https://zhuanlan.zhihu.com/p/54743941
[3]: https://zhuanlan.zhihu.com/p/36331888
[4]: https://zhuanlan.zhihu.com/p/29402703
[5]: https://zhuanlan.zhihu.com/p/46997268
[6]: https://zhuanlan.zhihu.com/p/50717786
[7]: https://zhuanlan.zhihu.com/p/46652512
