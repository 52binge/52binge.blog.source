---
title: 朴素贝叶斯
toc: true
date: 2017-08-09 07:50:21
categories: NLP
tags: nlp
description: Naive Bayes 
mathjax: true
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<style>
img {
        display: block !important;
        width: 600px;
        margin-left: 100px !important;
}
</style>


## 1. 引言

贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级 NLP 模型也可以从它演化而来。因此，学习贝叶斯方法，是研究 NLP 问题的一个非常好的切入口。

## 2. 贝叶斯公式

贝叶斯公式就一行：

$$
P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}
$$

它其实是由以下的联合概率公式推导出来：

$$
P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y)
$$

其中 $P(Y)$ 叫做先验概率， $P(Y|X)$ 叫做后验概率， $P(Y,X)$ 叫做联合概率。

## 3. 机器学习视角理解贝叶斯公式 

- 把 $X$ 理解成 “**有某 feature**”
- 把 $Y$ 理解成 “**属于某类 label**”

> 一般机器学习为题中都是X=>特征, Y=>结果对吧。
> 在最简单的二分类问题(是与否判定)下，我们将 $Y$ 理解成“属于某类”的标签。

于是贝叶斯公式就变形成了下面的样子:

$$
P(“属于某类”|“具有某特征”)=\frac{P(“具有某特征”|“属于某类”)P(“属于某类”)}{P(“具有某特征”)}
$$

而我们二分类问题的最终目的就是要判断 $P(“属于某类”|“具有某特征”)$ 是否大于1/2就够了。贝叶斯方法把计算 “**具有某特征的条件下属于某类**” 的概率转换成需要计算 “**属于某类的条件下具有某特征**” 的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。

## 4. 垃圾邮件识别

举个栗子 🌰

我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是判断 **P(“垃圾邮件”|“具有某特征”)** 是否大于1/2。

假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：

> “我司可办理正规发票（保真）17%增值税发票点数优惠！”

也就是判断概率 $P(“垃圾邮件”|“我司可办理正规发票（保真）17\%增值税发票点数优惠！”)$ 是否大于1/2。

$$
P = \frac{垃圾邮件中出现这句话的次数}{垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数}
$$

> 咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！

## 5. 分词

一个很悲哀但是很现实的结论： `训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的`。