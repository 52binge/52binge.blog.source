---
title: Ensumble 集成学习小记
toc: true
date: 2018-07-03 17:43:21
categories: machine-learning
tags: [Ensumble]
---

本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站

<!-- more -->

## 1. Ensumble 概念

Ensumble 是通过构建并结合多个`学习器`来完成学习任务。

> 曾经听过一句话，”Feature为主，Ensemble为后”。

Feature 决定了模型效果的上限，而 Ensemble 就是让你更接近这个上限。

> Ensemble 讲究“好而不同”，不同是指模型的学习到的侧重面不一样。
>
> 举个直观的例子，比如数学考试，A的函数题做的比B好，B的几何题做的比A好，那么他们合作完成的分数通常比他们各自单独完成的要高。

> 常见的Ensemble方法有Bagging、Boosting、Stacking、Blending.
> 
> Notes:  Stacking 与 Blending 类似，[区别可参见][1]

**ensumble 中的 好而不同**

> 如何使得集成学习性能比最好的单一学习器更好？
>
> - 准确性
> - 多样性
>
> 好而不同
> 
> 如何产生并结合 “好而不同” 的个体学习器 ?

集成学习研究的核心, 当前按照个体学习器的生成方式划分:

> bagging（及其变体随机森林）— 个体学习器间不存在强依赖关系，可同时生成的并行化方法
> boosting - 个体学习器间存在强依赖关系，必须串行生成的序列化方法

从偏差-方差分解的角度: 

> bagging 关注降低方差
> boosting 关注降低偏差

也可以按照机器学习技法的两张图来理解

{% image "/images/ml/ensumble/under_over.png", width="750px" %}

第一幅图(对应boosting)可看作进行了一个特征转换来防止欠拟合，第二幅图(对应bagging)则进行了一个正则化来防止过拟合

## 2. Boosting

> 首先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权组合.

### 2.1 adaboost

> 推导方式，基于“加性模型”，即基于基学习器的线性组合
> 如何在每一轮修改样本分布
> - 重赋权法：在每一轮训练中，根据样本分布为每一个训练样本重新赋予一个权重（比如《机器学习实战》一书中就是利用这种方法构建提升树算法，通过修改权重来计算每一轮的损失）
> - 重采样法：利用重采样的训练集来对基学习器进行训练–>重启动：避免训练过程过早停止
 
Notes:

> 西瓜书上的算法还提到训练的每一轮开始都要检查当前学习器是否比随机猜测好，若条件不满足则抛弃当前学习器，这种情形可能会导致学习过程未达到T轮即停止，所以有重采样的方法来进行重启动避免出现此种情况；但是另一方面《统计学习方法》以及《机器学习实战》中的算法并未有这条判断语句；
> 
> 《机器学习》一书中提到，从统计学的出发认为AdaBoost实质上是基于加性模型（后续指出这一视角阐释的是一个与AdaBoost很相似的过程而非其本身），以类似牛顿迭代法来优化指数损失函数，通过将迭代优化过程替换为其他优化方法产生了GradientBoosting、LPBoost等；而这里也提到每一种变体针对不同的问题（噪声、数据不平衡等）而拥有不同的权重更新规则.

### 2.2 特点

> 从bias-variance分解的角度来看，Boosting主要关注降低 bias，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成（与bagging不同）

Notes: boosting 对于噪音数据较为敏感

## 3. Bagging (Bootstrap aggregating )

出发点依然是“好而不同”

> - “不同” — 不同基学习器基于不同的样本子集
> - “好” — 使用相互有交叠的采样子集

### 3.1 工作机制

基于自助采样法（bootstrap sampling）— “有放回地全抽”

> 从训练集从进行**子抽样组成每个基模型所需要的子训练集**，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！

### 3.2 优点（相对于boosting）

- 高效 - 训练一个bagging集成与直接使用基学习器算法训练一个学习器的复杂度同阶
- baggign能不经修改地用于多分类、回归任务（标准AdaBoosting只能适用于二分类任务）
- 包外估计——自助采样过程中剩余的样本可以作为验证集来对泛化性能进行“包外估计”

### 3.3 特点

> 从偏差-方差角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用明显.

## 4. Random Forest 随机森林

Bagging 的一个扩展变体

### 4.1 概述

- 以决策树为基学习器构建Bagging集成
- 在决策树的训练过程中引入了随机属性选择

> 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在随机森林中，对基决策树的每个节点，**先从该节点属性集合中随机选择一个包含k个属性的子集**，然后再从这个子集中选择一个最优属性用于划分，其中`k`控制了随机性的引入程度

### 4.2 特点

> - 简单、易于实现、计算开销小
> - 样本扰动+属性扰动

### 4.3 bagging vs. 随机森林

- 两者收敛性相似，随机森林的起始性能往往相对较差，但会收敛到更低的泛化误差
- 随机森林的训练效率常优于Bagging，主要是决策树划分属性时，原始baggin需要对属性全集进行考虑，而 **RF** 是针对一个子集

## 5. 结合策略

### 5.1 学习器结合的好处

> - 统计的角度：假设空间很大时，可能存在多个假设在训练集上达到同等性能，但学习其可能误选导致泛化性能不佳，结合多个学习器可以减小该风险
> - 计算的角度：降低陷入糟糕局部极小点的风险
> - 表示的角度：结合多个学习器可扩大假设空间，对于真实假设在假设空间之外的情形可能学得更好的近似

### 5.2 策略

**平均法(Averaging)**

> - 简单平均法
> - 加权平均法

**投票法(Voting)**

> - 多数投票法
> - 加权投票法
> - 若按个体学习器输出值类型划分 (硬投票：预测为0/1 | 软投票：相当于对后验概率的一个估计)

**学习法**

> Stacking:先从初始数据集训练出初级学习器，然后生成一个新的数据集用于训练元学习器，在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记；一般使用交叉验证法或留一法来用训练初级学习器未使用的样本来产生元学习器的训练样本

**Notes**: 关于Stacking

> -《机器学习》作者也指出Stacking本身是一种著名的集成方法，且有不少变体和特例，但他这里是作为一种特殊的结合策略看待
> - 关于Stacking的细节详述，[如何在 Kaggle 首战中进入前 10%][4] 以一幅图来说说明5折Stacking的过程
> - 推荐一个Python的实现了Stacking集成的库mlxtend

{% image "/images/ml/ensumble/stacking-1.jpg", width="800px" %}

原作者举了一个5折stacking的例子，基本方法是，

- 每一折取训练集80%的数据训练一个基模型并对剩下的20%的数据进行预测，同时将该模型对测试集做出预测，保留训练子集的预测结果和测试集的预测结果
- 将5折的训练子集预测结果结合起来构成第二层元模型的输入特征进行训练得到元分类器
- 将前面每一折在测试集预测得到的结果取均值作为最终元分类器的预测输入(最终的测试数据)，并使用训练好的元分类器在该数据上作出最终预测


此外[知乎上的一篇文章还提到][zhihu1]

> 可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data

## 6. 多样性

### 6.1 误差-分歧分解

- 集成学习“好而不同”的理论分析，见《机器学习》P185~186
- 寻找 集成泛化误差、个体学习器泛化误差、个体学习器间 的分歧三者之间的关系

### 6.2 多样性度量

> 多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性

> 度量方法 {不合度量、相关系数、Q-统计量、k-统计量} 
> 
> Notes: 目前我还没有做过这种类似测试 😢

### 6.3 多样性增强

如何增强多样性？— 在学习过程中引入随机性

1. 数据样本扰动
2. 输入属性扰动
3. 输出表示扰动
4. 算法参数扰动

**数据样本扰动**

> 给定初始数据集，可从中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，通常基于采样法

**输入属性扰动**

> 从初始属性集中抽取若干个属性子集、基于每个属性子集训练一个基学习器（如随机子空间算法），最后结合

**算法参数扰动**

通常可以通过随机设置不同的参数，从而产生差别较大的个体学习器。

**Notes:**

> - 数据样本扰动中相对的稳定基学习器包括：线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等
> 
> - 输入属性扰动 : 感觉典型的是 Random Forest 就做了这件事.
> 
> - 对于算法参数扰动，与交叉验证做比较，交叉验证常常是在不同参数组合模型里选择最优参数组合模型，而集成则是将这些不同参数组合的模型结合起来，所以集成学习技术的实际计算开销并不比使用单一学习器大很多


[zhihu1]: https://zhuanlan.zhihu.com/p/27424282


[img1]: /images/ml/ensumble/under_over.png
[img2]: /images/ml/ensumble/stacking-1.jpg

## Reference article

- 周志华《机器学习》
- [kaggle-ensembling-guide][1]
- [Bagging, Boosting & Stacking][2]
- [stackexchange及评论区][3]
- [如何在 Kaggle 首战中进入前 10%][4]
- [分分钟带你杀入Kaggle Top 1%][5]
- [懒死骆驼][6]
- 机器学习技法

[1]: https://mlwave.com/kaggle-ensembling-guide/
[2]: https://www.quora.com/What-are-the-differences-between-the-three-commonly-ensemble-learning-techniques-stacking-boosting-and-bagging
[3]: https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning/19053#19053
[4]: https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/
[5]: https://zhuanlan.zhihu.com/p/27424282
[6]: http://izhaoyi.top/2017/07/03/ensemble/