---
title: 数据清理中，处理缺失值的方法是?
date: 2019-08-10 10:06:16
categories: machine-learning
tags: [estimation]
toc: true
---

<img class="img-fancy" src="/images/ml/metric/data-pre-processing.jpg", width="600" border="0px", alt="data pre-processing"%}

<!-- more -->

> 由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。

### 1.1 estimation

> - 估算(estimation)。最简单的办法就是用某个变量的样本 **均值、中位数或众数** 代替无效值和缺失值。
> 
> 这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。

### 1.2 casewise deletion

> - 整例删除(casewise deletion)是剔除含有缺失值的样本。
> 
> 由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。

### 1.3 variable deletion

> - 变量删除(variable deletion)。
> 
> 如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。

### 1.4 pairwise deletion

> - 成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。
> 
> 但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。
>
> 采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。

## 2. GBDT vs XGBoost

**GBDT 原理**

GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型

**XGBoost 原理**

- XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够自动地运用**CPU的多线程进行并行计算**，同时在算法精度上也进行了精度的提高。 
- 由于GBDT在合理的参数设置下，往往要生成一定数量的树才能达到令人满意的准确率，在数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。
 
**GBDT vs XGBoost**

- (1). GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这时XGBoost相当于L1和L2正则化的LR；

- (2). GBDT优化时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；

- (3). XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统GBDT的一个特性；

- (4). shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost 在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；

- (5) 列抽样。XGBoost 借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算；

- (6) 对缺失值的处理。对于特征的值有缺失的样本，XGBoost 还可以自动 学习出它的分裂方向；

- (7) XGBoost 工具支持并行。

Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。

XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。**这个block结构也使得并行成为了可能**，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。


## Reference

- [牛客网 处理缺失值的方法][1]
- [GBDT和XGBoost区别][2]

[1]: https://www.nowcoder.com/questionTerminal/c2d44d84529d426783e9631f92cbaad5
[2]: https://blog.csdn.net/qq_28031525/article/details/70207918
