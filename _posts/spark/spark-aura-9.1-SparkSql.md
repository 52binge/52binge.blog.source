---
title: SparkSql - ç»“æ„åŒ–æ•°æ®å¤„ç† (ä¸Š)
toc: true
date: 2020-08-15 14:07:21
categories: [spark]
tags: [spark]
---

<img src="/images/spark/SparkSql-logo-1.png" width="500" alt="" />

<!-- more -->

# 1. SparkSQL çš„æ•°æ®æºæ“ä½œ

æ‘˜è¦ï¼š

(1). sparksql çš„åŸºç¡€ç†è®º 
(2). sparksql çš„æ ¸å¿ƒç¼–ç¨‹å…¥å£ sparksession
(3). sparksql çš„æ ¸å¿ƒæ•°æ®æŠ½è±¡ dataFrame dataset
(4). sparksql çš„ç¼–ç¨‹å¥—è·¯
  
  spark-shell
  spark-submit
  
sparksqlçš„é‡ç‚¹ï¼š åˆ›å»º dataFrame çš„å‡ ç§æ–¹å¼

  spark1.xï¼š
  
      studentRDD.toDF
      sqlContext.createDataFrame(studentRDD)
      sqlContext.createDataFrame(rowRDD, schema)
      
  spark2.xï¼š
  
      spark.read.format().load()
  
æ•°æ®æºï¼š

      æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼š csv, json, parquet, jdbc
      load
      save
  
## 1.1 [Spark SQL Guide](http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)


```python
df = spark.read.load("examples/src/main/resources/users.parquet")
df.select("name", "favorite_color").write.save("namesAndFavColors.parquet")

# parquet file é»˜è®¤æ˜¯è¿™ç§æ–‡ä»¶æ–¹å¼ load
```

ä¸¾ä¸ªğŸŒ°: spark å®‰è£…è·¯å¾„ examples/src/main/resources :

```bash
# /usr/local/xsoft/spark/examples/src/main/resources [23:06:36]
âœ ll
total 88
drwxr-xr-x@ 5 blair  staff   160B Jun  6 21:34 dir1
-rw-r--r--@ 1 blair  staff   130B Jun  6 21:34 employees.json
-rw-r--r--@ 1 blair  staff   240B Jun  6 21:34 full_user.avsc
-rw-r--r--@ 1 blair  staff   5.7K Jun  6 21:34 kv1.txt
-rw-r--r--@ 1 blair  staff    49B Jun  6 21:34 people.csv
-rw-r--r--@ 1 blair  staff    73B Jun  6 21:34 people.json
-rw-r--r--@ 1 blair  staff    32B Jun  6 21:34 people.txt
-rw-r--r--@ 1 blair  staff   185B Jun  6 21:34 user.avsc
-rw-r--r--@ 1 blair  staff   334B Jun  6 21:34 users.avro
-rw-r--r--@ 1 blair  staff   547B Jun  6 21:34 users.orc
-rw-r--r--@ 1 blair  staff   615B Jun  6 21:34 users.parquet
```

## 1.2 hadoop cmd

```shell
hadoop fs -mkdir -p /sparksql/input/
hadoop fs -put people.txt people.csv people.json /sparksql/input/
```

## 1.3 to load a csv file

```python
df = spark.read.load("examples/src/main/resources/people.csv",
                     format="csv", sep=":", inferSchema="true", header="true")
```

To load a parquet / json file ï¼Œ [write Save Modes](http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes)

```python
df = spark.read.load("examples/src/main/resources/people.json", format="json")
df.select("name", "age").write.save("namesAndAges.parquet", format="parquet")
```

> é»˜è®¤æƒ…å†µä¸‹ä¿å­˜æ•°æ®åˆ° HDFS çš„æ•°æ®æ ¼å¼ï¼š .snappy.parquet
> 
> .snappyï¼š ç»“æœä¿å­˜åˆ° HDFS ä¸Šçš„æ—¶å€™è‡ªåŠ¨å‹ç¼©ï¼š å‹ç¼©ç®—æ³•ï¼š snappy
> 
>   zip / 7z / lzo / gzip / snappy
> 
> .parquetï¼š ç»“æœä½¿ç”¨ä¸€ç§åˆ—å¼æ–‡ä»¶å­˜å‚¨æ ¼å¼ä¿å­˜
> 
>   parquet / rc / orc / row column
> 

## 1.4 JDBC

<img src="/images/spark/sparkSql-aura-9.1.1.jpg" width="800" alt="" />


<img src="/images/spark/sparkSql-aura-9.1.2.jpg" width="800" alt="Jdbc mysql scala" />

[JDBC To Other Databases](http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)

```python
# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
# Loading data from a JDBC source
jdbcDF = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql:dbserver") \
    .option("dbtable", "schema.tablename") \
    .option("user", "username") \
    .option("password", "password") \
    .load()
```

Run SQL on files directly

```python
df = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

**åˆ›å»º dataFrame çš„4ç§æ–¹å¼**

val studentRDD: RDD[Student]

> 1. studentRDD.toDF
> 2. sqlContext.createDtaFrame(studentRDD)
> 3. é€šè¿‡ StructType æ¥æŒ‡å®š Schema
> 4. spark.read.format("json").load("path") ï¼ˆæ ¼å¼åŒ–çš„æ•°æ®ï¼‰


## [1.5 Spark SQL Guide](http://spark.apache.org/docs/latest/sql-getting-started.html)

```python
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
```

**Creating DataFrames**

```python
# spark is an existing SparkSession
df = spark.read.json("examples/src/main/resources/people.json")
# Displays the content of the DataFrame to stdout
df.show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
```

# 2. SparkSQL ç¼–å†™ä»£ç å¤šç§æ–¹å¼



## Reference


- [äº‘è¯¾å ‚ SparkSQL çš„æ•°æ®æºæ“ä½œ](https://study.163.com/course/courseLearn.htm?courseId=1208880821#/learn/video?lessonId=1278316678&courseId=1208880821)
- [å¤§æ•°æ®èµ„æ–™ç¬”è®°æ•´ç†](https://blog.csdn.net/huang66666666/category_9399107.html)