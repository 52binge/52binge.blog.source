---
title: SparkSQL ä»å…¥é—¨åˆ°è°ƒä¼˜
date: 2021-02-03 15:28:21
categories: [spark]
tags: [sparkSQL]
---

{% image "/images/spark/spark-sql-core-structure.jpeg", width="600px", alt="" %}

<!-- more -->

## 1. SparkSQL å‘å±•

<details><summary>SparkSQL å‘å±•å²</summary>

Version | Title Description
:---: | :---
1.0ä»¥å‰ | Shark
Spark-1.1 | SparkSQL(åªæ˜¯æµ‹è¯•æ€§çš„)  SQL
Spark<1.3 | DataFrame ç§°ä¸º SchemaRDD
Spark-1.3 | SparkSQL(æ­£å¼ç‰ˆæœ¬)+Dataframe API
Spark-1.4 | å¢åŠ çª—å£åˆ†æå‡½æ•°
Spark-1.5 | SparkSQL é’¨ä¸è®¡åˆ’ï¼Œ UDF/UDAF
Spark-1.6 | SparkSQL æ‰§è¡Œçš„ sql å¯ä»¥å¢åŠ æ³¨é‡Š
Spark-2.x | SparkSQL+DataFrame+DataSet(æ­£å¼ç‰ˆæœ¬), å¼•å…¥ SparkSession ç»Ÿä¸€ç¼–ç¨‹å…¥å£

No. | &nbsp;&nbsp;Title&nbsp;&nbsp; | SparkSQL ä¸»è¦ç”¨äºè¿›è¡Œ`ç»“æ„åŒ–æ•°æ®çš„å¤„ç†`ã€‚å®ƒæä¾›çš„æœ€æ ¸å¿ƒçš„ç¼–ç¨‹æŠ½è±¡å°±æ˜¯DataFrame.
:---: | :---: | :---
1. | åŸç† | å°† SparkSQL è½¬åŒ–ä¸º RDD ï¼Œç„¶åæäº¤åˆ°é›†ç¾¤æ‰§è¡Œ
2. | ä½œç”¨ | æä¾›ä¸€ä¸ªç¼–ç¨‹æŠ½è±¡ï¼ˆDataFrameï¼‰å¹¶ä¸”ä½œä¸ºåˆ†å¸ƒå¼SQLæŸ¥è¯¢å¼•æ“ã€‚ <br> DataFrame å¯æ®å¾ˆå¤šæºè¿›è¡Œæ„å»ºï¼ŒåŒ…æ‹¬ï¼šç»“æ„åŒ–çš„æ•°æ®æ–‡ä»¶ï¼ŒHiveä¸­çš„è¡¨ï¼ŒMYSQLï¼Œä»¥åŠRDD 
3. | ç‰¹ç‚¹ | 1. å®¹æ˜“æ•´åˆ &nbsp; 2. ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ–¹å¼ &nbsp; 3. å…¼å®¹Hive &nbsp; 4. æ ‡å‡†çš„æ•°æ®è¿æ¥
.. | spark 1.x | SparkContext sc / SqlContext sqlContextS
.. | spark 2.x | SparkContext sc / SparkSession spark

</details>

### 1.1 SparkSession åˆ›å»º

```python
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .enableHiveSupport() // åœ¨classpathä¸­,å¿…é¡»åŠ å…¥ä¸€ä¸ªé…ç½®æ–‡ä»¶ hive-site.xml
    .getOrCreate()
    #   hive url       : æºæ•°æ®åº“åœ¨å“ªé‡Œ
    #   hive warehouse : çœŸæ˜¯æ•°æ®åœ¨å“ªé‡Œ
```

<details><summary>SparkSQL æ•°æ®æŠ½è±¡</summary> 

### 1.2 SparkSQL æ•°æ®æŠ½è±¡

åœ¨Sparkä¸­ï¼ŒDataFrameæ˜¯ä¸€ç§ä»¥RDDä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºä¼ ç»Ÿæ•°æ®åº“ä¸­çš„äºŒç»´è¡¨æ ¼ã€‚DataFrameä¸RDDçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå‰è€…å¸¦æœ‰schemaå…ƒä¿¡æ¯ï¼Œå³DataFrameæ‰€è¡¨ç¤ºçš„äºŒç»´è¡¨æ•°æ®é›†çš„æ¯ä¸€åˆ—éƒ½å¸¦æœ‰åç§°å’Œç±»å‹ã€‚è¿™ä½¿å¾—Spark SQLå¾—ä»¥æ´å¯Ÿæ›´å¤šçš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œå¯¹è—äºDataFrameèƒŒåçš„æ•°æ®æºä»¥åŠä½œç”¨äºDataFrameä¹‹ä¸Šçš„å˜æ¢è¿›è¡Œäº†é’ˆå¯¹æ€§çš„ä¼˜åŒ–ï¼Œæœ€ç»ˆè¾¾åˆ°å¤§å¹…æå‡è¿è¡Œæ—¶æ•ˆç‡çš„ç›®æ ‡ã€‚

{% image "/images/spark/spark-aura-9.3.1.png", width="700px", alt="RDD[Record] == DataFrame == Table, DataFrame æ˜¯ä¸€ç§ç‰¹æ®Šå…³ç³»çš„ Dataset, DataSet[Row] = DataFrame" %}

<img src="/images/spark/spark-aura-9.3.2.png" width="600" alt="SparkSQL å¼•å…¥äº† DataSet, æä¾›äº†ç¼–è¯‘æ—¶ç±»å‹æ£€æµ‹, é¢å‘å¯¹è±¡ç¼–ç¨‹API
" />

[è°ˆè°ˆRDDã€DataFrameã€Datasetçš„åŒºåˆ«å’Œå„è‡ªçš„ä¼˜åŠ¿](https://www.cnblogs.com/starwater/p/6841807.html)

{% image "/images/spark/spark-aura-9.3.3.png", width="700px", alt="" %}

</details>

## 2. SparkSQL ä½¿ç”¨

<details><summary>SparkSQL, DataFrame, StructType, LongType, createOrReplaceTempView</summary> 

```python
# Import types
from pyspark.sql.types import *

# Generate comma delimited data
stringCSVRDD = sc.parallelize(
  [
    (123, 'Katie', 19, 'brown'), 
    (234, 'Michael', 22, 'green'), 
    (345, 'Simone', 23, 'blue')
  ]
)
# Specify schema
schema = StructType(
  [
    StructField("id", LongType(), True),    
    StructField("name", StringType(), True),
    StructField("age", LongType(), True),
    StructField("eyeColor", StringType(), True)
  ]
)
# Apply the schema to the RDD and Create DataFrame
swimmers = spark.createDataFrame(stringCSVRDD, schema)
# Creates a temporary view using the DataFrame
swimmers.createOrReplaceTempView("swimmers")
```

</details>

<details><summary>SparkSQL ä¸€äº›ç½‘ç»œé“¾æ¥</summary> 

No. | Title Author | Desc
--- | --- | ---
2. | Apache SparkSQL | [Spark SQL Guide](http://spark.apache.org/docs/latest/sql-getting-started.html)
3. | SparkSQL | [Spark2.xå­¦ä¹ ç¬”è®°ï¼š14ã€Spark SQLç¨‹åºè®¾è®¡](https://cloud.tencent.com/developer/article/1010936)
4. | DataFrame | [good å®é™…ä¾‹å­æ¼”ç¤ºï¼š Spark2.xå­¦ä¹ ç¬”è®°ï¼š14ã€Spark SQLç¨‹åºè®¾è®¡](https://cloud.tencent.com/developer/article/1010936)
5. | DataFrame | [DataFrameå¸¸ç”¨æ“ä½œï¼ˆDSLé£æ ¼è¯­æ³•ï¼‰ï¼Œsqlé£æ ¼è¯­æ³•](https://blog.csdn.net/toto1297488504/article/details/74907124)
6. | SparkSQL | [Spark SQLé‡ç‚¹çŸ¥è¯†æ€»ç»“](https://cloud.tencent.com/developer/article/1448730)
7. | Create DataFrame | spark1.xï¼šstudentRDD.toDF / sqlContext.createDataFrame(studentRDD) / sqlContext.createDataFrame(rowRDD, schema) <br> spark2.xï¼šspark.read.format().load()

</details>

<details><summary>SparkSQL ç¼–å†™ä»£ç å¤šç§æ–¹å¼</summary> 

```scala
// This code works perfectly from Spark 2.x with Scala 2.11

// Import necessary classes
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}

// Create SparkSession Object, and Here it's spark
val spark: SparkSession = SparkSession.builder.master("local").getOrCreate
val sc = spark.sparkContext // Just used to create test RDDs

// Just used to create test RDDs, Let's an RDD to make it DataFrame
val rdd = sc.parallelize(
  Seq(
    ("first", Array(2.0, 1.0, 2.1, 5.4)),
    ("test", Array(1.5, 0.5, 0.9, 3.7)),
    ("choose", Array(8.0, 2.9, 9.1, 2.5))
  )
)
```

**Method 1:** 

Using SparkSession.createDataFrame(RDD obj).

```scala
val dfWithoutSchema = spark.createDataFrame(rdd)

dfWithoutSchema.show()
+------+--------------------+
|    _1|                  _2|
+------+--------------------+
| first|[2.0, 1.0, 2.1, 5.4]|
|  test|[1.5, 0.5, 0.9, 3.7]|
|choose|[8.0, 2.9, 9.1, 2.5]|
+------+--------------------+
```

**Method 2:**

Using SparkSession.createDataFrame(RDD obj) and specifying column names.

```scala
val dfWithSchema = spark.createDataFrame(rdd).toDF("id", "vals")

dfWithSchema.show()
+------+--------------------+
|    id|                vals|
+------+--------------------+
| first|[2.0, 1.0, 2.1, 5.4]|
|  test|[1.5, 0.5, 0.9, 3.7]|
|choose|[8.0, 2.9, 9.1, 2.5]|
+------+--------------------+
```

**Method 3 (Actual answer to the question)**

This way requires the input rdd should be of type RDD[Row].

```scala
val rowsRdd: RDD[Row] = sc.parallelize(
  Seq(
    Row("first", 2.0, 7.0),
    Row("second", 3.5, 2.5),
    Row("third", 7.0, 5.9)
  )
)
```

create the schema

```scala
val schema = new StructType()
  .add(StructField("id", StringType, true))
  .add(StructField("val1", DoubleType, true))
  .add(StructField("val2", DoubleType, true))
```

Now apply both rowsRdd and schema to createDataFrame()

```scala
val df = spark.createDataFrame(rowsRdd, schema)

df.show()
+------+----+----+
|    id|val1|val2|
+------+----+----+
| first| 2.0| 7.0|
|second| 3.5| 2.5|
| third| 7.0| 5.9|
+------+----+----+
```

[Submitting Applications](http://spark.apache.org/docs/latest/submitting-applications.html)

```bash
spark-submit \
--class com.aura.sparksql.StructTypeDFTest \
--master spark://hadoop02:7077 \
--driver-memory 512M \
--executor-memory 512M \
--total-executor-cores 2 \
/home/hadoop/original-spark232_1805-1.0-SNAPSHOT.jar \
/student_sql_out/
```

</details>

<details><summary>SparkSQL çš„æ•°æ®æºæ“ä½œ: to load a json file</summary> 

```python
df = spark.read.load("examples/src/main/resources/people.json", format="json")
# Displays the content of the DataFrame to stdout
df.show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
df.select("name", "age").write.save("namesAndAges.parquet", format="parquet")
# Run SQL on files directly
df = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

> é»˜è®¤æƒ…å†µä¸‹ä¿å­˜æ•°æ®åˆ° HDFS çš„æ•°æ®æ ¼å¼ï¼š .snappy.parquet
> .snappyï¼š ç»“æœä¿å­˜åˆ° HDFS ä¸Šçš„æ—¶å€™è‡ªåŠ¨å‹ç¼©ï¼š å‹ç¼©ç®—æ³•ï¼š snappy
> .parquetï¼š ç»“æœä½¿ç”¨ä¸€ç§åˆ—å¼æ–‡ä»¶å­˜å‚¨æ ¼å¼ä¿å­˜
> parquet / rc / orc / row column

```python
df = spark.read.load("examples/src/main/resources/users.parquet")
df.select("name", "favorite_color").write.save("namesAndFavColors.parquet")

# parquet file é»˜è®¤æ˜¯è¿™ç§æ–‡ä»¶æ–¹å¼ load
```

ä¸¾ä¸ªğŸŒ°: spark å®‰è£…è·¯å¾„ examples/src/main/resources :

```bash
# /usr/local/xsoft/spark/examples/src/main/resources [23:06:36]
âœ ll
total 88
drwxr-xr-x@ 5 blair  staff   160B Jun  6 21:34 dir1
-rw-r--r--@ 1 blair  staff   130B Jun  6 21:34 employees.json
-rw-r--r--@ 1 blair  staff   240B Jun  6 21:34 full_user.avsc
-rw-r--r--@ 1 blair  staff   5.7K Jun  6 21:34 kv1.txt
-rw-r--r--@ 1 blair  staff    49B Jun  6 21:34 people.csv
-rw-r--r--@ 1 blair  staff    73B Jun  6 21:34 people.json
-rw-r--r--@ 1 blair  staff    32B Jun  6 21:34 people.txt
-rw-r--r--@ 1 blair  staff   185B Jun  6 21:34 user.avsc
-rw-r--r--@ 1 blair  staff   334B Jun  6 21:34 users.avro
-rw-r--r--@ 1 blair  staff   547B Jun  6 21:34 users.orc
-rw-r--r--@ 1 blair  staff   615B Jun  6 21:34 users.parquet
```

</details>

## 3. SparkSQL è°ƒä¼˜

No. | Title Author | Link
--- | --- | --- 
0. | åä¸ºå¼€å‘è€…<br>SparkCore<br><br>çŸ¥ä¹å¤§æ•°æ®<br>SparkSQL | [å¼€å‘è€…æŒ‡å— > ç»„ä»¶æˆåŠŸæ¡ˆä¾‹ > Spark > æ¡ˆä¾‹10ï¼šSpark Coreè°ƒä¼˜ > ç»éªŒæ€»ç»“](https://support-it.huawei.com/docs/zh-cn/fusioninsight-all/developer_guide/zh-cn_topic_0171822910.html) <br><br>[SparkåŸºç¡€ï¼šSpark SQLè°ƒä¼˜](https://zhuanlan.zhihu.com/p/148758337) <br><br> **1. Cache ç¼“å­˜** <br>&nbsp;&nbsp;1.1 spark.catalog.cacheTable("t") æˆ– df.cache() <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Spark SQLä¼šæŠŠéœ€è¦çš„åˆ—å‹ç¼©åç¼“å­˜ï¼Œé¿å…ä½¿ç”¨å’ŒGCçš„å‹åŠ›<br>&nbsp;&nbsp;1.2 spark.sql.inMemoryColumnarStorage.compressed é»˜è®¤true <br> &nbsp;&nbsp;1.3 spark.sql.inMemoryColumnarStorage.batchSize é»˜è®¤10000 <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; æ§åˆ¶åˆ—ç¼“å­˜æ—¶çš„æ•°é‡ï¼Œé¿å…OOMé£é™©ã€‚<br> å¼•ç”³è¦ç‚¹ï¼š è¡Œå¼å­˜å‚¨ & åˆ—å¼å­˜å‚¨ ä¼˜ç¼ºç‚¹ <br> **2. å…¶ä»–é…ç½®** <br>&nbsp;&nbsp;2.1 spark.sql.autoBroadcastJoinThreshold <br>&nbsp;&nbsp;2.2 spark.sql.shuffle.partitions é»˜è®¤200ï¼Œé…ç½®joinå’Œaggçš„æ—¶å€™çš„åˆ†åŒºæ•° <br>&nbsp;&nbsp;2.3 spark.sql.broadcastTimeout é»˜è®¤300ç§’ï¼Œå¹¿æ’­joinæ—¶å¹¿æ’­ç­‰å¾…çš„æ—¶é—´ <br>&nbsp;&nbsp;2.4 spark.sql.files.maxPartitionBytes é»˜è®¤128MBï¼Œå•ä¸ªåˆ†åŒºè¯»å–çš„æœ€å¤§æ–‡ä»¶å¤§å°<br>&nbsp;&nbsp;2.5 spark.sql.files.openCostInBytes <br>parquet.block.size<br>**3. å¹¿æ’­ hash join - BHJ** <br>&nbsp;&nbsp; 3.1 å½“ç³»ç»Ÿ spark.sql.autoBroadcastJoinThreshold åˆ¤æ–­æ»¡è¶³æ¡ä»¶ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨BHJ <br><br>[åä¸ºäº‘Stackå…¨æ™¯å›¾ > å¼€å‘è€…æŒ‡å— > SQLå’ŒDataFrameè°ƒä¼˜ > Spark SQL joinä¼˜åŒ–](https://support-it.huawei.com/docs/zh-cn/fusioninsight-all/developer_guide/zh-cn_topic_0171822912.html) <br><br> <details><summary>sparkä¸ä¼š</summary> æ³¨æ„sparkä¸ä¼šç¡®ä¿æ¯æ¬¡é€‰æ‹©å¹¿æ’­è¡¨éƒ½æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸ºæœ‰çš„åœºæ™¯æ¯”å¦‚ full outer join æ˜¯ä¸æ”¯æŒBHJçš„ã€‚æ‰‹åŠ¨æŒ‡å®šå¹¿æ’­: broadcast(spark.table("src")).join(spark.table("records"), "key").show() </details>
1. | Sparkå­¦ä¹ æŠ€å·§ | [Spark SQLä»å…¥é—¨åˆ°ç²¾é€š](https://cloud.tencent.com/developer/article/1423334?from=article.detail.1033005) <br><br> Data Sourceï¼š json, parquet, jdbc, orc, libsvm, csv, text, Hive è¡¨
<br><br>3. | <br><br>[SparkSQLè¦æ‡‚çš„](https://zhuanlan.zhihu.com/p/336693158) | Spark SQLåœ¨Sparké›†ç¾¤ä¸­æ˜¯å¦‚ä½•æ‰§è¡Œçš„ï¼Ÿ<br><br> 1. Spark SQL è½¬æ¢ä¸ºé€»è¾‘æ‰§è¡Œè®¡åˆ’ <br> 2. Catalyst Optimizerç»„ä»¶ï¼Œå°†é€»è¾‘æ‰§è¡Œè®¡åˆ’è½¬æ¢ä¸ºOptimizedé€»è¾‘æ‰§è¡Œè®¡åˆ’ <br> 3. å°†Optimizedé€»è¾‘æ‰§è¡Œè®¡åˆ’è½¬æ¢ä¸ºç‰©ç†æ‰§è¡Œè®¡åˆ’ <br> 4. Code Generationå¯¹ç‰©ç†æ‰§è¡Œè®¡åˆ’è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå°†ä¸€äº›(éshuffle)æ“ä½œä¸²è”åœ¨ä¸€èµ· <br> 5. ç”ŸæˆJobï¼ˆDAGï¼‰ç”±schedulerè°ƒåº¦åˆ°spark executorsä¸­æ‰§è¡Œ <br><br> é€»è¾‘æ‰§è¡Œè®¡åˆ’å’Œç‰©ç†æ‰§è¡Œè®¡åˆ’çš„åŒºåˆ«ï¼Ÿ <br><br> 1. é€»è¾‘æ‰§è¡Œè®¡åˆ’åªæ˜¯å¯¹SQLè¯­å¥ä¸­ä»¥ä»€ä¹ˆæ ·çš„æ‰§è¡Œé¡ºåºåšä¸€ä¸ªæ•´ä½“æè¿°.<br>2. ç‰©ç†æ‰§è¡Œè®¡åˆ’åŒ…å«å…·ä½“ä»€ä¹ˆæ“ä½œ. ä¾‹å¦‚ï¼šæ˜¯BroadcastJoinã€è¿˜æ˜¯SortMergeJoin..
4. | [SparkSQLä¼˜åŒ–]((https://blog.csdn.net/YQlakers/article/details/68925328?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs)) | 1. spark.sql.codegen=false/true æ¯æ¡æŸ¥è¯¢çš„è¯­å¥åœ¨è¿è¡Œæ—¶ç¼–è¯‘ä¸ºjavaçš„äºŒè¿›åˆ¶ä»£ç  <br> 2. spark.sql.inMemoryColumnStorage.compressed é»˜è®¤false å†…å­˜åˆ—å¼å­˜å‚¨å‹ç¼©<br>3. spark.sql.inMemoryColumnStorage.batchSize = 1000 <br>4. spark.sql.parquet.compressed.codec é»˜è®¤snappy, (uncompressed/gzip/lzo) <br><br>5. spark.catalog.cacheTable("tableName") æˆ– dataFrame.cache()<br> 6. --conf "spark.sql.autoBroadcastJoinThreshold = 50485760" 10 MB -> 50M <br> 7. --conf "spark.sql.shuffle.partitions=200" -> 2000
5. | [SparkSQLè°ƒä¼˜](http://marsishandsome.github.io/SparkSQL-Internal/03-performance-turning/) | http://marsishandsome.github.io/SparkSQL-Internal/03-performance-turning/ <br> 1. å¯¹äºæ•°æ®å€¾æ–œï¼Œé‡‡ç”¨åŠ å…¥éƒ¨åˆ†ä¸­é—´æ­¥éª¤ï¼Œå¦‚èšåˆåcacheï¼Œå…·ä½“æƒ…å†µå…·ä½“åˆ†æï¼›<br>2. é€‚å½“çš„ä½¿ç”¨åºåŒ–æ–¹æ¡ˆä»¥åŠå‹ç¼©æ–¹æ¡ˆï¼› <br>3. å–„äºè§£å†³é‡ç‚¹çŸ›ç›¾ï¼Œå¤šè§‚å¯ŸStageä¸­çš„Taskï¼ŒæŸ¥çœ‹æœ€è€—æ—¶çš„Taskï¼ŒæŸ¥æ‰¾åŸå› å¹¶æ”¹å–„ï¼› <br>4. å¯¹äºjoinæ“ä½œï¼Œä¼˜å…ˆç¼“å­˜è¾ƒå°çš„è¡¨ï¼›<br>5. è¦å¤šæ³¨æ„Stageçš„ç›‘æ§ï¼Œå¤šæ€è€ƒå¦‚ä½•æ‰èƒ½æ›´å¤šçš„Taskä½¿ç”¨PROCESS_LOCALï¼› <br> 6. è¦å¤šæ³¨æ„Storageçš„ç›‘æ§ï¼Œå¤šæ€è€ƒå¦‚ä½•æ‰èƒ½Fraction cachedçš„æ¯”ä¾‹æ›´å¤š

### 1.1 åŸºæœ¬æ“ä½œ

```python
val df = spark.read.json(â€œfile:///opt/meitu/bigdata/src/main/data/people.jsonâ€)
df.show()
import spark.implicits._
df.printSchema()
df.select("name").show()
df.select($"name", $"age" + 1).show()
df.filter($"age" > 21).show()
df.groupBy("age").count().show()
spark.stop()
```

ç¼–ç 

```scala
// spark2+, SparkSession
val spark = SparkSession.builder()
  .config(sparkConf).getOrCreate()
//ä½¿ç”¨hiveå…ƒæ•°æ®
val spark = SparkSession.builder()
  .config(sparkConf).enableHiveSupport().getOrCreate()
spark.sql("select count(*) from student").show()
```

ä½¿ç”¨ createOrReplaceTempView

```scala
val df =spark.read.json("examples/src/main/resources/people.json") 
df.createOrReplaceTempView("people") 
spark.sql("SELECT * FROM people").show()
```

## 4. SparkSQL è¿è¡Œè¿‡ç¨‹

## 5. Catalyst

- [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/#:~:text=DataFrame%20%E2%80%93%20A%20DataFrame%20is%20a,table%20in%20a%20relational%20database.)


No. | Link
--- | --- 
0. | [ã€å¤§æ•°æ®ã€‘SparkSqlè¿æ¥æŸ¥è¯¢ä¸­çš„è°“è¯ä¸‹æ¨å¤„ç†(ä¸€)](https://juejin.cn/post/6844903849405186055)
0. | [ä½¿ç”¨explainåˆ†æSpark SQLä¸­çš„è°“è¯ä¸‹æ¨ï¼Œåˆ—è£å‰ªï¼Œæ˜ å°„ä¸‹æ¨](https://www.jianshu.com/p/5ff225a70510)
1. | [SparkSQL â€“ ä»0åˆ°1è®¤è¯†Catalystï¼ˆè½¬è½½ï¼‰](https://www.cnblogs.com/shishanyuan/p/8456250.html) <br> [æ·±å…¥ç ”ç©¶Spark SQLçš„Catalystä¼˜åŒ–å™¨ï¼ˆåŸåˆ›ç¿»è¯‘ï¼‰](https://www.cnblogs.com/shishanyuan/p/8455786.html)
2. | [Spark SQL Catalystä¼˜åŒ–å™¨](https://www.jianshu.com/p/410c23efb565)
3. | [ã€æ•°æ®åº“å†…æ ¸ã€‘åŸºäºè§„åˆ™ä¼˜åŒ–ä¹‹è°“è¯ä¸‹æ¨](https://blog.csdn.net/Night_ZW/article/details/108338826)

## Reference

No. | Link
--- | --- 
1. | [W3C School - Spark RDD æ“ä½œ](https://www.w3cschool.cn/spark/toxq9ozt.html)
2. |  [W3C School - Spark RDDæŒä¹…åŒ–](https://www.w3cschool.cn/spark/4ied1ozt.html)
3. |  [W3C School - Spark SQLæ€§èƒ½è°ƒä¼˜](https://www.w3cschool.cn/spark/5ruxnozt.html)


- [è®°å½•ä¸€æ¬¡spark sqlçš„ä¼˜åŒ–è¿‡ç¨‹](https://zhuanlan.zhihu.com/p/77614511)
- [good - spark å†…å­˜æº¢å‡ºå¤„ç†](https://www.cnblogs.com/wcgstudy/p/11407607.html)
- [good - sparkå†…å­˜æº¢å‡ºåŠå…¶è§£å†³æ–¹æ¡ˆ](https://www.jianshu.com/p/9b7297626475)
- [SparkSQLè§£å†³æ•°æ®å€¾æ–œå®æˆ˜ä»‹ç»(é€‚ç”¨äºHiveSQL)](https://blog.csdn.net/weixin_44769733/article/details/102710943)
- [ä¼˜åŒ–Sparkä¸­çš„æ•°æ®å€¾æ–œ](https://blog.csdn.net/fengfengzai0101/article/details/103678685)
- [sparkåäº¿æ•°æ®joinä¼˜åŒ–](http://daizuozhuo.github.io/spark-join)
- [Apache Spark åœ¨eBay çš„ä¼˜åŒ–](https://www.sohu.com/a/400827630_315839)
- [å­—èŠ‚è·³åŠ¨åœ¨Spark SQLä¸Šçš„æ ¸å¿ƒä¼˜åŒ–å®è·µ | å­—èŠ‚è·³åŠ¨æŠ€æœ¯æ²™é¾™](https://juejin.cn/post/6844903989557854216)
- [Apache Sparkæºç èµ°è¯»ä¹‹11 -- sqlçš„è§£æä¸æ‰§è¡Œ](https://www.cnblogs.com/hseagle/p/3752917.html)

<br>

- [èƒŒå„ç§SparkSQLè°ƒä¼˜å‚æ•°ï¼Ÿè¿™ä¸ªä¸œè¥¿æ‰æ˜¯SparkSQLå¿…é¡»è¦æ‡‚çš„](https://zhuanlan.zhihu.com/p/336693158)
- [SparkSQLæ€§èƒ½è°ƒä¼˜](https://blog.csdn.net/YQlakers/article/details/68925328?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs)
- [ä¸€äº›å¸¸ç”¨çš„Spark SQLè°ƒä¼˜æŠ€å·§](https://www.cnblogs.com/lestatzhang/p/10611322.html)

<br>

- [SparkSQLè°ƒä¼˜](http://marsishandsome.github.io/SparkSQL-Internal/03-performance-turning/)
- [SparkåŸºç¡€ï¼šSpark SQLè°ƒä¼˜](https://zhuanlan.zhihu.com/p/148758337)
- [spark-sqlè°ƒä¼˜æŠ€å·§ - sparkSQLçš„å‰ä¸–ä»Šç”Ÿ](https://blog.csdn.net/weixin_40035337/article/details/108018058?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)
- [èŠèŠspark-submitçš„å‡ ä¸ªæœ‰ç”¨é€‰é¡¹](https://cloud.tencent.com/developer/article/1150767?from=article.detail.1423334)





