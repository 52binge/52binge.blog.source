---
title: Improving Deep Neural Networks (week3) - è¶…å‚æ•°è°ƒè¯•ã€Batch æ­£åˆ™åŒ– å’Œ ç¨‹åºæ¡†æ¶
toc: true
date: 2018-07-23 20:00:21
categories: deeplearning
tags: deeplearning.ai
mathjax: true
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

Hyperparameter Tuning processã€Normalizing Activations in a network

Fitting Batch Norm into a neural networkã€Why does Batch Norm work?ã€Batch Norm at test time

Softmax regressionã€TensorFlow

<!-- more -->

## 1. Hyperparameter Tuning process

æ­£å¸¸æƒ…å†µæœ‰å¦‚ä¸‹è¶…å‚æ•°:

Hyperparameter | Desc | Importance level
:-------: | :-------: | :-------: 
<font color="red">Î±</font> | æœ€é‡è¦ | 1
<font color="orange">hidden units</font> | | 2
<font color="orange">mini-batch size</font> | | 2
<font color="orange">Î²</font> | | 2
<font color="purple">layers</font> | | 3
<font color="purple">learning rate decay</font> | | 3
$Î²\_1,Î²\_2,Îµ$ | æœ€ä¸é‡è¦ | 4

> é¢œè‰²è¡¨ç¤ºé‡è¦æ€§ï¼Œä»¥åŠè°ƒè¯•è¿‡ç¨‹ä¸­å¯èƒ½ä¼šéœ€è¦ä¿®æ”¹çš„ç¨‹åº¦.

### é‚£ä¹ˆå¦‚ä½•é€‰æ‹©è¶…å‚æ•°çš„å€¼å‘¢ï¼Ÿ:

- é¦–å…ˆæ˜¯ç²—ç•¥åœ°éšæœºåœ°å¯»æ‰¾æœ€ä¼˜å‚æ•°

<img src="/images/deeplearning/C2W3-1_1.png" width="700" />

**å»ºè®®ä½¿ç”¨å›¾å³çš„æ–¹å¼ï¼ŒåŸå› å¦‚ä¸‹ï¼š**

> å¯¹äºå›¾å·¦çš„è¶…å‚æ•°åˆ†å¸ƒè€Œè¨€ï¼Œå¯èƒ½ä¼šä½¿å¾—å‚è€ƒæ€§é™ä½ï¼Œæˆ‘ä»¬å‡è®¾è¶…å‚1æ˜¯å­¦ä¹ ç‡Î±ï¼Œè¶…å‚2æ˜¯Îµï¼Œæ ¹æ®week2ä¸­Adamç®—æ³•çš„ä»‹ç»ï¼Œæˆ‘ä»¬çŸ¥é“Îµçš„ä½œç”¨å‡ ä¹å¯ä»¥å¿½ç•¥ï¼Œæ‰€ä»¥å¯¹äºå›¾å·¦25ä¸­å‚æ•°åˆ†å¸ƒæ¥è¯´ï¼Œå…¶æœ¬è´¨åªæœ‰5ç§å‚æ•°åˆ†å¸ƒã€‚è€Œå³è¾¹åˆ™æ˜¯25ç§éšæœºåˆ†å¸ƒï¼Œæ›´èƒ½å¸®åŠ©æˆ‘ä»¬é€‰æ‹©åˆé€‚çš„è¶…å‚æ•°.

**å…¶æ¬¡åœ¨ä¸Šé¢æ‰¾åˆ°çš„æœ€ä¼˜å‚æ•°åˆ†å¸ƒå‘¨å›´å†éšæœºåœ°å¯»æ‰¾æœ€æœ‰å‚æ•°**

<img src="/images/deeplearning/C2W3-2_1.png" width="700" />

## 2. Using an appropriate scale to pick hyperparameters

ä¸Šä¸€èŠ‚æåˆ°çš„çš„éšæœºé‡‡æ ·è™½ç„¶èƒ½å¸®åŠ©æˆ‘ä»¬å¯»æ‰¾æœ€ä¼˜å‚æ•°åˆ†å¸ƒï¼Œä½†æ˜¯è¿™æœ‰ç‚¹åƒå¤§æµ·æé’ˆï¼Œå¦‚æœèƒ½å¤ŸæŒ‡å‡ºå‚æ•°å–å€¼çš„èŒƒå›´ï¼Œç„¶åå†å»å¯»æ‰¾æœ€ä¼˜çš„å‚æ•°åˆ†å¸ƒå²‚ä¸æ˜¯æ›´åŠ çš„ç¾æ»‹æ»‹ï¼Ÿé‚£å¦‚ä½•ä¸ºè¶…å‚æ•°é€‰æ‹©åˆé€‚çš„èŒƒå›´å‘¢ï¼Ÿ

> $n^{[l]}=50,â€¦â€¦,100$
>
> $layers=2~4$
>
> $Î±=0.0001ï¼Œâ€¦â€¦,1$

æ­¤æ—¶æ³¨æ„: å¦‚æŒ‰ç…§çº¿æ€§åˆ’åˆ†çš„è¯(å¦‚ä¸‹å›¾)ï¼Œé‚£ä¹ˆéšæœºé‡‡æ ·çš„å€¼90%çš„æ•°æ®æ¥è‡ª[0.1,1]è¿™ä¸ªåŒºé—´, è¿™æ˜¾ç„¶ä¸ä¸å¤ªç¬¦åˆéšæœºæ€§.

<img src="/images/deeplearning/C2W3-3_1.png" width="700" />

> æ‰€ä»¥ä¸ºäº†æ”¹è¿›è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å°†åŒºé—´å¯¹æ•°åŒ–æ¥é‡‡æ ·.
> 
> **ä¸¾ä¸ªğŸŒ°ï¼š** æˆ‘ä»¬å°† [0.0001,1] è½¬åŒ–æˆå››ä¸ªåŒºé—´ [0.0001,0.001], [0.001,0.01], [0.01,0.1], [0.1,1], å†è½¬åŒ–æˆå¯¹æ•°å°±æ˜¯ [-4,-3], [-3,-2], [-2,-1], [-1,0].
> 
> ($10^{âˆ’4}=0.0001$ï¼Œå…¶ä»–åŒç†å–æŒ‡æ•°).

ç„¶åæˆ‘ä»¬å¯ä»¥ç”¨Pythonä¸­æä¾›çš„æ–¹æ³•æ¥å®ç°éšæœºé‡‡æ ·ï¼š

```python
r = -4*np.random.rand() # rand()è¡¨ç¤ºåœ¨[0,1]ä¸Šå‡åŒ€é‡‡æ ·, æœ€åçš„é‡‡æ ·åŒºé—´æ˜¯[-4, 0]
a = pow(10, r)
```

<img src="/images/deeplearning/C2W3-4_1.png" width="700" />

**$Î²=0.9,â€¦â€¦,0.999$**

åŒç†è¿™é‡Œä¹Ÿä¸èƒ½ä½¿ç”¨çº¿æ€§è½´æ¥é‡‡æ ·æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ **1-Î²=0.1,â€¦â€¦,0.001** æ¥é—´æ¥é‡‡æ ·ã€‚è½¬åŒ–æˆ [0.1, 0.01],[0.01,0.001], è½¬åŒ–æˆå¯¹æ•°æŒ‡æ•°[-1,-2],[-2,-3]ã€‚

å³: $râˆˆ[-3,-1], 1-Î²=10^r, Î²=1-10^r$

> å½“ Î² æ¥è¿‘ 1 æ—¶, Î² å°±ä¼šå¯¹ç»†å¾®çš„å˜åŒ–å˜å¾—å¾ˆæ•æ„Ÿ.
> 
> for example : 0.999, 0.9995 => 1000 -> 2000
> 
> æ‰€ä»¥ä½ éœ€è¦æ›´åŠ å¯†é›†çš„å–å€¼ï¼Œåœ¨ Î² æ¥è¿‘ 1 çš„æ—¶å€™.

## 3. Hyperparameters tuning in practice: Pandas vs Caviar

<img src="/images/deeplearning/C2W3-5_1.png" width="700" />

**Babysitting one model:**

è¿™ç§æ–¹æ³•é€‚ç”¨äºæœ‰è¶³å¤Ÿçš„æ•°æ®é›†ï¼Œä½†æ˜¯GPUï¼ŒCPUèµ„æºæœ‰é™çš„æƒ…å†µï¼Œæ‰€ä»¥å¯èƒ½åªèƒ½è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åæ¯å¤©å¯¹æ¨¡å‹åšæŸä¸€é¡¹è¶…å‚æ•°çš„ä¿®æ”¹ï¼ŒæŸ¥çœ‹æ•ˆæœæ˜¯å¦å˜å¾—æ›´å¥½.

> ä¾‹å¦‚ç¬¬ä¸€å¤©ä»¤æ‰€æœ‰è¶…å‚æ•°éšæœºåˆå§‹åŒ–ã€‚åˆ°äº†ç¬¬äºŒå¤©å‘ç°æ•ˆæœè¿˜ä¸é”™ï¼Œæ­¤æ—¶å¯ä»¥å»å¢åŠ å­¦ä¹ ç‡(ä¹Ÿå¯ä»¥ä¿®æ”¹å…¶ä»–å‚æ•°)ã€‚â€¦â€¦ï¼Œåˆ°äº†æŸä¸€å¤©åŠ å…¥ä¿®æ”¹äº†mini-batch sizeï¼Œç»“æœæ•ˆæœæ˜æ˜¾å‡å¼±ï¼Œè¿™æ—¶åˆ™éœ€è¦é‡æ–°æ¢å¤åˆ°å‰ä¸€å¤©çš„çŠ¶æ€ã€‚
>
> æ€»çš„æ¥è¯´è¿™ä¸€è¿‡ç¨‹å°±åƒç†ŠçŒ«ä¸€æ ·ï¼Œåªç…§é¡¾ä¸€ä¸ªå®å®ï¼Œå¤šçš„ç…§é¡¾ä¸è¿‡æ¥.

**Train many models in parallel:**

> è¿™ç§æ–¹æ³•é€‚ç”¨äºè´¢å¤§æ°”ç²—çš„æƒ…å†µï¼Œå³å¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œæœ€åé€‰å‡ºæ•ˆæœæœ€å¥½çš„ä¸€ä¸ªå³å¯ã€‚è¿™å°±åƒé±¼å­é…±ä¸€æ ·ï¼Œä¸€ä¸‹ç”Ÿå¤šå¤§ä¸€äº¿çš„å­©å­.

## 4. Normalizing Activations in a network 

ä¸ä»…è¦å½’ä¸€åŒ–è¾“å…¥æ•°æ® **$X$**,éšè—å±‚çš„æ•°æ®ä¹Ÿæ˜¯è¦å½’ä¸€åŒ–çš„. ä¸€èˆ¬æ¥è¯´éšè—å±‚æ•°æ®æœ‰ $Z$ å’Œ $a$ ä¸¤ç§ï¼ŒAndrew Ng æ¨èå½’ä¸€åŒ– **$z$**.

> Batch å½’ä¸€åŒ– ç”± Sergey loffe å’Œ Christian Szegedy ä¸¤ä½ç ”ç©¶è€…åˆ›é€ .

Batch å½’ä¸€åŒ–ï¼Œä¼šä½¿ä½ çš„å‚æ•°æœç´¢å˜å¾—å®¹æ˜“, ä½¿ç¥ç»ç½‘ç»œå¯¹è¶…å‚æ•°çš„æœç´¢æ›´åŠ ç¨³å®š. è¿™æ ·ä¹Ÿä¼šä½¿å¾—ä½ å®¹æ˜“è®­ç»ƒæ·±å±‚ç¥ç»ç½‘ç»œã€‚

**è¾“å…¥æ•°æ® $X$ å½’ä¸€åŒ–æ–¹æ³•:**

$$
Î¼=\frac{1}{m}\sum\_{i}{x^{(i)}}
$$

$$
Ïƒ^2=\frac{1}{m}\sum\_{i}x^{(i)^2}
$$

$$
x=\frac{x-Î¼}{Ïƒ^2}
$$

> m ä¸º mini-batch ä¸­çš„ mï¼Œ è€Œä¸æ˜¯æ•´ä¸ªè®­ç»ƒé›†

**éšè—å±‚æ•°æ®å½’ä¸€åŒ–æ–¹æ³•:**

$$
Î¼=\frac{1}{m}\sum\_{i}{z^{(i)}-Î¼}
$$

$$
Ïƒ^2=\frac{1}{m}\sum\_{i}(z^{(i)^2}-Î¼)^2
$$

$$
z^{(i)}\_{norm}=\frac{z^{(i)}-Î¼}{\sqrt{Ïƒ^2+Îµ}}
$$

ä¸Šé¢çš„å½’ä¸€åŒ–åçš„æ•°æ®$z$éƒ½æ˜¯æœä»å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„ï¼Œæ˜¾ç„¶è¿™æ ·ä¸èƒ½æ»¡è¶³å’±ä»¬çš„éœ€æ±‚ï¼Œæ‰€ä»¥è¿˜éœ€è¦åšè¿›ä¸€æ­¥å¤„ç†ï¼Œå¦‚ä¸‹ï¼š

$$
\tilde{z}^{(i)}=Î³z^{(i)} + Î²
$$

ä¸Šå¼ä¸­çš„ $Î³$ å¯ä»¥è®¾ç½®æ–¹å·®ï¼Œ$Î²$ å¯ä»¥è®¾ç½®å‡å€¼.

> ä½ ä¹Ÿè®¸ä¸æƒ³éšå±‚å•å…ƒå€¼å¿…é¡»æ˜¯å¹³å‡å€¼0 å’Œ æ–¹å·®1, æ¯”å¦‚ä½ æœ‰ä¸€ä¸ª sigmoid å‡½æ•°ï¼Œä½ ä¸æƒ³è®©å®ƒçš„å€¼å®Œå…¨é›†ä¸­åœ¨è¿™é‡Œ, ä½ ä¸æƒ³ä½¿ä»–ä»¬å¹³å‡å€¼å’Œæ–¹å·®ä¸€ç›´æ˜¯0å’Œ1ï¼Œ è¿™æ ·å¯ä»¥æ›´å¥½çš„åˆ©ç”¨éçº¿æ€§çš„ Sigmoid å‡½æ•°ï¼Œ è€Œä¸æ˜¯æ‰€æœ‰å€¼éƒ½é›†ä¸­åœ¨çº¿æ€§çš„åŒºåŸŸ, $Î³$ å’Œ $Î²$ å¯ä»¥ç¡®ä¿æ‰€æœ‰çš„ $Z^{(i)}$ å€¼ï¼Œå¯ä»¥æ˜¯ä½ æƒ³èµ‹äºˆçš„ä»»æ„å€¼. æˆ–è€… å®ƒçš„ä½œç”¨æ˜¯ä¿è¯éšè—çš„å•å…ƒå·²ä½¿å‡å€¼å’Œæ–¹å·®æ ‡å‡†åŒ–. é‚£é‡Œå‡å€¼å’Œæ–¹å·®ç”±ä¸¤å‚æ•°æ§åˆ¶. $Î³$ å’Œ $Î²$ å­¦ä¹ ç®—æ³•å¯ä»¥è®¾ç½®ä¸ºä»»ä½•å€¼. æ‰€ä»¥å®ƒçš„çœŸæ­£ä½œç”¨æ˜¯ä½¿å‡å€¼å’Œæ–¹å·®æ ‡å‡†åŒ–. $Z^{(i)}$ æœ‰å›ºå®šçš„å‡å€¼å’Œæ–¹å·®ï¼Œå‡å€¼å’Œæ–¹å·®å¯ä»¥æ˜¯0å’Œ1ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–å€¼ï¼Œç”± $Î³$ å’Œ $Î²$ ç¡®å®š.
>
> In practiceï¼Œ normlizing $Z^{\[2\]}$ is done much more often.

## 5. Fitting Batch Norm into a neural network

**adding batch Norm to a network**

<img src="/images/deeplearning/C2W3-6_1.png" width="750" />

**working with mini-batches**

ä¸€èˆ¬çš„æ–¹æ³•ä¸­

$$
z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}
$$

åœ¨ä¸Šé¢å½’ä¸€åŒ–æ•°æ®è¿‡ç¨‹ä¸­éœ€è¦å‡å»å‡å€¼ï¼Œæ‰€ä»¥ $b^{[l]}$ è¿™ä¸€é¡¹å¯ä»¥çœç•¥æ‰,æ‰€ä»¥å½’ä¸€åŒ–åæ˜¯

$$
z\_{norm}^{[l]}=w^{[l]}a^{[l-1]}
$$

ä¸ºäº†èƒ½å¤Ÿä½¿æ•°æ®åˆ†å¸ƒæ›´åŠ æ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚ï¼Œå¯ä»¥ç”¨å¦‚ä¸‹å…¬å¼

$$
\tilde{z}^{[l]}=Î³^{[l]}z\_{norm}^{[l]}+Î²^{[l]}
$$

**Implementing gradient descent**

for t= 1,â€¦â€¦,numMinBatches

- è®¡ç®—åŸºäºç¬¬ $t$ æ‰¹æ•°æ®çš„å‰å‘ä¼ æ’­
- åœ¨è®¡ç®—åå‘ä¼ æ’­æ—¶ä½¿ç”¨ $\tilde{z}^{[l]}$, å¾—åˆ° $dw^{[l]},dÎ²^{[l]},dÎ³^{[l]}$
- æ›´æ–°å‚æ•°

$$
w^{[l]}=w^{[l]}-Î±dw^{[l]} \\\\
Î²^{[l]}=Î²^{[l]}-Î±dÎ²^{[l]} \\\\
Î³^{[l]}=Î³^{[l]}-Î±dÎ³^{[l]}
$$

## 6. Why does Batch Norm work?

**åŸå› ä¸€:**

**batch norm** å¯ä»¥ä½¿å¾—æƒé‡æ¯”ä½ çš„ç½‘ç»œæ›´æ»åæˆ–æ›´æ·±å±‚ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£å¯ä»¥çœ‹ä¸‹é¢çš„ä¾‹å­:

<img src="/images/deeplearning/C2W3-7_1.png" width="700" />

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå‡è®¾æˆ‘ä»¬ç°åœ¨è¦è®¡ç®—ç¬¬ä¸‰å±‚éšè—å±‚çš„å€¼ï¼Œå¾ˆæ˜¾ç„¶è¯¥å±‚çš„è®¡ç®—ç»“æœä¾èµ–ç¬¬äºŒå±‚çš„æ•°æ®ï¼Œä½†æ˜¯ç¬¬äºŒå±‚çš„æ•°æ®å¦‚æœæœªå½’ä¸€åŒ–ä¹‹å‰æ˜¯ä¸å¯çŸ¥çš„ï¼Œåˆ†å¸ƒæ˜¯éšæœºçš„ã€‚è€Œå¦‚æœè¿›è¡Œå½’ä¸€åŒ–åï¼Œå³ $\tilde{z}^{\[2\]}=Î³^{\[2\]}z\_{norm}^{\[2\]}+Î²^{\[2\]}$ å¯ä»¥å°†ç¬¬äºŒå±‚æ•°æ®é™åˆ¶ä¸ºå‡å€¼ä¸º $Î²^{\[2\]}$, æ–¹å·®ä¸º $Î³^{\[2\]}$ çš„åˆ†å¸ƒ,æ³¨æ„è¿™ä¸¤ä¸ªå‚æ•°å¹¶ä¸éœ€è¦äººä¸ºè®¾ç½®ï¼Œå®ƒä¼šè‡ªåŠ¨å­¦ä¹ çš„ã€‚æ‰€ä»¥å³ä½¿è¾“å…¥æ•°æ®åƒå˜ä¸‡åŒ–ï¼Œä½†æ˜¯ç»è¿‡å½’ä¸€åŒ–ååˆ†å¸ƒéƒ½æ˜¯å¯ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚çš„ï¼Œæ›´ç®€å•åœ°è¯´å°±æ˜¯å½’ä¸€åŒ–æ•°æ®å¯ä»¥å‡å¼±å‰å±‚å‚æ•°çš„ä½œç”¨ä¸åå±‚å‚æ•°çš„ä½œç”¨ä¹‹é—´çš„è”ç³»ï¼Œå®ƒä½¿å¾—ç½‘ç»œæ¯å±‚éƒ½å¯ä»¥è‡ªå·±å­¦ä¹ ã€‚

**åŸå› äºŒ:**

batch norm å¥æ•ˆçš„å¦ä¸€ä¸ªåŸå› åˆ™æ˜¯å®ƒå…·æœ‰æ­£åˆ™åŒ–çš„æ•ˆæœã€‚å…¶ä¸dropoutæœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ï¼Œæˆ‘ä»¬çŸ¥é“dropoutä¼šéšæœºçš„ä¸¢æ‰ä¸€äº›èŠ‚ç‚¹ï¼Œå³æ•°æ®ï¼Œè¿™æ ·ä½¿å¾—æ¨¡å‹è®­ç»ƒä¸ä¼šè¿‡åˆ†ä¾èµ–æŸä¸€ä¸ªèŠ‚ç‚¹æˆ–æŸä¸€å±‚æ•°æ®ã€‚batch normä¹Ÿæ˜¯å¦‚æ­¤ï¼Œé€šè¿‡å½’ä¸€åŒ–ä½¿å¾—å„å±‚ä¹‹é—´çš„ä¾èµ–æ€§é™ä½ï¼Œå¹¶ä¸”ä¼šç»™æ¯å±‚éƒ½åŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»è€Œè¾¾åˆ°æ­£åˆ™åŒ–çš„ç›®çš„


> Batch å®ƒé™åˆ¶äº†åœ¨å‰å±‚çš„å‚æ•°æ›´æ–°ï¼Œä¼šå½±å“æ•°å€¼åˆ†å¸ƒçš„ç¨‹åº¦ï¼Œç¬¬ä¸‰å±‚çœ‹åˆ°çš„è¿™ç§æƒ…å†µï¼Œå› æ­¤å¾—å­¦ä¹ . **batch å½’ä¸€åŒ–å‡å°‘äº†è¾“å…¥å€¼æ”¹å˜çš„é—®é¢˜**, å®ƒçš„ç¡®æ˜¯è¿™äº›å€¼å˜å¾—æ›´ç¨³å®š. ç¥ç»ç½‘ç»œçš„ä¹‹åå±‚å°±ä¼šæœ‰æ›´åšå®çš„åŸºç¡€. å³ä½¿è¾“å…¥åˆ†å¸ƒæ”¹å˜äº†ä¸€äº›ï¼Œå®ƒä¼šæ”¹å˜å¾—æ›´å°‘ï¼Œå®ƒåšçš„æ˜¯ å½“å‰å±‚ä¿æŒå­¦ä¹ ï¼Œå½“å±‚æ”¹å˜æ—¶ï¼Œè¿«ä½¿åå±‚, é€‚åº”çš„ç¨‹åº¦å‡å°‘äº†ï¼Œä½ å¯ä»¥è¿™æ ·æƒ³ï¼Œå®ƒå‡å¼±äº†å‰å±‚å‚æ•°çš„ä½œç”¨ï¼Œä¸åå±‚å‚æ•°çš„ä½œç”¨ä¹‹é—´çš„è”ç³»ï¼Œå®ƒä½¿å¾—ç½‘ç»œæ¯å±‚éƒ½å¯ä»¥è‡ªå·±å­¦ä¹ . ç¨ç¨ç‹¬ç«‹äºå…¶å®ƒå±‚ï¼Œè¿™æœ‰åŠ©äºåŠ é€Ÿæ•´ä¸ªç½‘ç»œçš„å­¦ä¹ .
> 
> batch norm ä¸­æœ‰ä¸€ä¸ªä½œç”¨ï¼Œå¯ä»¥èµ·åˆ°è½»å¾® æ­£åˆ™åŒ– çš„ä½œç”¨. (å› ä¸ºæ·»åŠ çš„å™ªéŸ³å¾ˆå¾®å°ï¼Œæ‰€ä»¥å¹¶ä¸æ˜¯å·¨å¤§çš„æ­£åˆ™åŒ–)ï¼Œ ä½ å¯ä»¥å°† batch norm å’Œ dropout ä¸€èµ·ä½¿ç”¨.
> 
> dropout, ä½ åº”ç”¨è¾ƒå¤§çš„ mini-batch æ¯”å¦‚ 512ï¼Œé‚£ä¹ˆå¯ä»¥å‡å°‘å™ªéŸ³ä¹Ÿ, å› æ­¤å‡å°‘äº†æ­£åˆ™åŒ–çš„æ•ˆæœ. è¿™æ˜¯ dropout çš„ä¸€ä¸ªå¥‡æ€ªçš„æ€§è´¨.
> 
> batch norm æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–çš„è§„åˆ™ï¼Œè€Œä¸è¦æŠŠå®ƒå½“åšç›®çš„. ä½†æ˜¯æœ‰æ—¶å€™ï¼Œå®ƒä¼šå¯¹ä½ çš„ç®—æ³•æœ‰é¢å¤–çš„æœŸæœ›å’ŒéæœŸæœ›æ•ˆæœ.
> 
> batch norm ä¸€æ¬¡åªèƒ½å¤„ç† ä¸€ä¸ª mini-batch çš„æ•°æ®. å®ƒåœ¨ mini-batch ä¸Šè®¡ç®—æœŸæœ›ä¸æ–¹å·®.

## 7. Batch Norm at test time

å‰é¢æåˆ°çš„ batch norm éƒ½æ˜¯åŸºäºè®­ç»ƒé›†çš„ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸Šï¼Œæœ‰æ—¶å€™å¯èƒ½æˆ‘ä»¬çš„æµ‹è¯•æ•°æ®å¾ˆå°‘ï¼Œä¾‹å¦‚åªæœ‰1ä¸ªï¼Œåœ¨è¿™ä¸ªæ—¶å€™è¿›è¡Œå½’ä¸€åŒ–åˆ™æ˜¾å¾—æ²¡å¤šå¤§æ„ä¹‰äº†ã€‚é‚£ä¹ˆè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå‡å€¼$Î¼$ å’Œ æ–¹å·®$Ïƒ^2$è¯¥å¦‚ä½•ç¡®å®šå‘¢ï¼Ÿ

> æ–¹æ³•è¿˜æ˜¯æœ‰çš„ï¼Œè€Œä¸”å·²ç»åœ¨ä¸Šé¢æåˆ°è¿‡äº†, å°±æ˜¯ç¬¬ä¸‰èŠ‚æ‰€ä»‹ç»çš„**æŒ‡æ•°åŠ æƒå¹³å‡**å•¦ï¼ŒåŸç†æ˜¯ç±»ä¼¼çš„

å‡è®¾ä¸€å…±æœ‰å¦‚ä¸‹ $x^{\{1\}},x^{\{2\}},â€¦â€¦,x^{\{5000\}}$ çš„æ‰¹é‡æ•°æ®ï¼Œæ¯ç»„mini-batch éƒ½å¾—åˆ°äº†å¯¹åº”çš„å‡å€¼$Î¼$, (æ–¹å·®åŒç†ï¼Œä¸è¯¦ç»†è¯´æ˜äº†)ï¼Œå³ $Î¼^{\{1\}},Î¼^{\{2\}},â€¦â€¦,Î¼^{\{5000\}}$, å¦‚æœæµ‹è¯•é›†æ•°æ®å¾ˆå°‘ï¼Œé‚£ä¹ˆå°±å¯ä»¥ä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡çš„æ–¹æ³•æ¥å¾—åˆ°æµ‹è¯•é›†çš„å‡å€¼å’Œæ–¹å·®ã€‚

ä¹‹åå°±æ ¹æ®**æŒ‡æ•°åŠ æƒå¹³å‡**è®¡ç®—å¾—åˆ°çš„å€¼æ¥è®¡ç®—å½’ä¸€åŒ–åçš„è¾“å…¥å€¼å³å¯.

<img src="/images/deeplearning/C2W3-8_1.png" width="750" />

> **Andrew Ng è¯­å½•:**
>
> å¦‚æœå°†ä½ çš„ç¥ç»ç½‘ç»œç”¨äº**æµ‹è¯•**ï¼Œä½ éœ€è¦å•ç‹¬ä¼°ç®— $Î¼$ å’Œ $Ïƒ^2$, åœ¨å…¸å‹çš„ Batch å½’ä¸€åŒ–è¿ç”¨ä¸­ï¼Œä½ éœ€è¦ç”¨ä¸€ä¸ªæŒ‡æ•°åŠ æƒå¹³å‡æ¥ä¼°ç®—ï¼Œæ•´ä¸ªå¹³å‡æ•°è¦†ç›–äº†æ‰€æœ‰çš„ mini-batch .
>
> $$
z^{(i)}\_{norm}=\frac{z^{(i)}-Î¼}{\sqrt{Ïƒ^2+Îµ}}
$$
>
> ä¸Šä¸ªå¼å­ $z^{(i)}\_{norm}$ ä¸­çš„ï¼Œ$Î¼$, $Ïƒ^2$ æ˜¯ç±»ä¼¼åŠ æƒå¹³å‡å‡ºæ¥çš„å€¼.
> 
> æ³¨æ„ï¼šæµ‹è¯•é›†çš„å‡å€¼å’Œæ–¹å·®ç”Ÿæˆçš„æ–¹å¼ä¸ä¸€å®šéå¾—æ˜¯ä¸Šé¢æåˆ°çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œä¹Ÿå¯ä»¥æ˜¯ç®€å•ç²—æš´çš„è®¡ç®—æ‰€æœ‰è®­ç»ƒé›†çš„å‡å€¼å’Œæ–¹å·®ï¼Œè§†é¢‘ä¸­Andrew Ngè¯´è¿™ä¹Ÿæ˜¯å¯è¡Œçš„.

## 8. Softmax regression

<img src="/images/deeplearning/C2W3-9_1.png" width="750" />

å‡è®¾ç¬¬$l$å±‚æœ‰ $z^{\[l\]}=w^{\[l\]}a^{\[l-1\]}+b^{\[l\]}$,æ¿€æ´»å‡½æ•°ä¸º $a^{\[l\]}=\frac{e^{z^{\[l\]}}}{\sum\_{j=1}^{n\_l}e^{z^{\[l\]}\_j}}$

è¯¥èŠ‚è§†é¢‘ä¸­ Andrew Ng å¹¶æ²¡æœ‰å¾ˆè¯¦ç»†çš„ä»‹ç»softmaxçš„åŸç†å’Œå…¬å¼æ¨å¯¼ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥æˆ³å¦‚ä¸‹é“¾æ¥è¿›è¡Œè¿›ä¸€æ­¥äº†è§£ï¼š

- [ufldl:Softmaxå›å½’][y8]
- [softmaxå…¬å¼æ¨å¯¼&ç®—æ³•å®ç°][y9]

[y8]: http://ufldl.stanford.edu/wiki/index.php/Softmaxå›å½’
[y9]: https://zhuanlan.zhihu.com/p/21485970

## 10. æ·±åº¦å­¦ä¹ æ¡†æ¶ & TensorFlow

## 7. æœ¬å‘¨å†…å®¹å›é¡¾

- æ”¹å–„æ·±å±‚ç¥ç»ç½‘ç»œï¼šè¶…å‚æ•°è°ƒè¯•ã€æ­£åˆ™åŒ–

## 8. Reference

- [ç½‘æ˜“äº‘è¯¾å ‚ - deeplearning][1]
- [deeplearning.ai ä¸“é¡¹è¯¾ç¨‹äºŒç¬¬ä¸€å‘¨][2]
- [Coursera - Deep Learning Specialization][3]
- [DeepLearning.aiå­¦ä¹ ç¬”è®°æ±‡æ€»][4]

[1]: https://study.163.com/my#/smarts
[2]: https://daniellaah.github.io/2017/deeplearning-ai-Improving-Deep-Neural-Networks-week1.html
[3]: https://www.coursera.org/specializations/deep-learning
[4]: http://www.cnblogs.com/marsggbo/p/7470989.html

