---
title: Chatbot Research 2 - NLPåŸºç¡€çŸ¥è¯†å›é¡¾
toc: true
date: 2018-08-12 14:00:21
categories: chatbot
tags: NLTK
mathjax: true
---

[NLTK](http://pypi.python.org/pypi/nltk) Pythonä¸Šè‘—åçš„è‡ªç„¶è¯­è¨€å¤„ç†åº“ã€‚ 

- è‡ªå¸¦è¯­æ–™åº“ï¼Œè¯æ€§åˆ†ç±»åº“ï¼Œ è¿˜æœ‰å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒã€‚

<!-- more -->

**æ–‡æœ¬å¤„ç†æµç¨‹**

- åˆ†è¯
- å½’ä¸€åŒ– 
- åœæ­¢è¯

**NLPç»å…¸ä¸‰æ¡ˆä¾‹ **

- æƒ…æ„Ÿåˆ†æ
- æ–‡æœ¬ç›¸ä¼¼åº¦
- æ–‡æœ¬åˆ†ç±»

> æ–¯å¦ä½› CoreNLP (è‹±æ–‡ã€ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­)

## 1. NLTK

1. Python è‘—åçš„è‡ªç„¶è¯­è¨€å¤„ç†åº“
2. è‡ªå¸¦è¯­æ–™åº“ã€è¯æ€§åˆ†ç±»åº“
3. è‡ªå¸¦åˆ†ç±»ã€åˆ†è¯ ç­‰åŠŸèƒ½
4. å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ

```bash
sudo pip install -U nltk
sudo pip install -U numpy
```

```py
import nltk
nltk.download()
```

<!--<img src="/images/chatbot/chatbot-2_1.png" width="400" />
-->
æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸ

```python
>>> python
>>> import nltk
```

## 2. åŠŸèƒ½ä¸€è§ˆè¡¨

<img src="/images/chatbot/chatbot-2_2.png" width="800" />

**NLTK è‡ªå¸¦è¯­æ–™åº“**

<img src="/images/chatbot/chatbot-2_3.png" width="600" />

## 3. Tokenize

**tokenize æŠŠé•¿å¥å­æ‹†æˆæœ‰â€œæ„ä¹‰â€çš„å°éƒ¨ä»¶**

```py
>>> import nltk>>> sentence = â€œhello, world">>> tokens = nltk.word_tokenize(sentence)>>> tokens['hello', â€˜,', 'world']
```

**ä¸­æ–‡åˆ†è¯ jieba** (ç¬¬ä¸‰æ–¹å¼€æºåº“)

<img src="/images/chatbot/chatbot-2_4.png" width="780" />

**æœ‰æ—¶å€™tokenizeæ²¡é‚£ä¹ˆç®€å•**

> æ¯”å¦‚ç¤¾äº¤ç½‘ç»œä¸Š,è¿™äº›ä¹±ä¸ƒå…«ç³Ÿçš„ä¸åˆè¯­æ³•ä¸åˆæ­£å¸¸é€»è¾‘çš„è¯­è¨€å¾ˆå¤š:æ‹¯æ•‘ @æŸäºº, è¡¨æƒ…ç¬¦å·, URL, #è¯é¢˜ç¬¦å·

<img src="/images/chatbot/chatbot-2_5.png" width="650" />

**ç¤¾äº¤ç½‘ç»œè¯­è¨€çš„tokenize :**

```py
from nltk.tokenize import word_tokenizetweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'print(word_tokenize(tweet))
```

<img src="/images/chatbot/chatbot-2_6.png" width="880" />

[æ­£åˆ™è¡¨è¾¾å¼å¯¹ç…§è¡¨](http://www.regexlab.com/zh/regref.htm)

## 4. è¯å½¢å½’ä¸€åŒ–

Inflectionå˜åŒ–: walk => walking => walked ä¸å½±å“è¯æ€§
derivation å¼•ç”³: nation (noun) => national (adjective) => nationalize (verb) å½±å“è¯æ€§

Stemming è¯å¹²æå–:ä¸€èˆ¬æ¥è¯´ï¼Œå°±æ˜¯æŠŠä¸å½±å“è¯æ€§çš„inflectionçš„å°å°¾å·´ç æ‰

> walking ç ing = walk 
> walked ç ed = walk

Lemmatization è¯å½¢å½’ä¸€:æŠŠå„ç§ç±»å‹çš„è¯çš„å˜å½¢ï¼Œéƒ½å½’ä¸ºä¸€ä¸ªå½¢å¼ 

> went å½’ä¸€ = go
> are å½’ä¸€ = be

### 4.1 Stemming

1. PorterStemmer
2. SnowballStemmer
3. LancasterStemmer
4. PorterStemmer

**PorterStemmer**

```py
>>> from nltk.stem.porter import PorterStemmer>>> porter_stemmer = PorterStemmer()>>> porter_stemmer.stem(â€˜maximumâ€™)uâ€™maximumâ€™>>> porter_stemmer.stem(â€˜presumablyâ€™)uâ€™presumâ€™
```

**LancasterStemmer**

```py
>>> from nltk.stem.lancaster import LancasterStemmer
>>> lancaster_stemmer = LancasterStemmer()
>>> lancaster_stemmer.stem(â€˜maximumâ€™)
â€˜maximâ€™
>>> lancaster_stemmer.stem(â€˜presumablyâ€™)
â€˜presumâ€™
```
### 4.2 Lemmatization 

è¯å½¢å½’ä¸€ï¼š æŠŠå„ç§ç±»å‹çš„è¯çš„å˜å½¢,éƒ½å½’ä¸ºä¸€ä¸ªå½¢å¼ 

went å½’ä¸€ = go  are å½’ä¸€ = be

```py
>>> from nltk.stem import WordNetLemmatizer>>> wordnet_lemmatizer = WordNetLemmatizer()>>> wordnet_lemmatizer.lemmatize(â€˜dogsâ€™)uâ€™dogâ€™>>> wordnet_lemmatizer.lemmatize(â€˜churchesâ€™)uâ€™churchâ€™>>> wordnet_lemmatizer.lemmatize(â€˜aardwolvesâ€™)uâ€™aardwolfâ€™
```

NLTKæ›´å¥½åœ°å®ç°Lemma

```py
# â½Šæœ¨æœ‰POS Tag,é»˜è®¤æ˜¯NN åè¯>>> wordnet_lemmatizer.lemmatize(â€˜areâ€™) # â€˜areâ€™>>> wordnet_lemmatizer.lemmatize(â€˜isâ€™)  # â€˜isâ€™# åŠ ä¸ŠPOS Tag>>> wordnet_lemmatizer.lemmatize(â€˜isâ€™, pos=â€™vâ€™)  # uâ€™beâ€™>>> wordnet_lemmatizer.lemmatize(â€˜areâ€™, pos=â€™vâ€™) # uâ€™beâ€™
```

### 4.3 NLTK æ ‡æ³¨ POS Tag

```py
>>> import nltk>>> text = nltk.word_tokenize('what does the fox say')>>> text['what', 'does', 'the', 'fox', 'say']>>> nltk.pos_tag(text)[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]
```

## 5. Stopwords

ä¸€åƒä¸ª He æœ‰ä¸€åƒç§æŒ‡ä»£
ä¸€åƒä¸ª The æœ‰ä¸€åƒç§æŒ‡äº‹ å¯¹äºæ³¨é‡ç†è§£æ–‡æœ¬ã€æ„æ€ã€çš„åº”ç”¨åœºæ™¯æ¥è¯´

å…¨ä½“stopwordsåˆ—è¡¨ http://www.ranks.nl/stopwords

**å»é™¤ stopwords**

> é¦–å…ˆè®°å¾—åœ¨consoleé‡Œé¢ä¸‹è½½ä¸€ä¸‹è¯åº“ï¼Œ æˆ–è€… nltk.download(â€˜stopwordsâ€™)

```py
import nltk
from nltk.corpus import stopwords 

# å…ˆtokenâ¼€ä¸€æŠŠ,å¾—åˆ°â¼€ä¸€ä¸ªword_list

word_list = nltk.word_tokenize('what does the fox say')

# ç„¶åfilterâ¼€ä¸€æŠŠ

filtered_words = [word for word in word_list if word not in stopwords.words('english')]

filtered_words
```

## 6. æ–‡æœ¬é¢„å¤„ç†æµæ°´çº¿

ä¸€æ¡typicalçš„æ–‡æœ¬é¢„å¤„ç†æµæ°´çº¿

<img src="/images/chatbot/chatbot-2_8.png" width="320" />

æ–‡æœ¬é¢„å¤„ç†è®©æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆ?

<img src="/images/chatbot/chatbot-2_9.png" width="320" />

## 7. NLPä¸Šçš„ç»å…¸åº”ç”¨

æƒ…æ„Ÿåˆ†æ ã€ æ–‡æœ¬ç›¸ä¼¼åº¦ ã€ æ–‡æœ¬åˆ†ç±»

### 7.1 æƒ…æ„Ÿåˆ†æ

<img src="/images/chatbot/chatbot-2_10.png" width="720" />

å“ªäº›æ˜¯å¤¸ä½ ï¼Ÿå“ªäº›æ˜¯é»‘ä½ ï¼Ÿ

**MLçš„æƒ…æ„Ÿåˆ†æ**

```py
from nltk.classify import NaiveBayesClassifier# éšâ¼¿æ‰‹é€ ç‚¹è®­ç»ƒé›†s1 = 'this is a good book's2 = 'this is a awesome book's3 = 'this is a bad book's4 = 'this is a terrible book'def preprocess(s): # Func: å¥â¼¦å¤„ç†# è¿™â¾¥ç®€å•çš„â½¤äº†ï¦ºsplit(), æŠŠå¥å­ä¸­æ¯ä¸ªå•è¯åˆ†å¼€ # æ˜¾ç„¶ è¿˜æœ‰æ›´å¤šçš„processing methodå¯ä»¥â½¤ 
    return {word: True for word in s.lower().split()}# returné•¿è¿™æ ·:# {'this': True, 'is':True, 'a':True, 'good':True, 'book':True} # å…¶ä¸­, å‰â¼€ä¸€ä¸ªå«fname, å¯¹åº”æ¯ä¸ªå‡ºç°çš„æ–‡æœ¬å•è¯;# åâ¼€ä¸€ä¸ªå«fval, æŒ‡çš„æ˜¯æ¯ä¸ªâ½‚æ–‡æœ¬å•è¯å¯¹åº”çš„å€¼ã€‚# è¿™â¾¥é‡Œï§©æˆ‘ä»¬â½¤ç”¨æœ€ç®€å•çš„True,æ¥è¡¨ç¤º,è¿™ä¸ªè¯ã€å‡ºç°åœ¨å½“å‰çš„å¥ï¤†â¼¦å­ä¸­ã€çš„æ„ä¹‰ã€‚# å½“ç„¶å•¦, æˆ‘ä»¬ä»¥åå¯ä»¥å‡çº§è¿™ä¸ªâ½…æ–¹ç¨‹, è®©å®ƒå¸¦æœ‰æ›´ï¤åŠ â½œç‰›é€¼çš„fval, æ¯”å¦‚ word2vec


# æŠŠè®­ç»ƒé›†ç»™åšæˆæ ‡å‡†å½¢å¼training_data = [[preprocess(s1), 'pos'],                 [preprocess(s2), 'pos'],                 [preprocess(s3), 'neg'],                 [preprocess(s4), 'neg']]# å–‚ç»™modelåƒmodel = NaiveBayesClassifier.train(training_data)# æ‰“å‡ºç»“æœprint(model.classify(preprocess('this is a good book')))
```

### 7.2 æ–‡æœ¬ç›¸ä¼¼åº¦

**Frequency é¢‘ç‡ç»Ÿè®¡**

```py
import nltk
from nltk import FreqDist
# åšä¸ªè¯åº“å…ˆ
corpus = 'this is my sentence ' \
           'this is my life ' \
           'this is the day'
# éšä¾¿tokenizeä¸€ä¸‹, æ˜¾ç„¶, æ­£å¦‚ä¸Šæ–‡æåˆ°,
# è¿™ï§©å¯ä»¥æ ¹æ®éœ€è¦åšä»»ä½•çš„preprocessing:  stopwords, lemma, stemming, etc.
# å€Ÿâ½¤NLTKçš„FreqDistç»Ÿè®¡â¼€ä¸‹â½‚å­—å‡ºç°çš„é¢‘ç‡ fdist = FreqDist(tokens)
# å®ƒå°±ç±»ä¼¼äºâ¼€ä¸ªDic,  å¸¦ä¸ŠæŸä¸ªå•è¯, å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªæ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°

tokens = nltk.word_tokenize(corpus) 
print(tokens)
# å¾—åˆ° token å¥½çš„ word list
# ['this', 'is', 'my', 'sentence',
# 'this', 'is', 'my', 'life', 'this', # 'is', 'the', 'day']
# å€Ÿâ½¤ NLTK çš„ FreqDist ç»Ÿè®¡â¼€ä¸‹æ–‡å­—å‡ºç°çš„é¢‘ç‡
fdist = FreqDist(tokens)

# å®ƒå°±ç±»ä¼¼äºâ¼€ä¸€ä¸ªDict
# å¸¦ä¸ŠæŸä¸ªå•è¯, å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªæ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°
print(fdist['is']) #3

# å¥½, æ­¤åˆ», æˆ‘ä»¬å¯ä»¥æŠŠæœ€å¸¸â½¤çš„50ä¸ªå•è¯æ‹¿å‡ºæ¥ 
standard_freq_vector = fdist.most_common(50) 
size = len(standard_freq_vector) 
print "size: %s" % (size)
print(standard_freq_vector)
# [('is', 3), ('this', 3), ('my', 2),
# ('the', 1), ('day', 1), ('sentence', 1),
# ('life', 1)

# Func: æŒ‰ç…§å‡ºç°é¢‘ç‡â¼¤å°, è®°å½•ä¸‹æ¯â¼€ä¸ªå•è¯çš„ä½ç½® 
def position_lookup(v):
    res = {}
    counter = 0
    for word in v:
        res[word[0]] = counter
        counter += 1
    return res
# æŠŠæ ‡å‡†çš„å•è¯ä½ç½®è®°å½•ä¸‹æ¥
standard_position_dict = position_lookup(standard_freq_vector) 
print(standard_position_dict)
# å¾—åˆ°â¼€ä¸ªä½ç½®å¯¹ç…§è¡¨
# {'is': 0, 'the': 3, 'day': 4, 'this': 1, 'sentence': 5, 'my': 2, 'life': 6}

# è¿™æ—¶, å¦‚æœæˆ‘ä»¬æœ‰ä¸ªæ–°å¥å­:
sentence = 'this is cool'
# å…ˆæ–°å»ºâ¼€ä¸ªè·Ÿæˆ‘ä»¬çš„æ ‡å‡†vectoråŒæ ·â¼¤å°çš„å‘é‡ 
freq_vector = [0] * size
# ç®€å•çš„Preprocessing
tokens = nltk.word_tokenize(sentence) # å¯¹äºè¿™ä¸ªæ–°å¥â¼¦â¾¥çš„æ¯ä¸€ä¸ªå•è¯
for word in tokens:
    try:
    # å¦‚æœåœ¨æˆ‘ä»¬çš„è¯åº“ï§©å‡ºç°è¿‡
    # é‚£ä¹ˆå°±åœ¨"æ ‡å‡†ä½ç½®"ä¸Š+1 
        freq_vector[standard_position_dict[word]] += 1
    except KeyError: # å¦‚æœæ˜¯ä¸ªæ–°è¯, å°± passæ‰
        continue
print(freq_vector)   # [1, 1, 0, 0, 0, 0, 0]
# [1, 1, 0, 0, 0, 0, 0]
# ç¬¬ä¸€ä¸ªä½ç½®ä»£è¡¨ is, å‡ºç°äº†ä¸€æ¬¡
# ç¬¬äºŒä¸ªä½ç½®ä»£è¡¨ this, å‡ºç°äº†ä¸€æ¬¡ 
# åé¢éƒ½æ²¡æœ‰
```

### 7.3 æ–‡æœ¬åˆ†ç±» tf-idf

TF: Term Frequency, è¡¡é‡ä¸€ä¸ªtermåœ¨æ–‡æ¡£ä¸­å‡ºç°å¾—æœ‰å¤šé¢‘ç¹ã€‚

$$
F(t) = (tå‡ºç°åœ¨æ–‡æ¡£ä¸­çš„æ¬¡æ•°) / (æ–‡æ¡£ä¸­çš„termæ€»æ•°)
$$IDF: Inverse Document Frequency, è¡¡é‡ä¸€ä¸ªtermæœ‰å¤šé‡è¦ã€‚ æœ‰äº›è¯å‡ºç°çš„å¾ˆå¤š,ä½†æ˜¯æ˜æ˜¾ä¸æ˜¯å¾ˆæœ‰åµç”¨ã€‚
$$
IDF(t) = log\_e(æ–‡æ¡£æ€»æ•° / å«æœ‰tçš„æ–‡æ¡£æ€»æ•°)
$$

> **TF-IDF = TF \* IDF**
>
> ä¸¾ä¸ªæ —å­ğŸŒ° :
>
> ä¸€ä¸ªæ–‡æ¡£æœ‰100ä¸ªå•è¯,å…¶ä¸­å•è¯babyå‡ºç°äº†3æ¬¡ã€‚ é‚£ä¹ˆ,TF(baby) = (3/100) = 0.03.
>> å¥½,ç°åœ¨æˆ‘ä»¬å¦‚æœæœ‰10Mçš„æ–‡æ¡£, babyå‡ºç°åœ¨å…¶ä¸­çš„1000ä¸ªæ–‡æ¡£ä¸­ã€‚ é‚£ä¹ˆ,IDF(baby) = log(10,000,000 / 1,000) = 4>> æ‰€ä»¥, TF-IDF(baby) = TF(baby) \* IDF(baby) = 0.03 \* 4 = 0.12

**nltk implement tf-idf**

```py
from nltk.text import TextCollection
# â¾¸å…ˆ, æŠŠæ‰€æœ‰çš„â½‚æ¡£æ”¾åˆ°TextCollectionç±»ä¸­ã€‚
# è¿™ä¸ªç±»ä¼šâ¾ƒåŠ¨å¸®ä½ æ–­ï¤†, åšç»Ÿè®¡, åšè®¡ç®—
corpus = TextCollection(['this is sentence one',
                        'this is sentence two',
                        'sentence three six',
                        'this is sentence three'])
# ç›´æ¥å°±èƒ½ç®—å‡ºtfidf
# (term: â¼€ä¸€å¥ï¤†è¯ä¸­çš„æŸä¸ªterm, text: è¿™å¥ï¤†è¯)
print(corpus.tf_idf('this', 'this is sentence four'))
# 0.444342
# åŒç†, æ€ä¹ˆå¾—åˆ°â¼€ä¸€ä¸ªæ ‡å‡†â¼¤å°çš„vectoræ¥è¡¨ç¤ºæ‰€æœ‰çš„å¥å­?
# å¯¹äºæ¯ä¸ªæ–°å¥å­
#new_sentence = 'this is sentence five' # éå†â¼€ä¸€éæ‰€æœ‰çš„vocabularyä¸­çš„è¯:
#for word in standard_vocab:
#    print(corpus.tf_idf(word, new_sentence)) # æˆ‘ä»¬ä¼šå¾—åˆ°â¼€ä¸ªå·¨é•¿(=æ‰€æœ‰vocabâ»“åº¦)çš„å‘é‡
```

[1]: http://www.nltk.org/
[2]: /images/ml/nlp-text-deal-process.png
[3]: http://www.regexlab.com/zh/regref.htm
[4]: /images/ml/nlp-nltk-raw.png
[5]: http://www.ranks.nl/stopwords
[6]: /images/ml/nlp-pre-deal.png
[7]: /images/ml/nlp-nltk-2.png
[8]: /images/ml/nlp-nltk-3.png
[9]: /images/ml/nlp-nltk-4.png

