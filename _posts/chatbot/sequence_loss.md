---
title: tensorflow sequence_loss
toc: true
date: 2018-12-07 13:36:21
categories: chatbot
tags: sequence_loss
---

sequence_lossæ˜¯nlpç®—æ³•ä¸­éå¸¸é‡è¦çš„ä¸€ä¸ªå‡½æ•°.rnn,lstm,attentionéƒ½è¦ç”¨åˆ°è¿™ä¸ªå‡½æ•°.çœ‹ä¸‹é¢ä»£ç :

<!-- more -->

## 1. ç‰¹æ®Šçš„ğŸŒ°

```py
# coding: utf-8
import numpy as np
import tensorflow as tf
from tensorflow.contrib.seq2seq import sequence_loss

logits_np = np.array([
    [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]],
    [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]]
])
targets_np = np.array([
    [0, 0, 0],
    [0, 0, 0]
], dtype=np.int32)

logits = tf.convert_to_tensor(logits_np)
targets = tf.convert_to_tensor(targets_np)
cost = sequence_loss(logits=logits,
                     targets=targets,
                     weights=tf.ones_like(targets, dtype=tf.float64))
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    r = sess.run(cost)
    print(r)
```

å…ˆå¯¹æ¯ä¸ª[0.5,0.5,0.5,0.5]å–softmax. softmax([0.5,0.5,0.5,0.5])=(0.25,0.25,0.25,0.25)ç„¶åå†è®¡ç®—-ln(0.25)*6/6=1.38629436112.

## 2. ä¸€èˆ¬çš„ğŸŒ°

```py
# coding:utf-8
from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from tensorflow.contrib.seq2seq import sequence_loss

import tensorflow as tf
import numpy as np

# 2ä¸ªå¥å­ï¼Œ3ä¸ªæ—¶åˆ»ï¼Œ4ä¸ªå€¼(è¯æ±‡è¡¨)
output_np = np.array(
    [
        [[0.6, 0.5, 0.3, 0.2], [0.9, 0.5, 0.3, 0.2], [1.0, 0.5, 0.3, 0.2]],
        [[0.2, 0.5, 0.3, 0.2], [0.3, 0.5, 0.3, 0.2], [0.4, 0.5, 0.3, 0.2]]
    ]
)
print(output_np.shape)
# 2ä¸ªå¥å­ï¼Œ
target_np = np.array([[0, 1, 2],
                      [3, 0, 1]],
                     dtype=np.int32)
print(target_np.shape)
output = tf.convert_to_tensor(output_np, np.float32)
target = tf.convert_to_tensor(target_np, np.int32)

cost = sequence_loss(output,
                     target,
                     tf.ones_like(target, dtype=np.float32))

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    cost_r = sess.run(cost)
    print(cost_r)
```

è¿™ä¸ªä»£ç ä½œç”¨å’Œä¸‹é¢çš„tf.reduce_mean(softmax_cross_entropy_with_logits)ä½œç”¨ä¸€è‡´.

```py
# coding:utf-8
from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

import tensorflow as tf
import numpy as np


def to_onehot(a):
    max_index = np.max(a)
    b = np.zeros((a.shape[0], max_index + 1))
    b[np.arange(a.shape[0]), a] = 1
    return b

logits_ph = tf.placeholder(tf.float32, shape=(None, None))
labels_ph = tf.placeholder(tf.float32, shape=(None, None))
output_np = np.array([
    [0.6, 0.5, 0.3, 0.2],
    [0.9, 0.5, 0.3, 0.2],
    [1.0, 0.5, 0.3, 0.2],
    [0.2, 0.5, 0.3, 0.2],
    [0.3, 0.5, 0.3, 0.2],
    [0.4, 0.5, 0.3, 0.2]])


cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_ph, logits=logits_ph))
target_np = np.array([0, 1, 2, 3, 0, 1])
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    cost_r = sess.run(cost, feed_dict={logits_ph: output_np, labels_ph: to_onehot(target_np)})
    print(cost_r)
```

å†å–äº¤å‰ç†µ,å†å–å¹³å‡.

## seq2seq çš„åº”ç”¨

chatbot åº”ç”¨ seq2seq éœ€è¦ç”¨åˆ° sequence_loss

## Reference


