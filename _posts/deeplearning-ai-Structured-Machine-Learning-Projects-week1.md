---
title: Structured Machine Learning Projects (week1) - ML Strategy 1
toc: true
date: 2018-07-24 19:00:21
categories: deeplearning
tags: deeplearning.ai
mathjax: true
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

è¿™æ¬¡æˆ‘ä»¬è¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹ä¸­ç¬¬ä¸‰é—¨è¯¾ **Structured Machine Learning Projects**

å­¦å®Œè¿™é—¨è¯¾ä¹‹åï¼Œä½ å°†ä¼š:

> - ç†è§£å¦‚ä½•è¯Šæ–­æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„é”™è¯¯
> - èƒ½å¤Ÿä¼˜å…ˆå‡å°è¯¯å·®æœ€æœ‰æ•ˆçš„æ–¹å‘
> - ç†è§£å¤æ‚MLè®¾å®šï¼Œä¾‹å¦‚è®­ç»ƒ/æµ‹è¯•é›†ä¸åŒ¹é…ï¼Œæ¯”è¾ƒå¹¶/æˆ–è¶…è¿‡äººçš„è¡¨ç°
> - çŸ¥é“å¦‚ä½•åº”ç”¨ç«¯åˆ°ç«¯å­¦ä¹ ã€è¿ç§»å­¦ä¹ ä»¥åŠå¤šä»»åŠ¡å­¦ä¹ 

å¾ˆå¤šå›¢é˜Ÿæµªè´¹æ•°æœˆç”šè‡³æ•°å¹´æ¥ç†è§£è¿™é—¨è¯¾æ‰€æ•™æˆçš„å‡†åˆ™ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™é—¨ä¸¤å‘¨çš„è¯¾å¯ä»¥ä¸ºä½ èŠ‚çº¦æ•°æœˆçš„æ—¶é—´

<!-- more -->

## 1. Why ML Strategy?

<img src="/images/deeplearning/C3W1-1_1.png" width="700" />

> å¦‚ä¸Šå›¾ç¤ºï¼Œå‡å¦‚æˆ‘ä»¬åœ¨æ„å»ºä¸€ä¸ªå–µå’ªåˆ†ç±»å™¨ï¼Œæ•°æ®é›†å°±æ˜¯ä¸Šé¢å‡ ä¸ªå›¾ï¼Œè®­ç»ƒä¹‹åå‡†ç¡®ç‡è¾¾åˆ°90%ã€‚è™½ç„¶çœ‹èµ·æ¥æŒºé«˜çš„ï¼Œä½†æ˜¯è¿™æ˜¾ç„¶å¹¶ä¸å…·ä¸€èˆ¬æ€§ï¼Œå› ä¸ºæ•°æ®é›†å¤ªå°‘äº†ã€‚é‚£ä¹ˆæ­¤æ—¶å¯ä»¥æƒ³åˆ°çš„MLç­–ç•¥æœ‰å“ªäº›å‘¢ï¼Ÿæ€»ç»“å¦‚ä¸Šå›¾ä¸­ **`Ideas`**.

## 2. Orthogonalization

> Orthogonalization [É”:Î¸É’É¡É™nÉ™laÉª'zeÉªÊƒn] æ­£äº¤åŒ–

<img src="/images/deeplearning/C3W1-2_1.png" width="600" />
 
> And when I train a neural networkï¼ŒI tend not to use early shopping.
> 
> å› ä¸º Early Stroppingï¼Œè¿™ä¸ªæŒ‰é’®èƒ½åŒæ—¶å½±å“ä¸¤ä»¶äº‹æƒ…. å°±åƒä¸€ä¸ªæŒ‰é’®åŒæ—¶å½±å“ç”µè§†æœºçš„å®½åº¦å’Œé«˜åº¦. å¦‚æœä½ æœ‰æ›´å¤šçš„æ­£äº¤åŒ–(Orthogonalization)çš„æ‰‹æ®µï¼Œç”¨è¿™äº›æ‰‹æ®µè°ƒç½‘ç»œä¼šç®€å•ä¸å°‘.
When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.

<img src="/images/deeplearning/C3W1-3_1.png" width="600" />

No. | strategy | solutions
:-------:  | :-------:  | :-------:
1. | Fit training set well in cost function | If it doesnâ€™t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.
2. | Fit development set well on cost function | If it doesnâ€™t fit well, regularization or using bigger training set might help.
3. | Fit test set well on cost function | If it doesnâ€™t fit well, the use of a bigger development set might help
4. | Performs well in real world | If it doesnâ€™t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing

## 3. Single number evaluation metric

<img src="/images/deeplearning/C3W1-4_1.png" width="700" />

> å¤§è‡´çš„æ€æƒ³å°±æ˜¯é¦–å…ˆæŒ‰ç…§å•ä¸€æ•°å­—è¯„ä¼°æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä»·å’Œä¼˜åŒ–ã€‚ä»¥ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¸ºä¾‹ï¼Œè¿™äºŒè€…ä¸€èˆ¬æ¥è¯´æ˜¯ä¸€ä¸ªä¸å¯å…¼å¾—çš„æŒ‡æ ‡ï¼Œæ‰€ä»¥ä¸ºäº†æ›´å¥½çš„è¡¡é‡æ¨¡å‹çš„å¥½åï¼Œå¼•å…¥F1ç®—æ³•æ¥ç»¼åˆç²¾ç¡®ç‡å’Œå¬å›ç‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°.

<!--<img src="/images/deeplearning/C3W1-6_1.png" width="700" />
-->

<img src="/images/deeplearning/C3W1-7_1.png" width="700" />

[Ref: sklearnä¸­ F1-micro ä¸ F1-macroåŒºåˆ«å’Œè®¡ç®—åŸç†][F1]

[F1]: https://www.cnblogs.com/techengin/p/8962024.html

## 4. Satisficing and optimizing metrics

It's not always easy into a single real number evaluation metric

<img src="/images/deeplearning/C3W1-9_1.png" width="750" />

> So more generally, if you have N metrics that you care about, it's sometimes reasonable to pick one of them to be optimizing. So you want to do as well as is possible on that one. And then N minus 1 to be satisficing.
> 
> æ»¡è¶³å’Œä¼˜åŒ–æŒ‡æ ‡æ˜¯å¾ˆé‡è¦çš„

## 5. Train/dev/test distributions

<img src="/images/deeplearning/C3W1-10_1.png" width="700" />

**Training, development and test distributions**

> Setting up the training, development and test sets have a huge impact on productivity. It is important to
choose the development and test sets from the same distribution and it must be taken randomly from all
the data.

**Guideline**

> Choose a development set and test set to reflect data you expect to get in the future and consider important to do well.

**æ‰€ä»¥ä¸ºäº†å®ç°æœä»åŒä¸€åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åš:**

> é¦–å…ˆå°†æ‰€æœ‰å›½å®¶å’Œåœ°åŒºçš„æ•°æ®æ‰“æ•£ï¼Œæ··åˆ, æŒ‰ç…§ä¸€å®šçš„æ¯”ä¾‹å°†ä¸Šé¢æ··åˆæ‰“æ•£åçš„æ•°æ®åˆ’åˆ†ä¸º **development and test sets**

## 6. Size of dev and test sets

<img src="/images/deeplearning/C3W1-11_1.png" width="750" />

## 7. When to change dev/test sets and metrics

**ä¸¾ä¸ªğŸŒ°:** å‡è®¾ç°åœ¨ä¸€ä¸ªå…¬å¸åœ¨åšä¸€ä¸ªå–µå’ªå›¾ç‰‡æ¨é€æœåŠ¡ï¼ˆå³ç»™ç”¨æˆ·æ¨é€å–µå’ªçš„ç…§ç‰‡ï¼‰ï¼Œéƒ¨ç½²çš„æœ‰ä¸¤ä¸ªç®—æ³•:

> - ç®—æ³•A: å–µå’ªå›¾ç‰‡è¯†åˆ«è¯¯å·®æ˜¯3%ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸€ä¸å°å¿ƒå°±ç»™ç”¨æˆ·å‘äº†ä¸€äº›å°‘å„¿ä¸å®œçš„å›¾ç‰‡
> - ç®—æ³•Bï¼šè¯¯å·®æ˜¯5%ï¼Œä½†æ˜¯ä¸ä¼šç»™ç”¨æˆ·æ¨é€ä¸å¥åº·çš„å›¾ç‰‡
>
> æ‰€ä»¥å¯¹äºæŠ€æœ¯äººå‘˜æ¥è¯´å¯èƒ½å¸Œæœ›å‡†ç¡®æ€§é«˜ä¸€äº›çš„ç®—æ³•Aï¼Œè€Œç”¨æˆ·å¯èƒ½ä¼šéå¸¸åœ¨æ„ä½ ç»™ä»–æ¨é€äº†æŸäº›ä¸æƒ³çœ‹çš„ä¸œè¥¿, ä¹Ÿè®¸æ›´å–œæ¬¢ç®—æ³•Bã€‚æ‰€ä»¥æ€»çš„æ¥è¯´å°±æ˜¯æ ¹æ®å®é™…éœ€è¦æ¥ æ”¹å˜å¼€å‘/æµ‹è¯•é›†åˆæŒ‡æ ‡.

<img src="/images/deeplearning/C3W1-12_1.png" width="750" />

## 8. Why human-level performance?

<img src="/images/deeplearning/C3W1-14_1.png" width="750" />

> å¦‚å›¾ç¤ºï¼š
>
> - è“è‰²è™šçº¿ï¼šè¡¨ç¤ºäººç±»è¯†åˆ«çš„å‡†ç¡®ç‡
> - ç´«è‰²æ›²çº¿ï¼šè¡¨ç¤ºæœºå™¨å­¦ä¹ ä¸æ–­è®­ç»ƒè¿‡ç¨‹ä¸­å‡†ç¡®ç‡çš„å˜åŒ–
> - ç»¿è‰²è™šçº¿ï¼šè¡¨ç¤ºæœ€é«˜çš„å‡†ç¡®ç‡ï¼Œå³100%
>
> å…¶ä¸­ç´«è‰²æ›²çº¿åœ¨æœ«å°¾æ”¶æ•›åä¸ç»¿è‰²è™šçº¿ä¹‹é—´çš„å·®è·ç§°ä¸ºè´å¶æ–¯ä¼˜åŒ–è¯¯å·®(Bayse Optima Error)

<!--<img src="/images/deeplearning/C3W1-13_1.png" width="750" />-->

å› æ­¤åœ¨å®é™…æ“ä½œè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥äººç±»å‡†ç¡®ç‡ä¸ºæŒ‡æ ‡æ¥è¯„åˆ¤æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å¥½åç¨‹åº¦

<img src="/images/deeplearning/C3W1-15_1.png" width="750" />

## 9. Avoidable bias

<img src="/images/deeplearning/C3W1-16_1.png" width="750" />

> Humans error ä¸ Training Error ä¹‹é—´çš„å·®è·æˆ‘ä»¬æˆä¸º Avoidable bias
> Training Error ä¸ Dev Error ä¹‹é—´çš„å·®è·æˆ‘ä»¬æˆä¸º Variance

## 10. Understanding human-level performance

<img src="/images/deeplearning/C3W1-18_1.png" width="750" />

> **è§£é‡Šè¯´æ˜ Example 1**:
> 
> å‡å¦‚ä¸€ä¸ªåŒ»é™¢éœ€è¦å¯¹ä¸€ä¸ªåŒ»å­¦å½±åƒè¿›è¡Œåˆ†ç±»è¯†åˆ«ï¼Œæ™®é€šäººï¼Œæ™®é€šåŒ»ç”Ÿï¼Œæœ‰ç»éªŒçš„åŒ»ç”Ÿå’Œä¸€ç¾¤æœ‰ç»éªŒçš„åŒ»ç”Ÿè¯†åˆ«é”™è¯¯ç‡åˆ†åˆ«ä¸º3%ï¼Œ1%ï¼Œ0.7%ï¼Œ0.5%ã€‚ä¸Šä¸€èŠ‚ä¸­æåˆ°è¿‡Human Errorï¼Œé‚£æ­¤æ—¶çš„è¯¥å¦‚ä½•ç¡®å®šHuman Errorå‘¢ï¼Ÿä½ å¯èƒ½ä¼šè¯´å–å¹³å‡å€¼ï¼Œåªèƒ½è¯´Too Naiveï¼å½“ç„¶æ˜¯å–æœ€å¥½çš„ç»“æœå•¦ï¼Œä¹Ÿå°±æ˜¯ç”±ä¸€ç¾¤ç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿç»„æˆçš„å›¢ä½“å¾—åˆ°çš„ç»“æœä½œä¸ºHuman Errorã€‚å¦å¤–è´å¶æ–¯è¯¯å·®ä¸€å®šå°äº0.5%ã€‚

<img src="/images/deeplearning/C3W1-19_1.png" width="750" />

> **è§£é‡Šè¯´æ˜ Example 2**:
>
> è¿˜æ˜¯ä»¥åŒ»å­¦å½±åƒåˆ†ç±»è¯†åˆ«ä¸ºä¾‹ï¼Œå‡å¦‚ç°åœ¨åˆ†æˆäº†ä¸‰ç§æƒ…å†µï¼š

> Scenario A
> è®©ä¸‰ç±»äººç¾¤æ¥åˆ’åˆ†åå¾—åˆ°çš„è¯¯å·®åˆ†åˆ«ä¸º1%ï¼Œ0.7%ï¼Œ0.5%ï¼Œè€Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†è¯¯å·®åˆ†åˆ«ä¸º5%ï¼Œ6%ã€‚å¾ˆæ˜¾ç„¶æ­¤æ—¶çš„Avoidable Bias=4%~4.5%ï¼ŒVariance=1%ï¼Œbiasæ˜æ˜¾å¤§äºvarianceï¼Œæ‰€ä»¥æ­¤æ—¶åº”è¯¥å°†é‡å¿ƒæ”¾åˆ°å‡å°biasä¸Šå»ã€‚

> Scenario Bayse
> åŒç†æ­¤æƒ…å†µä¸‹çš„Avoidable Bias=0%~0.5%ï¼ŒVariance=4%ï¼Œæ‰€ä»¥éœ€è¦å‡å°varianceã€‚

> Scenario C
> Avoidable Bias=0.2%ï¼ŒVariance=0.1%ï¼ŒäºŒè€…ç›¸å·®æ— å‡ ï¼Œä½†æ˜¯æ­¤æ—¶è®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡è¿˜æ˜¯ä¸åŠäººç±»ï¼Œæ‰€ä»¥æ²¡åŠæ³•å’±ä»¬è¿˜å¾—ç»§ç»­ä¼˜åŒ–ï¼Œéƒ½è¯´æªæ‰“å‡ºå¤´é¸Ÿï¼Œæ‰€ä»¥ç»§ç»­ä¼˜åŒ–bias~

## 11. Surpassing human-level performance

<img src="/images/deeplearning/C3W1-20_1.png" width="750" />

> **Scenario A**
> 
> - Avoidable Bias=0.1%ï¼ŒVariance=0.2%ï¼Œæ‰€ä»¥æ­¤æ—¶åº”è¯¥å°†é‡å¿ƒæ”¾åˆ°å‡å°Varianceä¸Šå»

> **Scenario B**
> 
> - Avoidable Bias=-0.2%ï¼ŒVariance=0.1%.ä¹ä¸€çœ‹å¯èƒ½ä¼šæœ‰ç‚¹ä¸çŸ¥æ‰€æªï¼Œè€Œä¸”è®­ç»ƒé›†å‡†ç¡®åº¦ä¹Ÿè¶…è¿‡äº†äººçš„æœ€å¥½æˆç»©ï¼Œä¸çŸ¥é“åº”è¯¥é€‰æ‹©ä¼˜åŒ–å“ªä¸€é¡¹äº†ï¼Œæˆ–è€…è¯´è¿™æ˜¯ä¸æ˜¯å°±è¯´æ˜å¯ä»¥ä¸ç”¨å†ä¼˜åŒ–äº†å‘¢ï¼Ÿ
> 
> ï¼ˆè¿˜æ˜¯å¯ä»¥ç»§ç»­ä¼˜åŒ–çš„ã€‚ä¸å¯å¦è®¤åœ¨å›¾åƒè¯†åˆ«æ–¹é¢äººç±»çš„ç¡®å…¶ä¼˜äºæœºå™¨çš„æ–¹é¢ï¼Œä½†æ˜¯åœ¨å…¶ä»–æ–¹é¢ï¼Œå¦‚åœ¨çº¿å¹¿å‘Šæ¨é€ï¼Œè´·æ¬¾ç”³è¯·è¯„æµ‹ç­‰æ–¹é¢æœºå™¨äººè¦è¿œè¿œæ¯”äººç±»ä¼˜ç§€ï¼Œæ‰€ä»¥å¦‚æœæ˜¯åœ¨ä¸Šé¢è¯¾ä»¶ä¸­æåˆ°çš„ä¸€äº›é¢†åŸŸï¼Œå³ä½¿æœºå™¨å‡†ç¡®åº¦è¶…è¿‡äº†äººç±»ï¼Œä¹Ÿè¿˜æœ‰å¾ˆå¤§çš„ä¼˜åŒ–ç©ºé—´ã€‚å…·ä½“æ€ä¹ˆä¼˜åŒ–ã€‚ã€‚ã€‚ä»¥åå†æ¢ç´¢ã€‚ã€‚ã€‚ï¼‰

## 12. Improving your model performance

<img src="/images/deeplearning/C3W1-21_1.png" width="750" />

## 13. Reference

- [ç½‘æ˜“äº‘è¯¾å ‚ - deeplearning][1]
- [DeepLearning.aiå­¦ä¹ ç¬”è®°æ±‡æ€»][4]
- [DeepLearning.aiå­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰ç»“æ„åŒ–æœºå™¨å­¦ä¹ é¡¹ç›®--week1 æœºå™¨å­¦ä¹ ç­–ç•¥][5]

[1]: https://study.163.com/my#/smarts
[2]: https://daniellaah.github.io/2017/deeplearning-ai-Improving-Deep-Neural-Networks-week1.html
[3]: https://www.coursera.org/specializations/deep-learning
[4]: http://www.cnblogs.com/marsggbo/p/7470989.html
[5]: http://www.cnblogs.com/marsggbo/p/7681619.html

