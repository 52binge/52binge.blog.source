---
title: 朴素贝叶斯
toc: true
date: 2017-08-10 07:08:21
categories: NLP
tags: python-re
description: Naive Bayes
mathjax: true
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<style>
img {
        display: block !important;
        width: 380px;
        margin-left: 200px !important;
}
</style>

## 1. 引言

贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级 NLP 模型也可以从它演化而来。因此，学习贝叶斯方法，是研究 NLP 问题的一个非常好的切入口。

## 2. 贝叶斯公式

贝叶斯公式就一行：

$$
P(Y|X)=P(X|Y)P(Y)P(X)
$$

它其实是由以下的联合概率公式推导出来：

$$
P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)
$$

其中 $P(Y)$ 叫做先验概率， $P(Y|X)$ 叫做后验概率， $P(Y,X)$ 叫做联合概率。

## 3. 机器学习视角理解贝叶斯公式

把 $X$ 理解成 “$有某 feature$”
把 $Y$ 理解成 “$属于某类 label$”

> 一般机器学习为题中都是 $X$ => 特征, $Y$ => 结果 对吧。

> 在最简单的二分类问题(是与否判定)下，我们将 $Y$ 理解成 $“属于某类”$ 的标签。
于是贝叶斯公式就变形成了下面的样子:

$$
P(“属于某类”|“具有某特征”)=P(“具有某特征”|“属于某类”)P(“属于某类”)P(“具有某特征”)
$$

而我们二分类问题的最终目的就是要判断 $P(“属于某类”|“具有某特征”)$ 是否大于1/2就够了。贝叶斯方法把计算 “$具有某特征的条件下属于某类$” 的概率转换成需要计算 “$属于某类的条件下具有某特征$” 的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。

## 4. 垃圾邮件识别

举个栗子 🌰

我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是判断 $P(“垃圾邮件”|“具有某特征”)$ 是否大于1/2。

假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：

> “我司可办理正规发票（保真）17%增值税发票点数优惠！”

也就是判断概率 $P(“垃圾邮件”|“我司可办理正规发票（保真）17\%增值税发票点数优惠！”)$ 是否大于1/2。

$$
P = \frac{垃圾邮件中出现这句话的次数}{垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数}
$$

> 咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！

## 5. 分词

一个很悲哀但是很现实的结论：`训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的`。

所以解决方法是？ $句子的可能性无限，但是词语就那么些$！！汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如 $“我司可办理正规发票，17%增值税发票点数优惠！”$，这句话就比之前那句话少了“（保真）”这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就不方便我们计算了。

于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如 “$正规发票$” 可以作为一个单独的词语，“$增值税$” 也可以作为一个单独的词语等等。

> 句子“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）。

于是你接触到了中文NLP中，最最最重要的技术之一：**`分词`**！！！也就是把一整句话拆分成更细粒度的词语来进行表示。另外，分词之后去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术。  

中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)  

我们观察$（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)$，这可以理解成一个向量：向量的每一维度都表示着该 $特征词$ 在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。

因此贝叶斯公式就变成了：

$$
P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）
$$

$$
=\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）P(“垃圾邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }
$$

---

$$
P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）
$$

$$
=\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”正常邮件”）P(“正常邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }
$$

## 6. 条件独立假设

下面我们马上会看到一个非常简单粗暴的假设。

$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|”垃圾邮件”）$ 依旧不够好求，我们引进一个很朴素的近似。为了让公式显得更加紧凑，我们令字母 `S` 表示“垃圾邮件”,令字母 `H` 表示“正常邮件”。近似公式如下：

$$
P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）
$$

$$
=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S）
$$

$$
×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)
$$

这就是传说中的条件独立假设。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。 

于是我们就只需要比较以下两个式子的大小：

$$
C = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)
$$

$$
×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)
$$

---

$$
\overline{C}=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)
$$

$$
×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)
$$

厉(wo)害(cao)！酱紫处理后式子中的每一项都特别好求！只需要分别统计各类邮件中该关键词出现的概率就可以了！！！比如：

$$
P(“发票”|S）=\frac{垃圾邮件中所有“发票”的次数}{垃圾邮件中所有词语的次数}
$$

统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。

## 7. Naive Bayes，“Naive”在何处？

加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。 Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、“蠢蠢的”。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？

将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！计算过程如下：

$$
P(（“我”,“司”,“可”,“办理”,“正规发票”)|S) = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)
$$

$$
=P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S） =P(（“正规发票”,“可”,“办理”, “我”, “司”)|S)
$$

也就是说，在朴素贝叶斯眼里，“`我司可办理正规发票`” 与 “`正规发票可办理我司`” 完全相同。朴素贝叶斯失去了词语之间的顺序信息。这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作词袋子模型(bag of words)。

词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下，“`武松打死了老虎`” 与 “`老虎打死了武松`” 被它认作一个意思了。恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢

## 8. 简单高效，吊丝逆袭

虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还令人诧异地好。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。（Paul Graham《黑客与画家》）

那个...效果为啥好呢？

“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大。具体的数学公式请参考[这篇 paper][2]。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）

恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是简单、实用、且强大的。

## 9. 处理重复词语的三种方式

![][3]

## 10. 去除停用词与选择关键词

## 11. 浅谈平滑技术

## 12. 内容小结

## 13. 匹配关键词来识别垃圾邮件？

为什么不直接匹配关键词来识别垃圾邮件？

## 14. 实际工程的tricks

## 15. 贝叶斯方法的思维方式

## 16. (朴素)贝叶斯方法的常见应用

## 17. 内容小结

从前面大家基本可以看出，工程应用不同于学术理论，有许多tricks需要考虑，而理论本质就是翻来倒去折腾贝叶斯公式，都快玩出花来了。

[1]: /images/nlp/nlp-word-bags.jpg
[2]: http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf
[3]: /images/nlp/nlp-bayes-02.jpg