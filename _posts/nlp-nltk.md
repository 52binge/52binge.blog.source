---
title: NLP åŸç†ä¸åŸºç¡€
toc: true
date: 2017-06-29 21:08:21
categories: nlp
tags: nltk
description: python nltk
---

> NLTK æ˜¯ä¸€ä¸ªæœ‰è‚‰æœ‰è¡€çš„

> æ–¯å¦ä½› CoreNLP (è‹±æ–‡ã€ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­)

<!-- more -->

## 1. NLTK

[nltk.org][1]

1. Python è‘—åçš„è‡ªç„¶è¯­è¨€å¤„ç†åº“
2. è‡ªå¸¦è¯­æ–™åº“ã€è¯æ€§åˆ†ç±»åº“
3. è‡ªå¸¦åˆ†ç±»ã€åˆ†è¯ ç­‰åŠŸèƒ½
4. å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ

```bash
sudo pip install -U nltk
sudo pip install -U numpy
```

```py
import nltk
nltk.download()
```

NLTK Modules | Functionality
------- | -------
nltk.corpus | Corpus
nltk.tokenize, nltk.stem | Tokenizers, stemmers

## 2. NLTK è‡ªå¸¦è¯­æ–™åº“

```py
>>> from nltk.corpus import brown
>>> brown.categories()
[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']
>>> len(brown.sents())
57340
>>> len(brown.words())
1161192
>>>
```

## 3. æ–‡æœ¬å¤„ç†æµç¨‹

![][2]

## 4. Tokenize

```py
>>> import nltk>>> sentence = â€œhello, world">>> tokens = nltk.word_tokenize(sentence)>>> tokens['hello', â€˜,', 'world']
```

### 4.1 ä¸­è‹±æ–‡åˆ†è¯

ä»Šå¤© / å¤©â½“ / ä¸é”™ / !What a nice weather today !

[â€˜whatâ€™, â€˜aâ€™, â€˜niceâ€™, â€˜weatherâ€™, â€˜todayâ€™] 
[â€˜ä»Šå¤©â€™,â€™å¤©æ°”â€™,â€™çœŸâ€™,â€™ä¸é”™â€™]

NLTK

word_tokenize åˆ†è¯å™¨

### 4.2 ä¸­æ–‡åˆ†è¯

> jieba (ç¬¬ä¸‰æ–¹å¼€æºåº“)

```py
import jiebaseg_list = jieba.cut("æˆ‘æ¥åˆ°ï¥£äº¬æ¸…åâ¼¤å­¦", cut_all=True)print "Full Mode:", "/ ".join(seg_list) # å…¨æ¨¡å¼seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åâ¼¤å­¦", cut_all=False)print "Default Mode:", "/ ".join(seg_list) # ç²¾ç¡®æ¨¡å¼seg_list = jieba.cut("ä»–æ¥åˆ°äº†ç½‘ï§ æ­ç ”â¼¤å¦") # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼print ", ".join(seg_list)seg_list = jieba.cut_for_search("â¼©æ˜ç¡•å£«æ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€,ååœ¨æ—¥æœ¬äº¬éƒ½â¼¤å­¦æ·±é€ ") # æœç´¢å¼•æ“æ¨¡å¼print ", ".join(seg_list)
```
ã€å…¨æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—ï¥£äº¬/ æ¸…å/ æ¸…åâ¼¤å¤§å­¦/ åâ¼¤å¤§/ â¼¤å¤§å­¦   
ã€ç²¾ç¡®æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—ï¥£äº¬/ æ¸…åâ¼¤å¤§å­¦  ã€æ–°è¯è¯†åˆ«ã€‘:ä»–, æ¥åˆ°, äº†ï¦º, â½¹ç½‘æ˜“ï§ , æ­ç ”, â¼¤å¤§å¦ (æ­¤å¤„,â€œæ­ç ”â€å¹¶æ²¡æœ‰åœ¨è¯å…¸ä¸­,ä½†æ˜¯ä¹Ÿè¢«Viterbiç®—æ³•è¯†åˆ«å‡ºæ¥äº†ï¦º)  ã€æœç´¢å¼•æ“æ¨¡å¼ã€‘: â¼©å°æ˜, ç¡•â¼ å£«, æ¯•ä¸š, äº, ä¸­å›½, ç§‘å­¦, å­¦é™¢, ç§‘å­¦é™¢, ä¸­å›½ç§‘å­¦é™¢, è®¡ç®—, è®¡ç®—æ‰€, å, åœ¨, â½‡æ—¥æœ¬, äº¬éƒ½, â¼¤å¤§å­¦, â½‡æ—¥æœ¬äº¬éƒ½â¼¤å¤§å­¦, æ·±é€ 


> æœ‰æ—¶å€™tokenizeæ²¡é‚£ä¹ˆç®€å• :

> æ¯”å¦‚ç¤¾äº¤ç½‘ç»œä¸Š,è¿™äº›ä¹±ä¸ƒå…«ç³Ÿçš„ä¸åˆè¯­æ³•ä¸åˆæ­£å¸¸é€»è¾‘çš„è¯­è¨€å¾ˆå¤š:æ‹¯æ•‘ @æŸäºº, è¡¨æƒ…ç¬¦å·, URL, #è¯é¢˜ç¬¦å·

**ç¤¾äº¤ç½‘ç»œè¯­è¨€çš„tokenize :**

```py
from nltk.tokenize import word_tokenizetweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'print(word_tokenize(tweet))
```

[æ­£åˆ™è¡¨è¾¾å¼å¯¹ç…§è¡¨][1]


## 5. è¯å½¢å½’ä¸€åŒ–

Inflectionå˜åŒ–: walk => walking => walked ä¸å½±å“è¯æ€§
derivation å¼•ç”³: nation (noun) => national (adjective) => nationalize (verb) å½±å“è¯æ€§

### 5.1 Stemming 

Stemming è¯å¹²æå–:ä¸€èˆ¬æ¥è¯´,å°±æ˜¯æŠŠä¸å½±å“è¯æ€§çš„inflectionçš„å°å°¾å·´ç æ‰
walking ç ing = walk   
walked ç ed = walk

**PorterStemmer**

```py
>>> from nltk.stem.porter import PorterStemmer>>> porter_stemmer = PorterStemmer()>>> porter_stemmer.stem(â€˜maximumâ€™)uâ€™maximumâ€™>>> porter_stemmer.stem(â€˜presumablyâ€™)uâ€™presumâ€™>>> porter_stemmer.stem(â€˜multiplyâ€™)uâ€™multipliâ€™>>> porter_stemmer.stem(â€˜provisionâ€™)uâ€™provisâ€™
```

**SnowballStemmer**


```py
>>> from nltk.stem import SnowballStemmer
>>> snowball_stemmer = SnowballStemmer("english")
>>> snowball_stemmer.stem('maximum')
u'maximum'
>>> snowball_stemmer.stem('presumably')
uâ€™presumâ€™
```

**LancasterStemmer**

```py
>>> from nltk.stem.lancaster import LancasterStemmer
>>> lancaster_stemmer = LancasterStemmer()
>>> lancaster_stemmer.stem(â€˜maximumâ€™)
â€˜maximâ€™
>>> lancaster_stemmer.stem(â€˜presumablyâ€™)
â€˜presumâ€™
>>> lancaster_stemmer.stem(â€˜presumablyâ€™)
â€˜presumâ€™
```

**PorterStemmer**

```py
>>> from nltk.stem.porter import PorterStemmer
>>> p = PorterStemmer()
>>> p.stem('went')
'went'
>>> p.stem('wenting')
'went'
```
### 5.2 Lemmatization 

è¯å½¢å½’ä¸€:æŠŠå„ç§ç±»å‹çš„è¯çš„å˜å½¢,éƒ½å½’ä¸ºä¸€ä¸ªå½¢å¼ 

went å½’ä¸€ = go  are å½’ä¸€ = be

```py
>>> from nltk.stem import WordNetLemmatizer>>> wordnet_lemmatizer = WordNetLemmatizer()>>> wordnet_lemmatizer.lemmatize(â€˜dogsâ€™)uâ€™dogâ€™>>> wordnet_lemmatizer.lemmatize(â€˜churchesâ€™)uâ€™churchâ€™>>> wordnet_lemmatizer.lemmatize(â€˜aardwolvesâ€™)uâ€™aardwolfâ€™>>> wordnet_lemmatizer.lemmatize(â€˜abaciâ€™)uâ€™abacusâ€™>>> wordnet_lemmatizer.lemmatize(â€˜hardrockâ€™)â€˜hardrockâ€™
```

NLTKæ›´å¥½åœ°å®ç°Lemma

```py
# â½Šæœ¨æœ‰POS Tag,é»˜è®¤æ˜¯NN åè¯>>> wordnet_lemmatizer.lemmatize(â€˜areâ€™) â€˜areâ€™>>> wordnet_lemmatizer.lemmatize(â€˜isâ€™) â€˜isâ€™# åŠ ä¸ŠPOS Tag>>> wordnet_lemmatizer.lemmatize(â€˜isâ€™, pos=â€™vâ€™) uâ€™beâ€™>>> wordnet_lemmatizer.lemmatize(â€˜areâ€™, pos=â€™vâ€™) uâ€™beâ€™
```

## 6. Part Of Speech

TAG | Meaning
------- | -------
CC	| coordinating conjunction
CD	| cardinal digit
DT	| determiner
EX	| existential there (like: "there is" ... think of it like "there exists")
FW	| foreign word
IN	| preposition/subordinating conjunction
JJ	| adjective	'big'
... | ...

### 6.1 NLTK æ ‡æ³¨ POS Tag

```py
>>> import nltk>>> text = nltk.word_tokenize('what does the fox say')>>> text['what', 'does', 'the', 'fox', 'say']>>> nltk.pos_tag(text)[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]
```

## 7. Stopwords

ä¸€åƒä¸ªHEæœ‰ä¸€åƒç§æŒ‡ä»£
ä¸€åƒä¸ªTHEæœ‰ä¸€åƒç§æŒ‡äº‹ å¯¹äºæ³¨é‡ç†è§£æ–‡æœ¬ã€æ„æ€ã€çš„åº”ç”¨åœºæ™¯æ¥è¯´
æ­§ä¹‰å¤ªå¤š

[å…¨ä½“stopwordsåˆ—è¡¨][5] 

### 7.1 NLTKå»é™¤stopwords

> é¦–å…ˆè®°å¾—åœ¨consoleé‡Œé¢ä¸‹è½½ä¸€ä¸‹è¯åº“> æˆ–è€… nltk.download(â€˜stopwordsâ€™)

```py
import nltk
from nltk.corpus import stopwords 

# å…ˆtokenâ¼€ä¸€æŠŠ,å¾—åˆ°â¼€ä¸€ä¸ªword_list

word_list = nltk.word_tokenize('what does the fox say')

# ç„¶åfilterâ¼€ä¸€æŠŠ

filtered_words = [word for word in word_list if word not in stopwords.words('english')]

filtered_words
```

## 8. æ–‡æœ¬é¢„å¤„ç†æµæ°´çº¿

> ä¸€æ¡typicalçš„æ–‡æœ¬é¢„å¤„ç†æµæ°´çº¿

![][4]

**æ–‡æœ¬é¢„å¤„ç†è®©æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆ?**

![][6]

## 9. NLPä¸Šçš„ç»å…¸åº”ç”¨

1. æƒ…æ„Ÿåˆ†æ2. æ–‡æœ¬ç›¸ä¼¼åº¦ 
3. æ–‡æœ¬åˆ†ç±»

### 9.1 æƒ…æ„Ÿåˆ†æ

![][7]

å“ªäº›æ˜¯å¤¸ä½ ï¼Ÿå“ªäº›æ˜¯é»‘ä½ ï¼Ÿ

**æœ€ç®€å•çš„ sentiment dictionary**

- like 1 
- good 2 
- bad -2 
- terrible -3

> æœ€ç®€å•ä¹Ÿæ¯”è¾ƒæœ‰æ•ˆçš„æ–¹æ³•ï¼Œä¸éœ€è¦å­¦ä¹ 
> 
> æ¯”å¦‚:AFINN-111> http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010

**NLTK å®Œæˆç®€å•çš„æƒ…æ„Ÿåˆ†æ**

```py
sentiment_dictionary = {}for line in open('data/AFINN-111.txt')    word, score = line.split('\t')    sentiment_dictionary[word] = int(score)# æŠŠè¿™ä¸ªæ‰“åˆ†è¡¨è®°å½•åœ¨â¼€ä¸€ä¸ªDictä¸Šä»¥å 
# è·‘â¼€ä¸€éæ•´ä¸ªå¥ï¤†â¼¦å­,æŠŠå¯¹åº”çš„å€¼ç›¸åŠ total_score = sum(sentiment_dictionary.get(word, 0) for word in 
words) 

# æœ‰å€¼å°±æ˜¯Dictä¸­çš„å€¼,æ²¡æœ‰å°±æ˜¯0# äºæ˜¯ä½ å°±å¾—åˆ°äº†ï¦ºâ¼€ä¸€ä¸ª sentiment score
```

> æ˜¾ç„¶è¿™ä¸ªæ–¹æ³•å¤ªNaive 
> æ–°è¯æ€ä¹ˆåŠ? 
> ç‰¹æ®Šè¯æ±‡æ€ä¹ˆåŠ? 
> æ›´æ·±å±‚æ¬¡çš„ç©æ„å„¿æ€ä¹ˆåŠ?

**é…ä¸ŠMLçš„æƒ…æ„Ÿåˆ†æ**

```py
from nltk.classify import NaiveBayesClassifier# éšâ¼¿æ‰‹é€ ç‚¹è®­ç»ƒé›†s1 = 'this is a good book's2 = 'this is a awesome book's3 = 'this is a bad book's4 = 'this is a terrible book'def preprocess(s): # Func: å¥â¼¦å¤„ç†# è¿™â¾¥ç®€å•çš„â½¤äº†ï¦ºsplit(), æŠŠå¥å­ä¸­æ¯ä¸ªå•è¯åˆ†å¼€ # æ˜¾ç„¶ è¿˜æœ‰æ›´å¤šçš„processing methodå¯ä»¥â½¤ 
    return {word: True for word in s.lower().split()}# returné•¿è¿™æ ·:# {'this': True, 'is':True, 'a':True, 'good':True, 'book':True} # å…¶ä¸­, å‰â¼€ä¸€ä¸ªå«fname, å¯¹åº”æ¯ä¸ªå‡ºç°çš„æ–‡æœ¬å•è¯;# åâ¼€ä¸€ä¸ªå«fval, æŒ‡çš„æ˜¯æ¯ä¸ªâ½‚æ–‡æœ¬å•è¯å¯¹åº”çš„å€¼ã€‚# è¿™â¾¥é‡Œï§©æˆ‘ä»¬â½¤ç”¨æœ€ç®€å•çš„True,æ¥è¡¨ç¤º,è¿™ä¸ªè¯ã€å‡ºç°åœ¨å½“å‰çš„å¥ï¤†â¼¦å­ä¸­ã€çš„æ„ä¹‰ã€‚# å½“ç„¶å•¦, æˆ‘ä»¬ä»¥åå¯ä»¥å‡çº§è¿™ä¸ªâ½…æ–¹ç¨‹, è®©å®ƒå¸¦æœ‰æ›´ï¤åŠ â½œç‰›é€¼çš„fval, æ¯”å¦‚ word2vec


# æŠŠè®­ç»ƒé›†ç»™åšæˆæ ‡å‡†å½¢å¼training_data = [[preprocess(s1), 'pos'],                 [preprocess(s2), 'pos'],                 [preprocess(s3), 'neg'],                 [preprocess(s4), 'neg']]# å–‚ç»™modelåƒmodel = NaiveBayesClassifier.train(training_data)# æ‰“å‡ºç»“æœprint(model.classify(preprocess('this is a good book')))
```

### 9.2 æ–‡æœ¬ç›¸ä¼¼åº¦

![][8]

**9.2.1 ç”¨å…ƒç´ é¢‘ç‡è¡¨ç¤ºæ–‡æœ¬ç‰¹å¾**

![][9]

**9.2.2 Frequency é¢‘ç‡ç»Ÿè®¡**

```py
import nltk
from nltk import FreqDist
# åšä¸ªè¯åº“å…ˆ
corpus = 'this is my sentence ' \
           'this is my life ' \
           'this is the day'
# éšä¾¿tokenizeä¸€ä¸‹, æ˜¾ç„¶, æ­£å¦‚ä¸Šæ–‡æåˆ°,
# è¿™ï§©å¯ä»¥æ ¹æ®éœ€è¦åšä»»ä½•çš„preprocessing:  stopwords, lemma, stemming, etc.
# å€Ÿâ½¤NLTKçš„FreqDistç»Ÿè®¡â¼€ä¸‹â½‚å­—å‡ºç°çš„é¢‘ç‡ fdist = FreqDist(tokens)
# å®ƒå°±ç±»ä¼¼äºâ¼€ä¸ªDic,  å¸¦ä¸ŠæŸä¸ªå•è¯, å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªæ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°

tokens = nltk.word_tokenize(corpus) 
print(tokens)
# å¾—åˆ° token å¥½çš„ word list
# ['this', 'is', 'my', 'sentence',
# 'this', 'is', 'my', 'life', 'this', # 'is', 'the', 'day']
# å€Ÿâ½¤ NLTK çš„ FreqDist ç»Ÿè®¡â¼€ä¸‹æ–‡å­—å‡ºç°çš„é¢‘ç‡
fdist = FreqDist(tokens)

# å®ƒå°±ç±»ä¼¼äºâ¼€ä¸€ä¸ªDict
# å¸¦ä¸ŠæŸä¸ªå•è¯, å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªæ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°
print(fdist['is']) #3

# å¥½, æ­¤åˆ», æˆ‘ä»¬å¯ä»¥æŠŠæœ€å¸¸â½¤ç”¨çš„50ä¸ªå•è¯æ‹¿å‡ºæ¥ 
standard_freq_vector = fdist.most_common(50) 
size = len(standard_freq_vector) 
print "size: %s" % (size)
print(standard_freq_vector)
# [('is', 3), ('this', 3), ('my', 2),
# ('the', 1), ('day', 1), ('sentence', 1),
# ('life', 1)

# Func: æŒ‰ç…§å‡ºç°é¢‘ç‡â¼¤å°, è®°å½•ä¸‹æ¯â¼€ä¸ªå•è¯çš„ä½ç½® 
def position_lookup(v):
    res = {}
    counter = 0
    for word in v:
        res[word[0]] = counter
        counter += 1
    return res
# æŠŠæ ‡å‡†çš„å•è¯ä½ç½®è®°å½•ä¸‹æ¥
standard_position_dict = position_lookup(standard_freq_vector) 
print(standard_position_dict)
# å¾—åˆ°â¼€ä¸€ä¸ªä½ç½®å¯¹ç…§è¡¨
# {'is': 0, 'the': 3, 'day': 4, 'this': 1, 'sentence': 5, 'my': 2, 'life': 6}

# è¿™æ—¶, å¦‚æœæˆ‘ä»¬æœ‰ä¸ªæ–°å¥ï¤†â¼¦å­:
sentence = 'this is cool'
# å…ˆæ–°å»ºâ¼€ä¸€ä¸ªè·Ÿæˆ‘ä»¬çš„æ ‡å‡†vectoråŒæ ·â¼¤å°çš„å‘é‡ 
freq_vector = [0] * size
# ç®€å•çš„Preprocessing
tokens = nltk.word_tokenize(sentence) # å¯¹äºè¿™ä¸ªæ–°å¥â¼¦â¾¥çš„æ¯ä¸€ä¸ªå•è¯
for word in tokens:
    try:
    # å¦‚æœåœ¨æˆ‘ä»¬çš„è¯åº“ï§©å‡ºç°è¿‡
    # é‚£ä¹ˆå°±åœ¨"æ ‡å‡†ä½ç½®"ä¸Š+1 
        freq_vector[standard_position_dict[word]] += 1
    except KeyError: # å¦‚æœæ˜¯ä¸ªæ–°è¯
        continue
print(freq_vector)   # [1, 1, 0, 0, 0, 0, 0]
```

### 9.3 æ–‡æœ¬åˆ†ç±»

#### 9.3.1 tf-idf

**TF: Term Frequency**, è¡¡é‡ä¸€ä¸ªtermåœ¨æ–‡æ¡£ä¸­å‡ºç°å¾—æœ‰å¤šé¢‘ç¹ã€‚ T

F(t) = (tå‡ºç°åœ¨æ–‡æ¡£ä¸­çš„æ¬¡æ•°) / (æ–‡æ¡£ä¸­çš„termæ€»æ•°).**IDF: Inverse Document Frequency**, è¡¡é‡ä¸€ä¸ªtermæœ‰å¤šé‡è¦ã€‚ æœ‰äº›è¯å‡ºç°çš„å¾ˆå¤š,ä½†æ˜¯æ˜æ˜¾ä¸æ˜¯å¾ˆæœ‰åµç”¨ã€‚æ¯”å¦‚â€™is',â€™theâ€˜,â€™andâ€˜ä¹‹ç±» çš„ã€‚
ä¸ºäº†å¹³è¡¡,æˆ‘ä»¬æŠŠç½•è§çš„è¯çš„é‡è¦æ€§(weight)æé«˜, æŠŠå¸¸è§è¯çš„é‡è¦æ€§æä½ã€‚
IDF(t) = log_e(æ–‡æ¡£æ€»æ•° / å«æœ‰tçš„æ–‡æ¡£æ€»æ•°). 

**TF-IDF = TF * IDF**

ä¸¾ä¸ªæ —å­ğŸŒ° :

ä¸€ä¸ªæ–‡æ¡£æœ‰100ä¸ªå•è¯,å…¶ä¸­å•è¯babyå‡ºç°äº†3æ¬¡ã€‚ é‚£ä¹ˆ,TF(baby) = (3/100) = 0.03.
å¥½,ç°åœ¨æˆ‘ä»¬å¦‚æœæœ‰10Mçš„æ–‡æ¡£, babyå‡ºç°åœ¨å…¶ä¸­çš„1000ä¸ªæ–‡æ¡£ä¸­ã€‚ é‚£ä¹ˆ,IDF(baby) = log(10,000,000 / 1,000) = 4æ‰€ä»¥, TF-IDF(baby) = TF(baby) \* IDF(baby) = 0.03 \* 4 = 0.12

#### 9.3.2 nltk implement tf-idf

```py
from nltk.text import TextCollection
# â¾¸å…ˆ, æŠŠæ‰€æœ‰çš„â½‚æ¡£æ”¾åˆ°TextCollectionç±»ä¸­ã€‚
# è¿™ä¸ªç±»ä¼šâ¾ƒåŠ¨å¸®ä½ æ–­ï¤†, åšç»Ÿè®¡, åšè®¡ç®—
corpus = TextCollection(['this is sentence one',
                        'this is sentence two',
                        'sentence three six',
                        'this is sentence three'])
# ç›´æ¥å°±èƒ½ç®—å‡ºtfidf
# (term: â¼€ä¸€å¥ï¤†è¯ä¸­çš„æŸä¸ªterm, text: è¿™å¥ï¤†è¯)
print(corpus.tf_idf('this', 'this is sentence four'))
# 0.444342
# åŒç†, æ€ä¹ˆå¾—åˆ°â¼€ä¸€ä¸ªæ ‡å‡†â¼¤å°çš„vectoræ¥è¡¨ç¤ºæ‰€æœ‰çš„å¥å­?
# å¯¹äºæ¯ä¸ªæ–°å¥å­
#new_sentence = 'this is sentence five' # éå†â¼€ä¸€éæ‰€æœ‰çš„vocabularyä¸­çš„è¯:
#for word in standard_vocab:
#    print(corpus.tf_idf(word, new_sentence)) # æˆ‘ä»¬ä¼šå¾—åˆ°â¼€ä¸ªå·¨é•¿(=æ‰€æœ‰vocabâ»“åº¦)çš„å‘é‡
```

[1]: http://www.nltk.org/
[2]: /images/ml/nlp-text-deal-process.png
[3]: http://www.regexlab.com/zh/regref.htm
[4]: /images/ml/nlp-nltk-raw.png
[5]: http://www.ranks.nl/stopwords
[6]: /images/ml/nlp-pre-deal.png
[7]: /images/ml/nlp-nltk-2.png
[8]: /images/ml/nlp-nltk-3.png
[9]: /images/ml/nlp-nltk-4.png
